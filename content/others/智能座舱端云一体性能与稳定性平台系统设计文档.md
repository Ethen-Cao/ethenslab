# 智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档

## 版本信息

| 序号 | 版本 | 修订内容 | 状态 | 修订人 | 日期 |
| --- | --- | --- | --- | --- | --- |
|1|0.1|First draft||操权力|2025/12/9|

## 文档目的

本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：

* 管理层决策：清晰阐述项目背景、痛点、ROI（投入产出比）及资源需求，作为立项审批与资源调度的依据。
* 跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。
* 工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。
  
## 背景与问题定义

### 背景

当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：

* 应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如 PV/UV、页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率、错误日志）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。

* 平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。

* 系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。

### 当前痛点

| 痛点 | 描述 | 业务影响 |
| --- | --- | --- |
| **跨端故障排查成本较高** | 当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。 | **研发效率受限**：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。 |
| **性能量化数据覆盖不足** | 现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。 | **版本评价受限**：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。 |
| **偶发异常现场回溯困难** | 对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。 | **闭环周期较长**：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。 |
| **资源效能优化缺乏支撑** | 缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。 | **成本优化受限**：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。 |


## 目标与范围

### 项目目标

本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标：

1. 全链路可观测：打破 Android、Linux Host、MCU 的数据孤岛，建立统一的 全局事件标准 (Global Event ID)，将分散在不同系统的故障与状态数据聚合至同一平台，实现跨端调用的追踪，为后续的可视化链路分析奠定数据基础。
   
2. 故障现场自动聚合与关联：突破现有“日志碎片化”及“事后拉取不全”的局限。建立 “事件驱动”的现场快照机制，在异常发生瞬间，自动聚合与该事件强相关的全维度上下文信息（如 Trace、系统 Log、进程状态等）并生成 完整的故障证据包。这不仅实现了 Event 与 Log 的精准索引，更确保了现场信息的完整性，彻底解决因关键日志缺失导致无法定位的难题。
   
3. 数据驱动治理：建立系统级的性能与稳定性基线（Baseline），通过量化数据驱动版本质量验收与硬件资源优化，将质量管理从“定性”转向“定量”。

### 核心 KPI 指标

| 维度 | 指标名称 | 目标值 (示例) | 说明 |
| --- | --- | --- | --- |
| **质量** | **严重故障主动发现率** | **> 90%** | 在用户报修前，通过平台主动捕获并预警系统级崩溃与卡顿。 |
| **效率** | **日志精准命中率** | **100%** | 每一个上报的严重异常事件，都能直接下载到对应的、正确的 Log 文件，无需人工筛选。 |
| **复现** | **致命问题现场捕获率** | **> 80%** | 针对 Crash/Watchdog 等致命问题，确保有对应的 Trace/Log 可供分析。 |
| **成本** | **资源优化场景产出** | **TOP 5/季度** | 每季度识别并输出 5 个高资源消耗（CPU/内存）场景。 |

### 项目范围

#### 范围内

1. **端侧全栈感知体系**：
   * **Android 深度探针**：构建系统级监控服务 `PolarisAgentService`，实现对应用生命周期、核心服务状态、底层资源（LMK/IO/Binder）的**全维度深度监听**。
   * **Linux/MCU 异构覆盖**：建设 Linux Host 侧的**系统健康守护进程**，负责关键服务（Service）存活检测与系统指标采集；适配 MCU 遥测协议，实现异构芯片间的故障透传。
   * **边缘智能处理**：在端侧实现数据的**预处理与清洗**，包含事件聚合、流控防爆、日志现场的智能截取与压缩，减轻车云带宽压力。
   * **标准化基础设施**：建立《全局事件注册表》及自动化工具链，统一多端的数据定义与协议标准。

2. **云端分析能力需求**:
   * **元数据管理能力**：要求云端支持同步《全局事件注册表》，实现对上报事件的自动化解析、分类与标签化管理。
   * **自动化关联引擎**：要求云端具备**“事件-日志”自动匹配能力**，将结构化的 Event 数据与非结构化的 Log 文件（基于索引）在存储层自动关联，形成完整的故障证据包。
   * **趋势与模式识别**：要求云端支持基于时间窗口的聚合计算，能够识别异常爆发（Spike）趋势及性能指标（CPU/内存）的长期演进趋势。

3. **可视化与运营平台**
   * **数字化质量驾驶舱**：建设多维度的质量仪表盘（Dashboard），支持按版本、车型、时间段下钻分析千车故障率、性能基线达标率。
   * **智能排查工作台**：提供“一站式”问题分析界面，支持通过 EventID/TraceID 检索故障，直接浏览关联的日志、堆栈及设备状态，支持远程诊断指令的下发与结果展示。

#### 范围外

1. **可视化的全链路拓扑分析**：1.0 阶段聚焦于跨端链路数据的 标准化采集与逻辑串联，优先夯实数据底座能力；全链路图形化的调用链拓扑展示规划在后续版本迭代中实现。
2. **业务代码修复**：Polaris 平台负责精准“定位”并“指派”问题，**不负责** 具体业务 APP 内部的代码逻辑修复。
3. **交互体验设计**：本项目专注于性能数据的量化，**不包含** HMI 界面（UI/UE）的主观交互设计与优化。

收到。这是一个非常清晰的边界界定。Polaris 1.0 的定位是**工具和平台**，为质量验收提供数据支撑，但不介入公司的行政管理流程（发布门禁）。

我已将 **“3. 质量验收流程”** 删除，并调整了 **角色职责** 和 **用户故事** 中涉及“发布阻断”的描述，使其聚焦于**线上监控**和**数据分析**本身。

以下是修改后的 **业务流程与核心场景** 章节：

---

## 业务流程与核心场景

### 角色定义

| 角色 | 职责描述 | 关注点 |
| --- | --- | --- |
| **研发工程师** | 接收告警，分析堆栈与日志，修复 Bug；**针对疑难客诉问题，远程下发特定诊断指令** | 故障堆栈的完整性，日志关联的准确性，是否需要补充更多运行时信息。 |
| **质量工程师** | 配置告警阈值，**监控线上大盘水位**，识别版本质量风险 | 故障率趋势是否劣化，性能指标是否符合预期，流量消耗是否异常。 |
| **产品经理** | 查看应用活跃度与性能体验趋势 | 核心功能的响应速度趋势，用户使用过程中的卡顿频率。 |

### 核心作业流程图

**1. 故障主动发现闭环流程**

> *描述从异常发生到研发接入的处理路径*

* **捕获 (Capture)**: Polaris Agent 监听到异常（如 ANR），记录运行时状态，抓取 Trace/Logcat，并生成唯一 EventID。
* **处理 (Process)**: 端侧进行流量控制检查，通过 `lref` 索引将 Event 与 Log 文件进行逻辑组合。
* **上报 (Report)**: Event 数据实时上报，大文件 Log 在 WiFi/空闲时段异步上传（支持云端按需拉取）。
* **通知 (Notify)**: 云端检测到异常数据超过阈值（例如某版本 Crash 率上升），向 **责任模块负责人** 发送通知。
* **分析 (Analyze)**: 研发工程师查看通知，进入平台查看关联的上下文数据，确认问题根因并修复。

**2. 疑难问题排查流程**

> *描述针对复杂客诉或非必现问题的处理路径*

* **检索**: 研发工程师在平台输入车辆 VIN 码或 EventID 检索相关记录。
* **查看**: 系统展示该事件的发生时间、设备信息、以及**已自动关联**的 Log 文件下载链接。
* **诊断**: 针对区域技术支持无法处理的复杂客诉，若现有日志不足以定位，**研发工程师** 通过控制台下发 `Shell` 诊断指令，端侧执行后回传结果，以获取更深度的运行时信息。

### 典型用户故事 (User Story)

#### 场景一：风险预警

> **背景**: 某车型灰度推送 v1.5 OTA 版本。
> **事件**: 上线 24 小时内，Polaris 平台监测到 `GVM_SYS_STORAGE_LOW`（磁盘空间不足）事件在特定批次车辆上的上报量呈**异常上升趋势**。
> **行动**:
> 1. 平台自动触发 **风险预警**，即时通知研发负责人。
> 2. 研发工程师通过平台获取存储分布数据，精准定位到某应用私有目录占用空间急剧膨胀。
> 3. **分析**: 结合自动关联采样的 Log，确认该应用在特定异常分支下陷入**数据库高频重复写入**死循环。
> **结果**: 研发团队在磁盘被完全耗尽导致系统挂死（System Hang）前，紧急输出修复补丁并推送 OTA，成功拦截了批量重大事故。
> 

#### 场景二：稳定性治理

> **背景**: 某应用发布 v2.0 灰度版本。
> **事件**: 灰度发布期间，平台监测到应用出现**偶发性** `GVM_APP_ANR`（无响应）告警，且线下测试难以复现。
> **行动**:
> 1. 研发工程师点击告警详情，查看聚合后的故障样本。
> 2. 系统已通过 `lref` 字段自动关联了故障时刻的 `traces.txt` 以及系统侧 `perflog` (性能日志)。
> 3. **分析**: 工程师通过 Trace 文件发现应用主线程阻塞在 Binder IPC 调用中；进一步联合分析 `perflog`，定位到是对端 Service 在高并发场景下因锁竞争导致处理耗时过长，拖累了客户端。
> **结果**: 确认根因为**服务端卡顿**。研发工程师针对服务端逻辑进行异步化优化，彻底解决了这一隐蔽的跨进程阻塞问题。
> 

#### 场景三：性能监控

> **背景**: 某版本上线后，产品经理关注核心应用在复杂交互场景下的滑动流畅度。
> **事件**: Polaris 仪表盘显示 `GVM_APP_JANK` (掉帧/卡顿) 指标在特定列表滑动场景下出现劣化趋势。
> **行动**:
> 1. 系统展示了掉帧率与主线程负载的关联曲线。
> 2. **发现**: 在卡顿发生的时间段内，主线程 MessageQueue 待处理消息数量显著激增。
> 3. **分析**: 研发工程师通过分析采集到的 `Looper` 统计数据，发现是一次性加载过多列表项导致并在主线程频繁 Post UI 刷新消息，引发**主线程消息队列积压**，从而阻塞了渲染信号（Vsync）的处理。
> **结果**: 研发工程师引入消息合并与节流机制（Throttling），消除了主线程拥堵，恢复了滑动流畅性。
>

#### 场景四：远程指令下发

> **背景**: 用户反馈方控按键（下一曲）失效，或错误地控制了不显示在屏幕上的后台音乐应用，常规 Logcat 无法体现系统内部的分发逻辑。 
> **行动**: 
> 1. 研发工程师怀疑是 MediaSession 焦点抢占或状态同步异常。
> 2. 工程师通过 Polaris 控制台，向目标车辆下发 dumpsys media_session 指令。
> 3. 分析: 回传的诊断结果显示，Media button session 仍被后台应用 com.reachauto.clouddesk 占用（尽管其状态为 active=false），导致按键事件未正确分发给前台亮屏的 com.tencent.wecarflow。 
> 结果: 确认根因是后台应用未正确释放焦点，研发工程师将 Bug 准确指派给相关应用团队，无需现场抓包。


## 需求拆解

## 系统总体方案

### 系统总体架构图

## 功能性需求

### 端侧采集层

### 云端处理层

### 平台应用层

## 非功能需求

## 端云交互协议设计

## 安全与隐私

## 风险 & 限制 & 依赖

## 实施计划

### 阶段划分

### 资源需求计划

## 附录