我是一名智能座舱系统工程师。
下面是我的项目介绍：
1. 项目基础信息
   
|项目名称|智能座舱端云一体性能与稳定性平台|
| --- | --- |
|项目代号|Polaris1.0|
|发起人/部门|智能座舱-座舱平台开发|
|项目类型|平台/系统软件 |
|当前状态|☐ 想法 ☑ 预研中 ☐ 立项审批中 ☐ 执行中|
|关键时间点|首次提出：2025/12/03|

2. 为什么要做？
问题来源 

|来源类型|背景描述|
| --- | --- |
|战略规划|缺少智能座舱端云一体性能与稳定性平台|
|长期痛点| 1. 观测盲区：缺乏端云一体的观测手段，线上跨端故障难以复现和定责。2. 标准缺失：缺乏统一的性能量化指标与数据底座，无法客观评价版本质量。3. 排查低效：缺乏影响因子的自动化关联分析，根因定位严重依赖专家经验。|
|竞品/行业对标|竞品（如头部新势力）已具备“故障主动预警”和“静默数据回传”能力；当前我们仍处于“用户投诉后才介入”的被动阶段，用户体验竞争力不足。|
|成本/资源管控|缺乏精细化的资源（CPU/内存/带宽）应用级监控，无法精准识别“资源刺客”进程，导致硬件选型时只能盲目增加冗余，推高了 BOM 成本。
|架构演进需求|随着座舱架构向 SOA（面向服务架构）演进，跨进程、跨芯片的调用链路指数级增加。传统的单点监控工具已失效，无法追踪跨端调用链中断或延迟的根因。|

2.2 初始假设与目标
- 初始假设 
  如果构建一套集“端侧深度探针 + 云端聚合分析 + 全链路追踪”于一体的智能座舱端云一体性能与稳定性平台，那么：
  1. 对于研发：可以通过跨系统Trace 能力以及云端监测预警能力，极大加速问题修复速度。
  2. 对于产品：可以实现从“用户报修”向“主动发现”的转变，对标头部竞品体验。
  3. 对于成本：可以通过精细化的资源画像，识别冗余开销，为下一代硬件选型的 BOM 成本优化提供数据支撑。
- 预期价值 ：
  
|价值维度|核心收益点|关键衡量指标|
| --- | --- | --- |
|质量与效率|缩短故障闭环周期打通云端观测链路，消除数据孤岛，实现故障根因的快速定位。|
|• 平均故障定位时长降低 50%• 致命/严重问题复现率提升至 80%|
|成本管控|硬件成本优化依据 1. 识别高资源消耗进程，优化调度；2. 为新车型硬件选型提供真实负载数据。|输出《硬件选型资源基线报告》一份|
|体验竞争力|主动服务能力构建具备静默数据回传与故障预警能力，在用户感知到卡顿/黑屏前进行介入或无感修复。| 整体主动发现率提升至 90%|


下面是Polaris1.0的功能树：

@startmindmapbs
title Polaris1.0 功能 WBS
skinparam defaultFontName "Microsoft YaHei"
left to right direction

* Polaris1.0 功能分解
** 核心KPI指标
*** 千车严重故障率
*** 应用崩溃率
*** 待处理高危告警\n告警类型 + 影响车辆数 + 爆发趋势(↑) + 涉及版本
**** 安全相关：行驶过程中发生的黑屏、SystemServer 重启、仪表通信丢失
**** 爆发趋势：某一个特定的 Crash 指纹（Fingerprint）在过去 1 小时内上报量环比激增 50%
**** 资源枯竭：例如：某个车型在过去一个小时内，因内存不足导致的应用被系统杀死的数量环比激增 50%

** 稳定性专项
*** Top Crash/ANR 榜单（按应用维度统计）
*** 系统重启详情（重启时间、重启原因、重启的域）
**** SystemServer重启实时上报
**** Android系统重启非实时上报
**** Linux Host重启非实时上报

*** Binder异常通信监控
**** 当监测到Android系统binder通信异常，记录事件。同时dump对应进程的binder status到perflog文件夹，生成perflog文件
**** 云端定时拉取perflog文件，建立事件和perflog文件的关联
*** 应用OOM事件监控
**** 当应用进程因为内存不足被系统杀死时，记录OOM事件，同时dump对应进程的内存使用情况到perflog文件夹，生成perflog文件
**** 云端定时拉取perflog文件，建立事件和perflog文件的关联

*** LMK事件监控
**** 当系统触发LMK（Low Memory Killer）机制杀死进程时，记录LMK事件，同时dump系统内存状态到perflog文件夹，生成perflog文件
**** 云端定时拉取perflog文件，建立事件和perflog文件的关联

*** 文件句柄泄漏监控
**** 当监测到进程文件句柄数异常增长时，记录文件句柄泄漏事件，同时dump对应进程的文件句柄信息到perflog文件夹，生成perflog文件
**** 云端定时拉取perflog文件，建立事件和perflog文件的关联

*** 核心进程crash监控
**** 当系统核心进程（如system_server、surfaceflinger等）发生crash时，记录crash事件，同时dump对应进程的日志和状态信息到perflog文件夹，生成perflog文件
**** 云端定时拉取perflog文件，建立事件和perflog文件的关联

** 性能与体验专项
*** 应用性能监控
**** Activity启动速度监控
**** 主线程拥堵监控
***** 当检测到主线程阻塞超过设定阈值时，记录拥堵事件，同时dump对应进程的主线程堆栈信息到perflog文件夹，生成perflog文件
***** 云端定时拉取perflog文件，建立事件和perflog文件的关联
**** 界面流畅度监控
***** 界面掉帧监控
****** 当检测到界面掉帧超过设定阈值时，记录掉帧事件，每半个小时上报云端一次
**** 资源消耗监控
***** CPU使用率监控
***** 内存使用率监控

**** 系统性能监控
***** CPU消耗趋势
***** 内存消耗趋势
***** IO性能


** 系统健康度
*** 存储健康
**** Flash 读写寿命损耗监测
**** eMMC/UFS 错误计数
*** 热管理
**** SoC/PMIC 关键点位温度趋势
**** 过热降频事件记录

** 事件标准与管理
*** 事件分级标准
*** 事件ID归属规范
*** perflog 目录规范

** 遥测功能
*** 远程操作治理
**** 命令白名单
**** 行车态禁止策略
**** 操作审计与回溯
@endmindmap


现实的背景是：
* 研发部门分为三个大部门：我是做座舱平台软件研发部门；还有APP研发部门，车云研发部门。
* 云端数据分析中心以及页面显示属于车云部门；同时车云部门提供了SDK供其他想数据上云的部门使用。这个SDK主要的方法是：public int sendWcLog(long eventid,byte type,short elevel,long eTime,String description). 其中String不能超过1000字节。几个重要的字段车云SDK定义如下：
  1. eventid：
     1. android子系统分配的范围是：6660000000~6669999999
     2. Linux子系统分配的范围是：6680000000~6689999999
     3. MCU子系统分配的范围是：6850000000-6859999999
  2. type：
     1. value为1表示keyinfo，一些关键累计信息，Type 1 (KeyInfo) 做性能数据（启动速度、内存水位）
     2. value为2表示keyexception，例如进程crash，部分功能失效。Type 2 (KeyException) 做非致命问题（ANR、Binder超时）
     3. value为3表示system error，例如系统crash，系统失效。Type 3 (System Error) 做致命问题（重启、黑屏）
  3. elevel: 0表示业务级别，1表示系统级别
* 目前APP部门（例如语音、地图）已经调用了车云SDK上报APP内部的监测事件，例如页面的点击次数。
* 云端会在特定的条件下从车机拉取日志；SDK并不提供主动触发大文件log上传的功能；大文件log上传是通过云端下达指令到SDK，SDK接收到指令后，从固定的目录拉取。例如：云端下达拉取crash的log，那么SDK从业务部门预定义的目录路径，例如：/data/local/anr目录下拉取文件。事件和log目录的对应关系，需要业务部门提前告知。

* Android侧会部署vlmagent,这个vlmagent是一个native进程属于车云部门开发。它的功能是收集其他ECU发送的维测事件，然后发送云端。而维测log是通过tftp协议发送到Android侧，再由vlmagent发生到云端。

