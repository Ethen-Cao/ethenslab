<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Binder | Ethen 的实验室</title><meta name=keywords content><meta name=description content='同步通信

异步通信

异步线程管理
为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：

Client 向 Server 发送 Txn_1 (目标: Node_A)。 -> 立即执行 (ST1)
Client 向 Server 发送 Txn_2 (目标: Node_A)。 -> 串行排队 (Serialization)
Client 向 Server 发送 Txn_3 (目标: Node_B)。 -> 并发执行 (ST2)
ST1 完成 Txn_1，触发 Txn_2 执行。

这个图展示了 proc->todo（全局队列）与 node->async_todo（节点私有队列）的交互。
@startuml
!theme plain
autonumber "<b>[000]"
hide footbox

title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B)

box "Client Process" #E1F5FE
    participant "ClientThread" as CT
end box

box "Kernel Space (Binder Driver)" #F5F5F5
    participant "BinderDriver" as BD
    participant "ServerProc\n(proc->todo)" as P_TODO
    participant "Node_A\n(node->async_todo)" as NA_TODO
    participant "Node_B\n(node->async_todo)" as NB_TODO
end box

box "Server Process" #FFF3E0
    participant "ServerThread_1\n(ST1)" as ST1
    participant "ServerThread_2\n(ST2)" as ST2
end box

== Phase 0: Server Threads Enter Pool (Wait) ==

note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work

ST1 -> BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read()
BD -> BD: binder_wait_for_work()
note right of BD: ST1 sleeps on\nwait_event_freezable_exclusive(proc->wait)
deactivate BD

ST2 -> BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read()
BD -> BD: binder_wait_for_work()
note right of BD: ST2 sleeps on\nwait_event_freezable_exclusive(proc->wait)
deactivate BD

|||

== Phase 1: Txn_1 to Node_A (Success: Wake ST1) ==

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_A->has_async_transaction == 0
end note

BD -> BD: Node_A->has_async_transaction = 1
BD -> BD: binder_alloc_new_buf()

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Since !pending_async:
  Enqueue to Process Global TODO
end note
BD -> P_TODO: binder_enqueue_work_ilocked(Txn_1)

BD -> BD: binder_select_thread_ilocked(proc)
note right of BD
  Pops ST1 from 
  proc->waiting_threads
end note

BD -> BD: binder_wakeup_thread_ilocked(proc, ST1)
BD -> ST1: wake_up_interruptible(&amp;ST1->wait)

BD -->> CT: return (Async returns immediately)
deactivate BD

activate ST1
ST1 -> BD: (Wakes up inside binder_thread_read)
activate BD
BD -> P_TODO: binder_dequeue_work_head_ilocked()
note left of P_TODO: Txn_1 moved from proc->todo to ST1
BD -->> ST1: return BR_TRANSACTION (Txn_1 data)
deactivate BD
note over ST1: <b>ST1 Processing Txn_1 (Node_A)</b>
deactivate ST1

|||

== Phase 2: Txn_2 to Node_A (Busy: Queued) ==

note left of CT: ST1 is still busy with Node_A.\nClient sends another to Node_A.

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_A->has_async_transaction == 1
  (Because Txn_1 is active)
end note

BD -> BD: pending_async = true

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Since pending_async == true:
  Enqueue to Node Private TODO
  <b>NO THREAD WOKEN UP!</b>
end note
BD -> NA_TODO: binder_enqueue_work_ilocked(Txn_2)

BD -->> CT: return
deactivate BD

|||

== Phase 3: Txn_3 to Node_B (Independent: Wake ST2) ==

note left of CT: ST1 busy with A, Txn_2 queued on A.\nClient sends to <b>Node_B</b>.

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_B->has_async_transaction == 0
  (Node_B is independent)
end note

BD -> BD: Node_B->has_async_transaction = 1

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Enqueue to Process Global TODO
end note
BD -> P_TODO: binder_enqueue_work_ilocked(Txn_3)

BD -> BD: binder_select_thread_ilocked(proc)
note right of BD
  ST1 is not in waiting_threads.
  Pops ST2 from waiting_threads.
end note

BD -> BD: binder_wakeup_thread_ilocked(proc, ST2)
BD -> ST2: wake_up_interruptible(&amp;ST2->wait)

BD -->> CT: return
deactivate BD

activate ST2
ST2 -> BD: (Wakes up)
activate BD
BD -> P_TODO: binder_dequeue_work_head_ilocked()
BD -->> ST2: return BR_TRANSACTION (Txn_3 data)
deactivate BD
note over ST2: <b>ST2 Processing Txn_3 (Node_B)</b>
deactivate ST2

|||

== Phase 4: ST1 Finishes Txn_1 & Triggers Txn_2 ==

note over ST1: ST1 finishes work,\ncalls free buffer

ST1 -> BD: ioctl(BC_FREE_BUFFER, buffer_ptr)
activate BD
BD -> BD: binder_thread_write() -> binder_free_buf()

note right of BD
  <b>Logic Check:</b>
  buffer->async_transaction && buffer->target_node
end note

BD -> NA_TODO: binder_dequeue_work_head_ilocked()
note right of NA_TODO
  Found Txn_2 waiting!
end note

alt Work Found in Async_Todo
    BD -> P_TODO: binder_enqueue_work_ilocked(Txn_2)
    note right of P_TODO
      Move Txn_2 from Node_A private queue
      to Global Process queue
    end note
    
    BD -> BD: binder_wakeup_proc_ilocked(proc)
    note right of BD
      <b>Wake up any available thread.</b>
      ST1 is currently in ioctl, so it will likely
      loop back and pick this up immediately
      in binder_thread_read.
    end note
else No Work Found
    BD -> BD: buf_node->has_async_transaction = false
end

BD -->> ST1: return
deactivate BD

ST1 -> BD: ioctl(BINDER_WRITE_READ, read...)
activate BD
BD -> BD: binder_thread_read()
BD -> P_TODO: binder_dequeue_work_head_ilocked()
note right of P_TODO: Picking up Txn_2
BD -->> ST1: return BR_TRANSACTION (Txn_2 data)
deactivate BD

note over ST1: <b>ST1 Processing Txn_2 (Node_A)</b>

@enduml
时序图核心逻辑解析 (基于 binder.c)

线程池等待 (Phase 0):


ST1 和 ST2 调用 binder_thread_read 并最终在一个等待队列上睡眠 (wait_event_freezable_exclusive)。此时它们都在 proc->waiting_threads 链表中。


首次异步调用 (Phase 1):


发送: binder_transaction 检查 Node_A->has_async_transaction 为 0。
入队: 设置标志位为 1，将 Txn_1 放入全局 proc->todo。
选人: binder_select_thread_ilocked 从 waiting_threads 中弹出 ST1。
唤醒: binder_wakeup_thread_ilocked 唤醒 ST1。


串行化排队 (Phase 2):


发送: 此时 ST1 正在处理 Txn_1，还没有释放 Buffer。驱动再次检查 Node_A->has_async_transaction，发现是 1。
入队: pending_async 为真。Txn_2 被放入 Node_A 的私有队列 node->async_todo。
不唤醒: 此时不会调用 binder_wakeup_thread_ilocked。即使 ST2 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。


并发处理不同实体 (Phase 3):


发送: 发送给 Node_B。Node_B 的 has_async_transaction 还是 0（独立计数）。
入队: Txn_3 放入全局 proc->todo。
选人: ST1 不在等待队列中（忙碌）。驱动从等待队列中弹出 ST2。
唤醒: 唤醒 ST2。此时 ST1 和 ST2 并行工作。


接力执行 (Phase 4):


ST1 处理完 Txn_1，调用 BC_FREE_BUFFER。
binder_free_buf 发现这是异步事务，于是去检查 Node_A->async_todo。
发现 Txn_2 在排队，将其搬运到全局 proc->todo。
调用 binder_wakeup_proc_ilocked 唤醒任意空闲线程（通常 ST1 马上进入 Read 循环，会直接再次拿到这个任务）。

内存管理
Binder 驱动的通信内存管理机制
Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。'><meta name=author content><link rel=canonical href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/><link crossorigin=anonymous href=/ethenslab/assets/css/stylesheet.a1917769c3c78460b110da6d7905321bb53af4a56f22ba4cc0de824cf4d097ab.css integrity="sha256-oZF3acPHhGCxENpteQUyG7U69KVvIrpMwN6CTPTQl6s=" rel="preload stylesheet" as=style><link rel=icon href=https://ethen-cao.github.io/ethenslab/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethen-cao.github.io/ethenslab/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethen-cao.github.io/ethenslab/favicon-32x32.png><link rel=apple-touch-icon href=https://ethen-cao.github.io/ethenslab/apple-touch-icon.png><link rel=mask-icon href=https://ethen-cao.github.io/ethenslab/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"><meta property="og:site_name" content="Ethen 的实验室"><meta property="og:title" content="Binder"><meta property="og:description" content='同步通信 异步通信 异步线程管理 为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：
Client 向 Server 发送 Txn_1 (目标: Node_A)。 -> 立即执行 (ST1) Client 向 Server 发送 Txn_2 (目标: Node_A)。 -> 串行排队 (Serialization) Client 向 Server 发送 Txn_3 (目标: Node_B)。 -> 并发执行 (ST2) ST1 完成 Txn_1，触发 Txn_2 执行。 这个图展示了 proc->todo（全局队列）与 node->async_todo（节点私有队列）的交互。
@startuml !theme plain autonumber "<b>[000]" hide footbox title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B) box "Client Process" #E1F5FE participant "ClientThread" as CT end box box "Kernel Space (Binder Driver)" #F5F5F5 participant "BinderDriver" as BD participant "ServerProc\n(proc->todo)" as P_TODO participant "Node_A\n(node->async_todo)" as NA_TODO participant "Node_B\n(node->async_todo)" as NB_TODO end box box "Server Process" #FFF3E0 participant "ServerThread_1\n(ST1)" as ST1 participant "ServerThread_2\n(ST2)" as ST2 end box == Phase 0: Server Threads Enter Pool (Wait) == note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work ST1 -> BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read() BD -> BD: binder_wait_for_work() note right of BD: ST1 sleeps on\nwait_event_freezable_exclusive(proc->wait) deactivate BD ST2 -> BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read() BD -> BD: binder_wait_for_work() note right of BD: ST2 sleeps on\nwait_event_freezable_exclusive(proc->wait) deactivate BD ||| == Phase 1: Txn_1 to Node_A (Success: Wake ST1) == CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -> BD: binder_transaction() note right of BD <b>Logic Check:</b> Node_A->has_async_transaction == 0 end note BD -> BD: Node_A->has_async_transaction = 1 BD -> BD: binder_alloc_new_buf() BD -> BD: binder_proc_transaction(target_proc) note right of BD Since !pending_async: Enqueue to Process Global TODO end note BD -> P_TODO: binder_enqueue_work_ilocked(Txn_1) BD -> BD: binder_select_thread_ilocked(proc) note right of BD Pops ST1 from proc->waiting_threads end note BD -> BD: binder_wakeup_thread_ilocked(proc, ST1) BD -> ST1: wake_up_interruptible(&amp;ST1->wait) BD -->> CT: return (Async returns immediately) deactivate BD activate ST1 ST1 -> BD: (Wakes up inside binder_thread_read) activate BD BD -> P_TODO: binder_dequeue_work_head_ilocked() note left of P_TODO: Txn_1 moved from proc->todo to ST1 BD -->> ST1: return BR_TRANSACTION (Txn_1 data) deactivate BD note over ST1: <b>ST1 Processing Txn_1 (Node_A)</b> deactivate ST1 ||| == Phase 2: Txn_2 to Node_A (Busy: Queued) == note left of CT: ST1 is still busy with Node_A.\nClient sends another to Node_A. CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -> BD: binder_transaction() note right of BD <b>Logic Check:</b> Node_A->has_async_transaction == 1 (Because Txn_1 is active) end note BD -> BD: pending_async = true BD -> BD: binder_proc_transaction(target_proc) note right of BD Since pending_async == true: Enqueue to Node Private TODO <b>NO THREAD WOKEN UP!</b> end note BD -> NA_TODO: binder_enqueue_work_ilocked(Txn_2) BD -->> CT: return deactivate BD ||| == Phase 3: Txn_3 to Node_B (Independent: Wake ST2) == note left of CT: ST1 busy with A, Txn_2 queued on A.\nClient sends to <b>Node_B</b>. CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B) activate BD BD -> BD: binder_transaction() note right of BD <b>Logic Check:</b> Node_B->has_async_transaction == 0 (Node_B is independent) end note BD -> BD: Node_B->has_async_transaction = 1 BD -> BD: binder_proc_transaction(target_proc) note right of BD Enqueue to Process Global TODO end note BD -> P_TODO: binder_enqueue_work_ilocked(Txn_3) BD -> BD: binder_select_thread_ilocked(proc) note right of BD ST1 is not in waiting_threads. Pops ST2 from waiting_threads. end note BD -> BD: binder_wakeup_thread_ilocked(proc, ST2) BD -> ST2: wake_up_interruptible(&amp;ST2->wait) BD -->> CT: return deactivate BD activate ST2 ST2 -> BD: (Wakes up) activate BD BD -> P_TODO: binder_dequeue_work_head_ilocked() BD -->> ST2: return BR_TRANSACTION (Txn_3 data) deactivate BD note over ST2: <b>ST2 Processing Txn_3 (Node_B)</b> deactivate ST2 ||| == Phase 4: ST1 Finishes Txn_1 & Triggers Txn_2 == note over ST1: ST1 finishes work,\ncalls free buffer ST1 -> BD: ioctl(BC_FREE_BUFFER, buffer_ptr) activate BD BD -> BD: binder_thread_write() -> binder_free_buf() note right of BD <b>Logic Check:</b> buffer->async_transaction && buffer->target_node end note BD -> NA_TODO: binder_dequeue_work_head_ilocked() note right of NA_TODO Found Txn_2 waiting! end note alt Work Found in Async_Todo BD -> P_TODO: binder_enqueue_work_ilocked(Txn_2) note right of P_TODO Move Txn_2 from Node_A private queue to Global Process queue end note BD -> BD: binder_wakeup_proc_ilocked(proc) note right of BD <b>Wake up any available thread.</b> ST1 is currently in ioctl, so it will likely loop back and pick this up immediately in binder_thread_read. end note else No Work Found BD -> BD: buf_node->has_async_transaction = false end BD -->> ST1: return deactivate BD ST1 -> BD: ioctl(BINDER_WRITE_READ, read...) activate BD BD -> BD: binder_thread_read() BD -> P_TODO: binder_dequeue_work_head_ilocked() note right of P_TODO: Picking up Txn_2 BD -->> ST1: return BR_TRANSACTION (Txn_2 data) deactivate BD note over ST1: <b>ST1 Processing Txn_2 (Node_A)</b> @enduml 时序图核心逻辑解析 (基于 binder.c) 线程池等待 (Phase 0): ST1 和 ST2 调用 binder_thread_read 并最终在一个等待队列上睡眠 (wait_event_freezable_exclusive)。此时它们都在 proc->waiting_threads 链表中。 首次异步调用 (Phase 1): 发送: binder_transaction 检查 Node_A->has_async_transaction 为 0。 入队: 设置标志位为 1，将 Txn_1 放入全局 proc->todo。 选人: binder_select_thread_ilocked 从 waiting_threads 中弹出 ST1。 唤醒: binder_wakeup_thread_ilocked 唤醒 ST1。 串行化排队 (Phase 2): 发送: 此时 ST1 正在处理 Txn_1，还没有释放 Buffer。驱动再次检查 Node_A->has_async_transaction，发现是 1。 入队: pending_async 为真。Txn_2 被放入 Node_A 的私有队列 node->async_todo。 不唤醒: 此时不会调用 binder_wakeup_thread_ilocked。即使 ST2 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。 并发处理不同实体 (Phase 3): 发送: 发送给 Node_B。Node_B 的 has_async_transaction 还是 0（独立计数）。 入队: Txn_3 放入全局 proc->todo。 选人: ST1 不在等待队列中（忙碌）。驱动从等待队列中弹出 ST2。 唤醒: 唤醒 ST2。此时 ST1 和 ST2 并行工作。 接力执行 (Phase 4): ST1 处理完 Txn_1，调用 BC_FREE_BUFFER。 binder_free_buf 发现这是异步事务，于是去检查 Node_A->async_todo。 发现 Txn_2 在排队，将其搬运到全局 proc->todo。 调用 binder_wakeup_proc_ilocked 唤醒任意空闲线程（通常 ST1 马上进入 Read 循环，会直接再次拿到这个任务）。 内存管理 Binder 驱动的通信内存管理机制 Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。'><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="android-dev"><meta property="article:published_time" content="2025-08-28T14:30:02+08:00"><meta property="article:modified_time" content="2025-08-28T14:30:02+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Binder"><meta name=twitter:description content='同步通信

异步通信

异步线程管理
为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：

Client 向 Server 发送 Txn_1 (目标: Node_A)。 -> 立即执行 (ST1)
Client 向 Server 发送 Txn_2 (目标: Node_A)。 -> 串行排队 (Serialization)
Client 向 Server 发送 Txn_3 (目标: Node_B)。 -> 并发执行 (ST2)
ST1 完成 Txn_1，触发 Txn_2 执行。

这个图展示了 proc->todo（全局队列）与 node->async_todo（节点私有队列）的交互。
@startuml
!theme plain
autonumber "<b>[000]"
hide footbox

title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B)

box "Client Process" #E1F5FE
    participant "ClientThread" as CT
end box

box "Kernel Space (Binder Driver)" #F5F5F5
    participant "BinderDriver" as BD
    participant "ServerProc\n(proc->todo)" as P_TODO
    participant "Node_A\n(node->async_todo)" as NA_TODO
    participant "Node_B\n(node->async_todo)" as NB_TODO
end box

box "Server Process" #FFF3E0
    participant "ServerThread_1\n(ST1)" as ST1
    participant "ServerThread_2\n(ST2)" as ST2
end box

== Phase 0: Server Threads Enter Pool (Wait) ==

note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work

ST1 -> BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read()
BD -> BD: binder_wait_for_work()
note right of BD: ST1 sleeps on\nwait_event_freezable_exclusive(proc->wait)
deactivate BD

ST2 -> BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -> BD: binder_ioctl_write_read()\n -> binder_thread_read()
BD -> BD: binder_wait_for_work()
note right of BD: ST2 sleeps on\nwait_event_freezable_exclusive(proc->wait)
deactivate BD

|||

== Phase 1: Txn_1 to Node_A (Success: Wake ST1) ==

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_A->has_async_transaction == 0
end note

BD -> BD: Node_A->has_async_transaction = 1
BD -> BD: binder_alloc_new_buf()

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Since !pending_async:
  Enqueue to Process Global TODO
end note
BD -> P_TODO: binder_enqueue_work_ilocked(Txn_1)

BD -> BD: binder_select_thread_ilocked(proc)
note right of BD
  Pops ST1 from 
  proc->waiting_threads
end note

BD -> BD: binder_wakeup_thread_ilocked(proc, ST1)
BD -> ST1: wake_up_interruptible(&amp;ST1->wait)

BD -->> CT: return (Async returns immediately)
deactivate BD

activate ST1
ST1 -> BD: (Wakes up inside binder_thread_read)
activate BD
BD -> P_TODO: binder_dequeue_work_head_ilocked()
note left of P_TODO: Txn_1 moved from proc->todo to ST1
BD -->> ST1: return BR_TRANSACTION (Txn_1 data)
deactivate BD
note over ST1: <b>ST1 Processing Txn_1 (Node_A)</b>
deactivate ST1

|||

== Phase 2: Txn_2 to Node_A (Busy: Queued) ==

note left of CT: ST1 is still busy with Node_A.\nClient sends another to Node_A.

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_A->has_async_transaction == 1
  (Because Txn_1 is active)
end note

BD -> BD: pending_async = true

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Since pending_async == true:
  Enqueue to Node Private TODO
  <b>NO THREAD WOKEN UP!</b>
end note
BD -> NA_TODO: binder_enqueue_work_ilocked(Txn_2)

BD -->> CT: return
deactivate BD

|||

== Phase 3: Txn_3 to Node_B (Independent: Wake ST2) ==

note left of CT: ST1 busy with A, Txn_2 queued on A.\nClient sends to <b>Node_B</b>.

CT -> BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B)
activate BD
BD -> BD: binder_transaction()

note right of BD
  <b>Logic Check:</b>
  Node_B->has_async_transaction == 0
  (Node_B is independent)
end note

BD -> BD: Node_B->has_async_transaction = 1

BD -> BD: binder_proc_transaction(target_proc)
note right of BD
  Enqueue to Process Global TODO
end note
BD -> P_TODO: binder_enqueue_work_ilocked(Txn_3)

BD -> BD: binder_select_thread_ilocked(proc)
note right of BD
  ST1 is not in waiting_threads.
  Pops ST2 from waiting_threads.
end note

BD -> BD: binder_wakeup_thread_ilocked(proc, ST2)
BD -> ST2: wake_up_interruptible(&amp;ST2->wait)

BD -->> CT: return
deactivate BD

activate ST2
ST2 -> BD: (Wakes up)
activate BD
BD -> P_TODO: binder_dequeue_work_head_ilocked()
BD -->> ST2: return BR_TRANSACTION (Txn_3 data)
deactivate BD
note over ST2: <b>ST2 Processing Txn_3 (Node_B)</b>
deactivate ST2

|||

== Phase 4: ST1 Finishes Txn_1 & Triggers Txn_2 ==

note over ST1: ST1 finishes work,\ncalls free buffer

ST1 -> BD: ioctl(BC_FREE_BUFFER, buffer_ptr)
activate BD
BD -> BD: binder_thread_write() -> binder_free_buf()

note right of BD
  <b>Logic Check:</b>
  buffer->async_transaction && buffer->target_node
end note

BD -> NA_TODO: binder_dequeue_work_head_ilocked()
note right of NA_TODO
  Found Txn_2 waiting!
end note

alt Work Found in Async_Todo
    BD -> P_TODO: binder_enqueue_work_ilocked(Txn_2)
    note right of P_TODO
      Move Txn_2 from Node_A private queue
      to Global Process queue
    end note
    
    BD -> BD: binder_wakeup_proc_ilocked(proc)
    note right of BD
      <b>Wake up any available thread.</b>
      ST1 is currently in ioctl, so it will likely
      loop back and pick this up immediately
      in binder_thread_read.
    end note
else No Work Found
    BD -> BD: buf_node->has_async_transaction = false
end

BD -->> ST1: return
deactivate BD

ST1 -> BD: ioctl(BINDER_WRITE_READ, read...)
activate BD
BD -> BD: binder_thread_read()
BD -> P_TODO: binder_dequeue_work_head_ilocked()
note right of P_TODO: Picking up Txn_2
BD -->> ST1: return BR_TRANSACTION (Txn_2 data)
deactivate BD

note over ST1: <b>ST1 Processing Txn_2 (Node_A)</b>

@enduml
时序图核心逻辑解析 (基于 binder.c)

线程池等待 (Phase 0):


ST1 和 ST2 调用 binder_thread_read 并最终在一个等待队列上睡眠 (wait_event_freezable_exclusive)。此时它们都在 proc->waiting_threads 链表中。


首次异步调用 (Phase 1):


发送: binder_transaction 检查 Node_A->has_async_transaction 为 0。
入队: 设置标志位为 1，将 Txn_1 放入全局 proc->todo。
选人: binder_select_thread_ilocked 从 waiting_threads 中弹出 ST1。
唤醒: binder_wakeup_thread_ilocked 唤醒 ST1。


串行化排队 (Phase 2):


发送: 此时 ST1 正在处理 Txn_1，还没有释放 Buffer。驱动再次检查 Node_A->has_async_transaction，发现是 1。
入队: pending_async 为真。Txn_2 被放入 Node_A 的私有队列 node->async_todo。
不唤醒: 此时不会调用 binder_wakeup_thread_ilocked。即使 ST2 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。


并发处理不同实体 (Phase 3):


发送: 发送给 Node_B。Node_B 的 has_async_transaction 还是 0（独立计数）。
入队: Txn_3 放入全局 proc->todo。
选人: ST1 不在等待队列中（忙碌）。驱动从等待队列中弹出 ST2。
唤醒: 唤醒 ST2。此时 ST1 和 ST2 并行工作。


接力执行 (Phase 4):


ST1 处理完 Txn_1，调用 BC_FREE_BUFFER。
binder_free_buf 发现这是异步事务，于是去检查 Node_A->async_todo。
发现 Txn_2 在排队，将其搬运到全局 proc->todo。
调用 binder_wakeup_proc_ilocked 唤醒任意空闲线程（通常 ST1 马上进入 Read 循环，会直接再次拿到这个任务）。

内存管理
Binder 驱动的通信内存管理机制
Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Android系统开发","item":"https://ethen-cao.github.io/ethenslab/android-dev/"},{"@type":"ListItem","position":2,"name":"基础通信","item":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/"},{"@type":"ListItem","position":3,"name":"Binder","item":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Binder","name":"Binder","description":"同步通信 异步通信 异步线程管理 为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：\nClient 向 Server 发送 Txn_1 (目标: Node_A)。 -\u0026gt; 立即执行 (ST1) Client 向 Server 发送 Txn_2 (目标: Node_A)。 -\u0026gt; 串行排队 (Serialization) Client 向 Server 发送 Txn_3 (目标: Node_B)。 -\u0026gt; 并发执行 (ST2) ST1 完成 Txn_1，触发 Txn_2 执行。 这个图展示了 proc-\u0026gt;todo（全局队列）与 node-\u0026gt;async_todo（节点私有队列）的交互。\n@startuml !theme plain autonumber \u0026#34;\u0026lt;b\u0026gt;[000]\u0026#34; hide footbox title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B) box \u0026#34;Client Process\u0026#34; #E1F5FE participant \u0026#34;ClientThread\u0026#34; as CT end box box \u0026#34;Kernel Space (Binder Driver)\u0026#34; #F5F5F5 participant \u0026#34;BinderDriver\u0026#34; as BD participant \u0026#34;ServerProc\\n(proc-\u0026gt;todo)\u0026#34; as P_TODO participant \u0026#34;Node_A\\n(node-\u0026gt;async_todo)\u0026#34; as NA_TODO participant \u0026#34;Node_B\\n(node-\u0026gt;async_todo)\u0026#34; as NB_TODO end box box \u0026#34;Server Process\u0026#34; #FFF3E0 participant \u0026#34;ServerThread_1\\n(ST1)\u0026#34; as ST1 participant \u0026#34;ServerThread_2\\n(ST2)\u0026#34; as ST2 end box == Phase 0: Server Threads Enter Pool (Wait) == note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work ST1 -\u0026gt; BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u0026gt; BD: binder_ioctl_write_read()\\n -\u0026gt; binder_thread_read() BD -\u0026gt; BD: binder_wait_for_work() note right of BD: ST1 sleeps on\\nwait_event_freezable_exclusive(proc-\u0026gt;wait) deactivate BD ST2 -\u0026gt; BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u0026gt; BD: binder_ioctl_write_read()\\n -\u0026gt; binder_thread_read() BD -\u0026gt; BD: binder_wait_for_work() note right of BD: ST2 sleeps on\\nwait_event_freezable_exclusive(proc-\u0026gt;wait) deactivate BD ||| == Phase 1: Txn_1 to Node_A (Success: Wake ST1) == CT -\u0026gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u0026gt; BD: binder_transaction() note right of BD \u0026lt;b\u0026gt;Logic Check:\u0026lt;/b\u0026gt; Node_A-\u0026gt;has_async_transaction == 0 end note BD -\u0026gt; BD: Node_A-\u0026gt;has_async_transaction = 1 BD -\u0026gt; BD: binder_alloc_new_buf() BD -\u0026gt; BD: binder_proc_transaction(target_proc) note right of BD Since !pending_async: Enqueue to Process Global TODO end note BD -\u0026gt; P_TODO: binder_enqueue_work_ilocked(Txn_1) BD -\u0026gt; BD: binder_select_thread_ilocked(proc) note right of BD Pops ST1 from proc-\u0026gt;waiting_threads end note BD -\u0026gt; BD: binder_wakeup_thread_ilocked(proc, ST1) BD -\u0026gt; ST1: wake_up_interruptible(\u0026amp;ST1-\u0026gt;wait) BD --\u0026gt;\u0026gt; CT: return (Async returns immediately) deactivate BD activate ST1 ST1 -\u0026gt; BD: (Wakes up inside binder_thread_read) activate BD BD -\u0026gt; P_TODO: binder_dequeue_work_head_ilocked() note left of P_TODO: Txn_1 moved from proc-\u0026gt;todo to ST1 BD --\u0026gt;\u0026gt; ST1: return BR_TRANSACTION (Txn_1 data) deactivate BD note over ST1: \u0026lt;b\u0026gt;ST1 Processing Txn_1 (Node_A)\u0026lt;/b\u0026gt; deactivate ST1 ||| == Phase 2: Txn_2 to Node_A (Busy: Queued) == note left of CT: ST1 is still busy with Node_A.\\nClient sends another to Node_A. CT -\u0026gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u0026gt; BD: binder_transaction() note right of BD \u0026lt;b\u0026gt;Logic Check:\u0026lt;/b\u0026gt; Node_A-\u0026gt;has_async_transaction == 1 (Because Txn_1 is active) end note BD -\u0026gt; BD: pending_async = true BD -\u0026gt; BD: binder_proc_transaction(target_proc) note right of BD Since pending_async == true: Enqueue to Node Private TODO \u0026lt;b\u0026gt;NO THREAD WOKEN UP!\u0026lt;/b\u0026gt; end note BD -\u0026gt; NA_TODO: binder_enqueue_work_ilocked(Txn_2) BD --\u0026gt;\u0026gt; CT: return deactivate BD ||| == Phase 3: Txn_3 to Node_B (Independent: Wake ST2) == note left of CT: ST1 busy with A, Txn_2 queued on A.\\nClient sends to \u0026lt;b\u0026gt;Node_B\u0026lt;/b\u0026gt;. CT -\u0026gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B) activate BD BD -\u0026gt; BD: binder_transaction() note right of BD \u0026lt;b\u0026gt;Logic Check:\u0026lt;/b\u0026gt; Node_B-\u0026gt;has_async_transaction == 0 (Node_B is independent) end note BD -\u0026gt; BD: Node_B-\u0026gt;has_async_transaction = 1 BD -\u0026gt; BD: binder_proc_transaction(target_proc) note right of BD Enqueue to Process Global TODO end note BD -\u0026gt; P_TODO: binder_enqueue_work_ilocked(Txn_3) BD -\u0026gt; BD: binder_select_thread_ilocked(proc) note right of BD ST1 is not in waiting_threads. Pops ST2 from waiting_threads. end note BD -\u0026gt; BD: binder_wakeup_thread_ilocked(proc, ST2) BD -\u0026gt; ST2: wake_up_interruptible(\u0026amp;ST2-\u0026gt;wait) BD --\u0026gt;\u0026gt; CT: return deactivate BD activate ST2 ST2 -\u0026gt; BD: (Wakes up) activate BD BD -\u0026gt; P_TODO: binder_dequeue_work_head_ilocked() BD --\u0026gt;\u0026gt; ST2: return BR_TRANSACTION (Txn_3 data) deactivate BD note over ST2: \u0026lt;b\u0026gt;ST2 Processing Txn_3 (Node_B)\u0026lt;/b\u0026gt; deactivate ST2 ||| == Phase 4: ST1 Finishes Txn_1 \u0026amp; Triggers Txn_2 == note over ST1: ST1 finishes work,\\ncalls free buffer ST1 -\u0026gt; BD: ioctl(BC_FREE_BUFFER, buffer_ptr) activate BD BD -\u0026gt; BD: binder_thread_write() -\u0026gt; binder_free_buf() note right of BD \u0026lt;b\u0026gt;Logic Check:\u0026lt;/b\u0026gt; buffer-\u0026gt;async_transaction \u0026amp;\u0026amp; buffer-\u0026gt;target_node end note BD -\u0026gt; NA_TODO: binder_dequeue_work_head_ilocked() note right of NA_TODO Found Txn_2 waiting! end note alt Work Found in Async_Todo BD -\u0026gt; P_TODO: binder_enqueue_work_ilocked(Txn_2) note right of P_TODO Move Txn_2 from Node_A private queue to Global Process queue end note BD -\u0026gt; BD: binder_wakeup_proc_ilocked(proc) note right of BD \u0026lt;b\u0026gt;Wake up any available thread.\u0026lt;/b\u0026gt; ST1 is currently in ioctl, so it will likely loop back and pick this up immediately in binder_thread_read. end note else No Work Found BD -\u0026gt; BD: buf_node-\u0026gt;has_async_transaction = false end BD --\u0026gt;\u0026gt; ST1: return deactivate BD ST1 -\u0026gt; BD: ioctl(BINDER_WRITE_READ, read...) activate BD BD -\u0026gt; BD: binder_thread_read() BD -\u0026gt; P_TODO: binder_dequeue_work_head_ilocked() note right of P_TODO: Picking up Txn_2 BD --\u0026gt;\u0026gt; ST1: return BR_TRANSACTION (Txn_2 data) deactivate BD note over ST1: \u0026lt;b\u0026gt;ST1 Processing Txn_2 (Node_A)\u0026lt;/b\u0026gt; @enduml 时序图核心逻辑解析 (基于 binder.c) 线程池等待 (Phase 0): ST1 和 ST2 调用 binder_thread_read 并最终在一个等待队列上睡眠 (wait_event_freezable_exclusive)。此时它们都在 proc-\u0026gt;waiting_threads 链表中。 首次异步调用 (Phase 1): 发送: binder_transaction 检查 Node_A-\u0026gt;has_async_transaction 为 0。 入队: 设置标志位为 1，将 Txn_1 放入全局 proc-\u0026gt;todo。 选人: binder_select_thread_ilocked 从 waiting_threads 中弹出 ST1。 唤醒: binder_wakeup_thread_ilocked 唤醒 ST1。 串行化排队 (Phase 2): 发送: 此时 ST1 正在处理 Txn_1，还没有释放 Buffer。驱动再次检查 Node_A-\u0026gt;has_async_transaction，发现是 1。 入队: pending_async 为真。Txn_2 被放入 Node_A 的私有队列 node-\u0026gt;async_todo。 不唤醒: 此时不会调用 binder_wakeup_thread_ilocked。即使 ST2 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。 并发处理不同实体 (Phase 3): 发送: 发送给 Node_B。Node_B 的 has_async_transaction 还是 0（独立计数）。 入队: Txn_3 放入全局 proc-\u0026gt;todo。 选人: ST1 不在等待队列中（忙碌）。驱动从等待队列中弹出 ST2。 唤醒: 唤醒 ST2。此时 ST1 和 ST2 并行工作。 接力执行 (Phase 4): ST1 处理完 Txn_1，调用 BC_FREE_BUFFER。 binder_free_buf 发现这是异步事务，于是去检查 Node_A-\u0026gt;async_todo。 发现 Txn_2 在排队，将其搬运到全局 proc-\u0026gt;todo。 调用 binder_wakeup_proc_ilocked 唤醒任意空闲线程（通常 ST1 马上进入 Read 循环，会直接再次拿到这个任务）。 内存管理 Binder 驱动的通信内存管理机制 Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。\n","keywords":[],"articleBody":"同步通信 异步通信 异步线程管理 为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：\nClient 向 Server 发送 Txn_1 (目标: Node_A)。 -\u003e 立即执行 (ST1) Client 向 Server 发送 Txn_2 (目标: Node_A)。 -\u003e 串行排队 (Serialization) Client 向 Server 发送 Txn_3 (目标: Node_B)。 -\u003e 并发执行 (ST2) ST1 完成 Txn_1，触发 Txn_2 执行。 这个图展示了 proc-\u003etodo（全局队列）与 node-\u003easync_todo（节点私有队列）的交互。\n@startuml !theme plain autonumber \"[000]\" hide footbox title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B) box \"Client Process\" #E1F5FE participant \"ClientThread\" as CT end box box \"Kernel Space (Binder Driver)\" #F5F5F5 participant \"BinderDriver\" as BD participant \"ServerProc\\n(proc-\u003etodo)\" as P_TODO participant \"Node_A\\n(node-\u003easync_todo)\" as NA_TODO participant \"Node_B\\n(node-\u003easync_todo)\" as NB_TODO end box box \"Server Process\" #FFF3E0 participant \"ServerThread_1\\n(ST1)\" as ST1 participant \"ServerThread_2\\n(ST2)\" as ST2 end box == Phase 0: Server Threads Enter Pool (Wait) == note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work ST1 -\u003e BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u003e BD: binder_ioctl_write_read()\\n -\u003e binder_thread_read() BD -\u003e BD: binder_wait_for_work() note right of BD: ST1 sleeps on\\nwait_event_freezable_exclusive(proc-\u003ewait) deactivate BD ST2 -\u003e BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u003e BD: binder_ioctl_write_read()\\n -\u003e binder_thread_read() BD -\u003e BD: binder_wait_for_work() note right of BD: ST2 sleeps on\\nwait_event_freezable_exclusive(proc-\u003ewait) deactivate BD ||| == Phase 1: Txn_1 to Node_A (Success: Wake ST1) == CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_A-\u003ehas_async_transaction == 0 end note BD -\u003e BD: Node_A-\u003ehas_async_transaction = 1 BD -\u003e BD: binder_alloc_new_buf() BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Since !pending_async: Enqueue to Process Global TODO end note BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_1) BD -\u003e BD: binder_select_thread_ilocked(proc) note right of BD Pops ST1 from proc-\u003ewaiting_threads end note BD -\u003e BD: binder_wakeup_thread_ilocked(proc, ST1) BD -\u003e ST1: wake_up_interruptible(\u0026ST1-\u003ewait) BD --\u003e\u003e CT: return (Async returns immediately) deactivate BD activate ST1 ST1 -\u003e BD: (Wakes up inside binder_thread_read) activate BD BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() note left of P_TODO: Txn_1 moved from proc-\u003etodo to ST1 BD --\u003e\u003e ST1: return BR_TRANSACTION (Txn_1 data) deactivate BD note over ST1: ST1 Processing Txn_1 (Node_A) deactivate ST1 ||| == Phase 2: Txn_2 to Node_A (Busy: Queued) == note left of CT: ST1 is still busy with Node_A.\\nClient sends another to Node_A. CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_A-\u003ehas_async_transaction == 1 (Because Txn_1 is active) end note BD -\u003e BD: pending_async = true BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Since pending_async == true: Enqueue to Node Private TODO NO THREAD WOKEN UP! end note BD -\u003e NA_TODO: binder_enqueue_work_ilocked(Txn_2) BD --\u003e\u003e CT: return deactivate BD ||| == Phase 3: Txn_3 to Node_B (Independent: Wake ST2) == note left of CT: ST1 busy with A, Txn_2 queued on A.\\nClient sends to Node_B. CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_B-\u003ehas_async_transaction == 0 (Node_B is independent) end note BD -\u003e BD: Node_B-\u003ehas_async_transaction = 1 BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Enqueue to Process Global TODO end note BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_3) BD -\u003e BD: binder_select_thread_ilocked(proc) note right of BD ST1 is not in waiting_threads. Pops ST2 from waiting_threads. end note BD -\u003e BD: binder_wakeup_thread_ilocked(proc, ST2) BD -\u003e ST2: wake_up_interruptible(\u0026ST2-\u003ewait) BD --\u003e\u003e CT: return deactivate BD activate ST2 ST2 -\u003e BD: (Wakes up) activate BD BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() BD --\u003e\u003e ST2: return BR_TRANSACTION (Txn_3 data) deactivate BD note over ST2: ST2 Processing Txn_3 (Node_B) deactivate ST2 ||| == Phase 4: ST1 Finishes Txn_1 \u0026 Triggers Txn_2 == note over ST1: ST1 finishes work,\\ncalls free buffer ST1 -\u003e BD: ioctl(BC_FREE_BUFFER, buffer_ptr) activate BD BD -\u003e BD: binder_thread_write() -\u003e binder_free_buf() note right of BD Logic Check: buffer-\u003easync_transaction \u0026\u0026 buffer-\u003etarget_node end note BD -\u003e NA_TODO: binder_dequeue_work_head_ilocked() note right of NA_TODO Found Txn_2 waiting! end note alt Work Found in Async_Todo BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_2) note right of P_TODO Move Txn_2 from Node_A private queue to Global Process queue end note BD -\u003e BD: binder_wakeup_proc_ilocked(proc) note right of BD Wake up any available thread. ST1 is currently in ioctl, so it will likely loop back and pick this up immediately in binder_thread_read. end note else No Work Found BD -\u003e BD: buf_node-\u003ehas_async_transaction = false end BD --\u003e\u003e ST1: return deactivate BD ST1 -\u003e BD: ioctl(BINDER_WRITE_READ, read...) activate BD BD -\u003e BD: binder_thread_read() BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() note right of P_TODO: Picking up Txn_2 BD --\u003e\u003e ST1: return BR_TRANSACTION (Txn_2 data) deactivate BD note over ST1: ST1 Processing Txn_2 (Node_A) @enduml 时序图核心逻辑解析 (基于 binder.c) 线程池等待 (Phase 0): ST1 和 ST2 调用 binder_thread_read 并最终在一个等待队列上睡眠 (wait_event_freezable_exclusive)。此时它们都在 proc-\u003ewaiting_threads 链表中。 首次异步调用 (Phase 1): 发送: binder_transaction 检查 Node_A-\u003ehas_async_transaction 为 0。 入队: 设置标志位为 1，将 Txn_1 放入全局 proc-\u003etodo。 选人: binder_select_thread_ilocked 从 waiting_threads 中弹出 ST1。 唤醒: binder_wakeup_thread_ilocked 唤醒 ST1。 串行化排队 (Phase 2): 发送: 此时 ST1 正在处理 Txn_1，还没有释放 Buffer。驱动再次检查 Node_A-\u003ehas_async_transaction，发现是 1。 入队: pending_async 为真。Txn_2 被放入 Node_A 的私有队列 node-\u003easync_todo。 不唤醒: 此时不会调用 binder_wakeup_thread_ilocked。即使 ST2 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。 并发处理不同实体 (Phase 3): 发送: 发送给 Node_B。Node_B 的 has_async_transaction 还是 0（独立计数）。 入队: Txn_3 放入全局 proc-\u003etodo。 选人: ST1 不在等待队列中（忙碌）。驱动从等待队列中弹出 ST2。 唤醒: 唤醒 ST2。此时 ST1 和 ST2 并行工作。 接力执行 (Phase 4): ST1 处理完 Txn_1，调用 BC_FREE_BUFFER。 binder_free_buf 发现这是异步事务，于是去检查 Node_A-\u003easync_todo。 发现 Txn_2 在排队，将其搬运到全局 proc-\u003etodo。 调用 binder_wakeup_proc_ilocked 唤醒任意空闲线程（通常 ST1 马上进入 Read 循环，会直接再次拿到这个任务）。 内存管理 Binder 驱动的通信内存管理机制 Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。\n第一部分：Binder 通信内存的总体管理机制 该机制主要分为三个阶段：虚拟内存区的建立、事务缓冲区的分配与映射、以及事务缓冲区的释放。\n1.1 进程虚拟内存区的建立\n当一个用户进程首次打开 /dev/binder 设备并对其执行 mmap() 系统调用时，Binder 驱动会执行 binder_mmap() 内核函数。此函数的主要工作包括：\n为该进程在内核中创建一个 binder_proc 结构体实例，用于追踪其所有 Binder 相关状态。 在该进程的虚拟地址空间中，分配并初始化一块指定大小（由 BINDER_VM_SIZE 决定，通常为 1MB）的虚拟内存区域（Virtual Memory Area, VMA）。 将这个 VMA 与进程的 binder_proc 结构体关联。 此阶段的关键在于，仅分配了虚拟地址空间，并未分配实际的物理内存。这块 VMA 的作用是为未来接收 Binder 数据提供一个预先确定的目标地址范围。\n1.2 事务缓冲区的分配与映射\n当一个进程（Client）通过 ioctl(BINDER_WRITE_READ) 发起一次事务（Transaction）时：\n分配内核缓冲区：Binder 驱动的 binder_thread_write() 函数会调用 binder_alloc_buf()，根据事务数据的大小，从内核的通用物理内存池（如 vmalloc 或 slab 分配器）中分配一个 binder_buffer 内核对象。 数据拷贝（一次拷贝）：驱动调用 copy_from_user()，将数据从 Client 进程的用户空间地址，拷贝到上一步分配的内核 binder_buffer 中。这是整个跨进程通信中唯一的一次数据内容拷贝。 地址空间映射：驱动识别出目标进程（Server）后，并不会再次拷贝数据。它会执行一个核心操作：修改 Server 进程的内核页表，将承载着数据的 binder_buffer 的物理内存页，直接映射到 Server 进程在 1.1 阶段建立的 VMA 中。 数据投递：驱动将映射后的缓冲区在 Server 进程中的虚拟地址，连同事务命令（如 BR_TRANSACTION 或 BR_ONEWAY），一同投递给 Server 中等待的 Binder 线程。Server 线程可以直接访问该地址，如同访问进程内内存一样。 1.3 事务缓冲区的释放\n当 Server 进程处理完事务数据后，它会通过 ioctl 向驱动发送 BC_FREE_BUFFER 命令，并附上需要释放的缓冲区的虚拟地址。\n驱动接收到命令后，调用 binder_free_buf() 函数。 该函数首先解除该物理内存在 Server 进程 VMA 中的映射（通过修改页表），然后将 binder_buffer 对象及其占用的物理内存归还给内核的通用内存池。 第二部分：同步与异步消息的内存划分机制 Binder 驱动对同步和异步消息的内存管理，遵循“统一物理来源，独立记账配额”的核心原则。\n2.1 核心原则阐述\n统一物理来源：无论是同步还是异步事务，其 binder_buffer 均从上文所述的同一个内核通用物理内存池中分配。物理来源上没有划分。 独立记账配额：驱动对每个 binder_proc（即每个使用 Binder 的进程）内部，实施了严格独立的会计和配额制度，以此来区分和限制不同类型事务所能占用的内存资源。 2.2 异步事务的内存配额机制\n为防止非阻塞的异步调用（oneway）耗尽系统资源而形成拒绝服务攻击，Binder 驱动对异步事务强制实施了内存配额限制。\n配额初始化：在 binder_mmap() 阶段，驱动会为进程的 binder_proc 结构体初始化一个 free_async_space 成员变量。其初始值被设定为该进程总 Binder 缓冲区大小（buffer_size）的一半。 分配时检查：当 binder_alloc_buf() 为一个带有 TF_ONE_WAY 标志的异步事务分配缓冲区时，它会执行以下检查： 若请求的缓冲区大小 size 大于 当前可用的 free_async_space，分配将失败，内核返回 -ENOSPC 错误，该异步事务被丢弃。 若 size 小于等于 free_async_space，则分配成功，并从配额中扣除相应大小：free_async_space -= size。 释放时归还：当 binder_free_buf() 释放一个用于异步事务的缓冲区时，会将该缓冲区的大小加回到配额中：free_async_space += size。 2.3 同步事务的内存使用机制\n同步事务所请求的内存不受 free_async_space 配额的限制。 其内存使用主要受限于整个进程的总缓冲区大小（buffer_size）。由于同步调用会阻塞客户端线程，天然地形成了反向压力（Back-pressure），使其无法在短时间内无限制地消耗内存，因此不需要类似的显式配额限制。 综上所述，Binder 驱动通过 mmap 和页表修改实现了高效的“一次拷贝”内存管理。在此基础上，它并未对物理内存池进行划分，而是通过对每个进程实施一个占其总可用缓冲区一半的异步内存配额，来精确地约束和管理同步与异步消息的资源使用，确保系统的稳定性和公平性。\nDebug 以下是导致 oneway Binder 调用失败的主要原因，从最常见到最少见排列：\n1. 目标进程（B进程）不存在或已死亡 这是最常见的原因。如果 B 进程由于崩溃、被系统杀死（例如，低内存时）或者正常退出，而 A 进程仍然持有一个指向 B 进程服务的 Binder 代理对象，那么当 A 进程尝试通过这个代理发送消息时，Binder 驱动会发现目标进程已经不存在了。\n返回错误: 调用会立即失败，通常 JNI 层会抛出 DeadObjectException，底层返回的错误码是 DEAD_OBJECT。\n如何排查:\n检查 Logcat，过滤 B 进程的 PID 或者应用名，看是否有崩溃日志或 Process ended 的信息。 在 Logcat 中搜索 DeadObjectException 关键字。 解决方案: A 进程应该实现 IBinder.DeathRecipient 接口，并调用 binder.linkToDeath() 来监听 B 进程的死亡通知。在收到 binderDied() 回调后，A 进程应该清理掉旧的 Binder 代理对象，并在需要时重新获取服务。\n// 示例代码 private IBinder.DeathRecipient mDeathRecipient = new IBinder.DeathRecipient() { @Override public void binderDied() { if (mService != null) { mService.asBinder().unlinkToDeath(this, 0); mService = null; // 在这里执行重新连接服务的逻辑 } } }; // 获取服务后 mService.asBinder().linkToDeath(mDeathRecipient, 0); 2. Binder 事务缓冲区已满 Binder 通信依赖于一块内核管理的共享内存作为缓冲区。虽然 oneway 调用不需要等待 B 进程处理完，但它仍然需要将数据（方法标识符和参数）从 A 进程复制到这个内核缓冲区。\n如果 B 进程非常繁忙，或者有大量的进程（包括 A 进程）在短时间内向 B 进程发送了大量的 Binder 消息（无论是 oneway 还是双向的），就可能导致分配给 B 进程的 Binder 缓冲区被占满。当 A 进程再次尝试发送 oneway 消息时，内核无法为其分配空间，调用就会失败。\n返回错误: 底层返回 BR_FAILED_REPLY 或 FAILED_TRANSACTION。 如何排查: 使用 adb shell dumpsys binder 或 adb shell cat /sys/kernel/debug/binder/stats 查看 Binder 的统计信息，关注失败的事务（failed transactions）数量。 检查 B 进程是否有 ANR（Application Not Responding），如果 B 进程的主线程或 Binder 线程池被阻塞，它就无法及时处理收到的事务，导致缓冲区堆积。 3. 事务数据过大 Binder 事务能够承载的数据量是有限的，这个限制通常是 1MB 左右（实际上是整个 Binder 缓冲区的一部分）。如果你尝试在 oneway 调用中传递一个非常大的对象（例如一个巨大的 Bitmap 或 List），超过了这个限制，事务在发送阶段就会失败。\n返回错误: 底层会返回 TRANSACTION_TOO_LARGE 错误，Java 层会抛出 TransactionTooLargeException。 如何排查: 在 Logcat 中直接搜索 TransactionTooLargeException。 检查你通过 oneway 调用传递的数据大小。如果是图片或文件，应考虑使用其他 IPC 方式，如共享内存（Ashmem）或文件描述符（File Descriptor）。 4. 目标进程（B进程）无响应 (ANR) 即使 B 进程还活着，但如果它的 Binder 线程池中的所有线程都被长时间运行的任务占用了，或者主线程发生了 ANR，那么它就无法处理新的 Binder 请求。这会间接导致第 2 点中提到的“Binder 事务缓冲区已满”问题。ANR问题可能导致线程阻塞，间接导致异步线程处理能力下降。\n对于 oneway 调用来说，A 进程虽然不会被阻塞，但 Binder 驱动发现无法将消息派发给 B 进程的任何一个空闲线程时，可能会导致后续的调用失败（缓冲区满）。\n如何排查: 在 /data/anr/traces.txt 文件中查找 B 进程的 ANR 日志，分析其线程堆栈，看 Binder 线程是否被卡住。 5. SELinux 权限问题 在现代 Android 系统中，SELinux（安全增强型 Linux）对进程间的通信有严格的访问控制。如果 A 进程的 SELinux 上下文没有被授予向 B 进程的服务发起 Binder 调用的权限，那么这个调用在内核层面就会被拒绝。\n返回错误: 通常是 PERMISSION_DENIED。 如何排查: 在 Logcat 中过滤 avc: denied 关键字。SELinux 的拒绝日志会清晰地标明源上下文（source context, A进程）、目标上下文（target context, B进程）以及缺少的权限。 总结 总的来说，oneway Binder 调用失败的原因可以归结为以下几点：\n失败原因 关键错误/日志 常见场景 目标进程死亡 DeadObjectException B 进程崩溃或被系统杀死。 Binder 缓冲区满 FAILED_TRANSACTION B 进程处理慢，或短时请求过多。 传输数据过大 TransactionTooLargeException 传递了超过 1MB 的大数据。 目标进程无响应 B 进程的 ANR 日志 B 进程 Binder 线程池或主线程阻塞。 SELinux 权限不足 avc: denied 缺少正确的 SELinux 策略规则。 排查时，首先应该检查 Logcat，因为绝大多数问题（如进程死亡、数据过大、权限问题）都会在日志中留下明确的线索。如果日志没有直接线索，再考虑是否是由于 B 进程无响应导致的缓冲区溢出。\n","wordCount":"1280","inLanguage":"en","datePublished":"2025-08-28T14:30:02+08:00","dateModified":"2025-08-28T14:30:02+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"},"publisher":{"@type":"Organization","name":"Ethen 的实验室","logo":{"@type":"ImageObject","url":"https://ethen-cao.github.io/ethenslab/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethen-cao.github.io/ethenslab/ accesskey=h title="Ethen 的实验室 (Alt + H)">Ethen 的实验室</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethen-cao.github.io/ethenslab/android-dev/ title=Android系统开发><span>Android系统开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/android-automotive-os-dev/ title="Android Automotive"><span>Android Automotive</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/qnx/ title=QNX开发><span>QNX开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/gunyah/ title=Gunyah><span>Gunyah</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/ivi-solution/ title=智能座舱方案><span>智能座舱方案</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/explore-ai title="Explore AI"><span>Explore AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethen-cao.github.io/ethenslab/>Home</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/android-dev/>Android系统开发</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/>基础通信</a></div><h1 class="post-title entry-hint-parent">Binder</h1><div class=post-meta><span title='2025-08-28 14:30:02 +0800 CST'>August 28, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1280 words</div></header><div class=post-content><h2 id=同步通信>同步通信<a hidden class=anchor aria-hidden=true href=#同步通信>#</a></h2><p><img src=/ethenslab/images/binder-sync.png alt></p><h2 id=异步通信>异步通信<a hidden class=anchor aria-hidden=true href=#异步通信>#</a></h2><p><img src=/ethenslab/images/binder-async.png alt></p><h3 id=异步线程管理>异步线程管理<a hidden class=anchor aria-hidden=true href=#异步线程管理>#</a></h3><p>为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我设计了以下场景：</p><ol><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_1</strong> (目标: Node_A)。 -> <strong>立即执行 (ST1)</strong></li><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_2</strong> (目标: Node_A)。 -> <strong>串行排队 (Serialization)</strong></li><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_3</strong> (目标: Node_B)。 -> <strong>并发执行 (ST2)</strong></li><li><strong>ST1</strong> 完成 Txn_1，触发 Txn_2 执行。</li></ol><p>这个图展示了 <code>proc->todo</code>（全局队列）与 <code>node->async_todo</code>（节点私有队列）的交互。</p><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
autonumber &#34;&lt;b&gt;[000]&#34;
hide footbox

title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B)

box &#34;Client Process&#34; #E1F5FE
    participant &#34;ClientThread&#34; as CT
end box

box &#34;Kernel Space (Binder Driver)&#34; #F5F5F5
    participant &#34;BinderDriver&#34; as BD
    participant &#34;ServerProc\n(proc-&gt;todo)&#34; as P_TODO
    participant &#34;Node_A\n(node-&gt;async_todo)&#34; as NA_TODO
    participant &#34;Node_B\n(node-&gt;async_todo)&#34; as NB_TODO
end box

box &#34;Server Process&#34; #FFF3E0
    participant &#34;ServerThread_1\n(ST1)&#34; as ST1
    participant &#34;ServerThread_2\n(ST2)&#34; as ST2
end box

== Phase 0: Server Threads Enter Pool (Wait) ==

note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work

ST1 -&gt; BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -&gt; BD: binder_ioctl_write_read()\n -&gt; binder_thread_read()
BD -&gt; BD: binder_wait_for_work()
note right of BD: ST1 sleeps on\nwait_event_freezable_exclusive(proc-&gt;wait)
deactivate BD

ST2 -&gt; BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -&gt; BD: binder_ioctl_write_read()\n -&gt; binder_thread_read()
BD -&gt; BD: binder_wait_for_work()
note right of BD: ST2 sleeps on\nwait_event_freezable_exclusive(proc-&gt;wait)
deactivate BD

|||

== Phase 1: Txn_1 to Node_A (Success: Wake ST1) ==

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_A-&gt;has_async_transaction == 0
end note

BD -&gt; BD: Node_A-&gt;has_async_transaction = 1
BD -&gt; BD: binder_alloc_new_buf()

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Since !pending_async:
  Enqueue to Process Global TODO
end note
BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_1)

BD -&gt; BD: binder_select_thread_ilocked(proc)
note right of BD
  Pops ST1 from 
  proc-&gt;waiting_threads
end note

BD -&gt; BD: binder_wakeup_thread_ilocked(proc, ST1)
BD -&gt; ST1: wake_up_interruptible(&amp;ST1-&gt;wait)

BD --&gt;&gt; CT: return (Async returns immediately)
deactivate BD

activate ST1
ST1 -&gt; BD: (Wakes up inside binder_thread_read)
activate BD
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
note left of P_TODO: Txn_1 moved from proc-&gt;todo to ST1
BD --&gt;&gt; ST1: return BR_TRANSACTION (Txn_1 data)
deactivate BD
note over ST1: &lt;b&gt;ST1 Processing Txn_1 (Node_A)&lt;/b&gt;
deactivate ST1

|||

== Phase 2: Txn_2 to Node_A (Busy: Queued) ==

note left of CT: ST1 is still busy with Node_A.\nClient sends another to Node_A.

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_A-&gt;has_async_transaction == 1
  (Because Txn_1 is active)
end note

BD -&gt; BD: pending_async = true

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Since pending_async == true:
  Enqueue to Node Private TODO
  &lt;b&gt;NO THREAD WOKEN UP!&lt;/b&gt;
end note
BD -&gt; NA_TODO: binder_enqueue_work_ilocked(Txn_2)

BD --&gt;&gt; CT: return
deactivate BD

|||

== Phase 3: Txn_3 to Node_B (Independent: Wake ST2) ==

note left of CT: ST1 busy with A, Txn_2 queued on A.\nClient sends to &lt;b&gt;Node_B&lt;/b&gt;.

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_B-&gt;has_async_transaction == 0
  (Node_B is independent)
end note

BD -&gt; BD: Node_B-&gt;has_async_transaction = 1

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Enqueue to Process Global TODO
end note
BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_3)

BD -&gt; BD: binder_select_thread_ilocked(proc)
note right of BD
  ST1 is not in waiting_threads.
  Pops ST2 from waiting_threads.
end note

BD -&gt; BD: binder_wakeup_thread_ilocked(proc, ST2)
BD -&gt; ST2: wake_up_interruptible(&amp;ST2-&gt;wait)

BD --&gt;&gt; CT: return
deactivate BD

activate ST2
ST2 -&gt; BD: (Wakes up)
activate BD
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
BD --&gt;&gt; ST2: return BR_TRANSACTION (Txn_3 data)
deactivate BD
note over ST2: &lt;b&gt;ST2 Processing Txn_3 (Node_B)&lt;/b&gt;
deactivate ST2

|||

== Phase 4: ST1 Finishes Txn_1 &amp; Triggers Txn_2 ==

note over ST1: ST1 finishes work,\ncalls free buffer

ST1 -&gt; BD: ioctl(BC_FREE_BUFFER, buffer_ptr)
activate BD
BD -&gt; BD: binder_thread_write() -&gt; binder_free_buf()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  buffer-&gt;async_transaction &amp;&amp; buffer-&gt;target_node
end note

BD -&gt; NA_TODO: binder_dequeue_work_head_ilocked()
note right of NA_TODO
  Found Txn_2 waiting!
end note

alt Work Found in Async_Todo
    BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_2)
    note right of P_TODO
      Move Txn_2 from Node_A private queue
      to Global Process queue
    end note
    
    BD -&gt; BD: binder_wakeup_proc_ilocked(proc)
    note right of BD
      &lt;b&gt;Wake up any available thread.&lt;/b&gt;
      ST1 is currently in ioctl, so it will likely
      loop back and pick this up immediately
      in binder_thread_read.
    end note
else No Work Found
    BD -&gt; BD: buf_node-&gt;has_async_transaction = false
end

BD --&gt;&gt; ST1: return
deactivate BD

ST1 -&gt; BD: ioctl(BINDER_WRITE_READ, read...)
activate BD
BD -&gt; BD: binder_thread_read()
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
note right of P_TODO: Picking up Txn_2
BD --&gt;&gt; ST1: return BR_TRANSACTION (Txn_2 data)
deactivate BD

note over ST1: &lt;b&gt;ST1 Processing Txn_2 (Node_A)&lt;/b&gt;

@enduml
</code></pre><h4 id=时序图核心逻辑解析-基于-binderc>时序图核心逻辑解析 (基于 <code>binder.c</code>)<a hidden class=anchor aria-hidden=true href=#时序图核心逻辑解析-基于-binderc>#</a></h4><ol><li><strong>线程池等待 (Phase 0)</strong>:</li></ol><ul><li><code>ST1</code> 和 <code>ST2</code> 调用 <code>binder_thread_read</code> 并最终在一个等待队列上睡眠 (<code>wait_event_freezable_exclusive</code>)。此时它们都在 <code>proc->waiting_threads</code> 链表中。</li></ul><ol start=2><li><strong>首次异步调用 (Phase 1)</strong>:</li></ol><ul><li><strong>发送</strong>: <code>binder_transaction</code> 检查 <code>Node_A->has_async_transaction</code> 为 0。</li><li><strong>入队</strong>: 设置标志位为 1，将 <code>Txn_1</code> 放入全局 <code>proc->todo</code>。</li><li><strong>选人</strong>: <code>binder_select_thread_ilocked</code> 从 <code>waiting_threads</code> 中弹出 <code>ST1</code>。</li><li><strong>唤醒</strong>: <code>binder_wakeup_thread_ilocked</code> 唤醒 <code>ST1</code>。</li></ul><ol start=3><li><strong>串行化排队 (Phase 2)</strong>:</li></ol><ul><li><strong>发送</strong>: 此时 <code>ST1</code> 正在处理 <code>Txn_1</code>，还没有释放 Buffer。驱动再次检查 <code>Node_A->has_async_transaction</code>，发现是 1。</li><li><strong>入队</strong>: <code>pending_async</code> 为真。<code>Txn_2</code> 被放入 <code>Node_A</code> 的私有队列 <code>node->async_todo</code>。</li><li><strong>不唤醒</strong>: 此时<strong>不会</strong>调用 <code>binder_wakeup_thread_ilocked</code>。即使 <code>ST2</code> 空闲，它也拿不到这个任务。这就是“单线程处理”效果的来源。</li></ul><ol start=4><li><strong>并发处理不同实体 (Phase 3)</strong>:</li></ol><ul><li><strong>发送</strong>: 发送给 <code>Node_B</code>。<code>Node_B</code> 的 <code>has_async_transaction</code> 还是 0（独立计数）。</li><li><strong>入队</strong>: <code>Txn_3</code> 放入全局 <code>proc->todo</code>。</li><li><strong>选人</strong>: <code>ST1</code> 不在等待队列中（忙碌）。驱动从等待队列中弹出 <code>ST2</code>。</li><li><strong>唤醒</strong>: 唤醒 <code>ST2</code>。此时 <code>ST1</code> 和 <code>ST2</code> 并行工作。</li></ul><ol start=5><li><strong>接力执行 (Phase 4)</strong>:</li></ol><ul><li><code>ST1</code> 处理完 <code>Txn_1</code>，调用 <code>BC_FREE_BUFFER</code>。</li><li><code>binder_free_buf</code> 发现这是异步事务，于是去检查 <code>Node_A->async_todo</code>。</li><li>发现 <code>Txn_2</code> 在排队，将其<strong>搬运</strong>到全局 <code>proc->todo</code>。</li><li>调用 <code>binder_wakeup_proc_ilocked</code> 唤醒任意空闲线程（通常 <code>ST1</code> 马上进入 Read 循环，会直接再次拿到这个任务）。</li></ul><h2 id=内存管理>内存管理<a hidden class=anchor aria-hidden=true href=#内存管理>#</a></h2><h3 id=binder-驱动的通信内存管理机制><strong>Binder 驱动的通信内存管理机制</strong><a hidden class=anchor aria-hidden=true href=#binder-驱动的通信内存管理机制>#</a></h3><p>Binder 驱动的内存管理是一套内核层面的复杂机制，其核心目标是在进程间高效、安全地传输数据，并实现“一次拷贝”原则。</p><h4 id=第一部分binder-通信内存的总体管理机制><strong>第一部分：Binder 通信内存的总体管理机制</strong><a hidden class=anchor aria-hidden=true href=#第一部分binder-通信内存的总体管理机制>#</a></h4><p>该机制主要分为三个阶段：虚拟内存区的建立、事务缓冲区的分配与映射、以及事务缓冲区的释放。</p><p><strong>1.1 进程虚拟内存区的建立</strong></p><p>当一个用户进程首次打开 <code>/dev/binder</code> 设备并对其执行 <code>mmap()</code> 系统调用时，Binder 驱动会执行 <code>binder_mmap()</code> 内核函数。此函数的主要工作包括：</p><ul><li>为该进程在内核中创建一个 <code>binder_proc</code> 结构体实例，用于追踪其所有 Binder 相关状态。</li><li>在该进程的虚拟地址空间中，分配并初始化一块指定大小（由 <code>BINDER_VM_SIZE</code> 决定，通常为 1MB）的虚拟内存区域（Virtual Memory Area, VMA）。</li><li>将这个 VMA 与进程的 <code>binder_proc</code> 结构体关联。</li></ul><p>此阶段的关键在于，<strong>仅分配了虚拟地址空间，并未分配实际的物理内存</strong>。这块 VMA 的作用是为未来接收 Binder 数据提供一个预先确定的目标地址范围。</p><p><strong>1.2 事务缓冲区的分配与映射</strong></p><p>当一个进程（Client）通过 <code>ioctl(BINDER_WRITE_READ)</code> 发起一次事务（Transaction）时：</p><ol><li><strong>分配内核缓冲区</strong>：Binder 驱动的 <code>binder_thread_write()</code> 函数会调用 <code>binder_alloc_buf()</code>，根据事务数据的大小，从内核的通用物理内存池（如 <code>vmalloc</code> 或 slab 分配器）中分配一个 <code>binder_buffer</code> 内核对象。</li><li><strong>数据拷贝（一次拷贝）</strong>：驱动调用 <code>copy_from_user()</code>，将数据从 Client 进程的用户空间地址，拷贝到上一步分配的内核 <code>binder_buffer</code> 中。这是整个跨进程通信中唯一的一次数据内容拷贝。</li><li><strong>地址空间映射</strong>：驱动识别出目标进程（Server）后，并不会再次拷贝数据。它会执行一个核心操作：<strong>修改 Server 进程的内核页表</strong>，将承载着数据的 <code>binder_buffer</code> 的物理内存页，直接映射到 Server 进程在 1.1 阶段建立的 VMA 中。</li><li><strong>数据投递</strong>：驱动将映射后的缓冲区在 Server 进程中的虚拟地址，连同事务命令（如 <code>BR_TRANSACTION</code> 或 <code>BR_ONEWAY</code>），一同投递给 Server 中等待的 Binder 线程。Server 线程可以直接访问该地址，如同访问进程内内存一样。</li></ol><p><strong>1.3 事务缓冲区的释放</strong></p><p>当 Server 进程处理完事务数据后，它会通过 <code>ioctl</code> 向驱动发送 <code>BC_FREE_BUFFER</code> 命令，并附上需要释放的缓冲区的虚拟地址。</p><ul><li>驱动接收到命令后，调用 <code>binder_free_buf()</code> 函数。</li><li>该函数首先解除该物理内存在 Server 进程 VMA 中的映射（通过修改页表），然后将 <code>binder_buffer</code> 对象及其占用的物理内存归还给内核的通用内存池。</li></ul><hr><h4 id=第二部分同步与异步消息的内存划分机制><strong>第二部分：同步与异步消息的内存划分机制</strong><a hidden class=anchor aria-hidden=true href=#第二部分同步与异步消息的内存划分机制>#</a></h4><p>Binder 驱动对同步和异步消息的内存管理，遵循“<strong>统一物理来源，独立记账配额</strong>”的核心原则。</p><p><strong>2.1 核心原则阐述</strong></p><ul><li><strong>统一物理来源</strong>：无论是同步还是异步事务，其 <code>binder_buffer</code> 均从上文所述的<strong>同一个内核通用物理内存池</strong>中分配。物理来源上没有划分。</li><li><strong>独立记账配额</strong>：驱动对每个 <code>binder_proc</code>（即每个使用 Binder 的进程）内部，实施了严格独立的会计和配额制度，以此来区分和限制不同类型事务所能占用的内存资源。</li></ul><p><strong>2.2 异步事务的内存配额机制</strong></p><p>为防止非阻塞的异步调用（<code>oneway</code>）耗尽系统资源而形成拒绝服务攻击，Binder 驱动对异步事务强制实施了内存配额限制。</p><ul><li><strong>配额初始化</strong>：在 <code>binder_mmap()</code> 阶段，驱动会为进程的 <code>binder_proc</code> 结构体初始化一个 <code>free_async_space</code> 成员变量。其初始值被设定为该进程总 Binder 缓冲区大小（<code>buffer_size</code>）的<strong>一半</strong>。</li><li><strong>分配时检查</strong>：当 <code>binder_alloc_buf()</code> 为一个带有 <code>TF_ONE_WAY</code> 标志的异步事务分配缓冲区时，它会执行以下检查：<ul><li>若请求的缓冲区大小 <code>size</code> <strong>大于</strong> 当前可用的 <code>free_async_space</code>，分配将失败，内核返回 <code>-ENOSPC</code> 错误，该异步事务被丢弃。</li><li>若 <code>size</code> <strong>小于等于</strong> <code>free_async_space</code>，则分配成功，并从配额中扣除相应大小：<code>free_async_space -= size</code>。</li></ul></li><li><strong>释放时归还</strong>：当 <code>binder_free_buf()</code> 释放一个用于异步事务的缓冲区时，会将该缓冲区的大小加回到配额中：<code>free_async_space += size</code>。</li></ul><p><strong>2.3 同步事务的内存使用机制</strong></p><ul><li>同步事务所请求的内存<strong>不受 <code>free_async_space</code> 配额的限制</strong>。</li><li>其内存使用主要受限于整个进程的总缓冲区大小（<code>buffer_size</code>）。由于同步调用会阻塞客户端线程，天然地形成了反向压力（Back-pressure），使其无法在短时间内无限制地消耗内存，因此不需要类似的显式配额限制。</li></ul><p>综上所述，Binder 驱动通过 <code>mmap</code> 和页表修改实现了高效的“一次拷贝”内存管理。在此基础上，它并未对物理内存池进行划分，而是通过对每个进程实施一个占其总可用缓冲区一半的<strong>异步内存配额</strong>，来精确地约束和管理同步与异步消息的资源使用，确保系统的稳定性和公平性。</p><h2 id=debug>Debug<a hidden class=anchor aria-hidden=true href=#debug>#</a></h2><p>以下是导致 oneway Binder 调用失败的主要原因，从最常见到最少见排列：</p><h3 id=1-目标进程b进程不存在或已死亡>1. 目标进程（B进程）不存在或已死亡<a hidden class=anchor aria-hidden=true href=#1-目标进程b进程不存在或已死亡>#</a></h3><p>这是最常见的原因。如果 B 进程由于崩溃、被系统杀死（例如，低内存时）或者正常退出，而 A 进程仍然持有一个指向 B 进程服务的 Binder 代理对象，那么当 A 进程尝试通过这个代理发送消息时，Binder 驱动会发现目标进程已经不存在了。</p><ul><li><p><strong>返回错误</strong>: 调用会立即失败，通常 JNI 层会抛出 <code>DeadObjectException</code>，底层返回的错误码是 <code>DEAD_OBJECT</code>。</p></li><li><p><strong>如何排查</strong>:</p><ul><li>检查 Logcat，过滤 B 进程的 PID 或者应用名，看是否有崩溃日志或 <code>Process ended</code> 的信息。</li><li>在 Logcat 中搜索 <code>DeadObjectException</code> 关键字。</li></ul></li><li><p><strong>解决方案</strong>: A 进程应该实现 <code>IBinder.DeathRecipient</code> 接口，并调用 <code>binder.linkToDeath()</code> 来监听 B 进程的死亡通知。在收到 <code>binderDied()</code> 回调后，A 进程应该清理掉旧的 Binder 代理对象，并在需要时重新获取服务。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#75715e>// 示例代码</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>private</span> IBinder.<span style=color:#a6e22e>DeathRecipient</span> mDeathRecipient <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> IBinder.<span style=color:#a6e22e>DeathRecipient</span>() {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@Override</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>public</span> <span style=color:#66d9ef>void</span> <span style=color:#a6e22e>binderDied</span>() {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (mService <span style=color:#f92672>!=</span> <span style=color:#66d9ef>null</span>) {
</span></span><span style=display:flex><span>            mService.<span style=color:#a6e22e>asBinder</span>().<span style=color:#a6e22e>unlinkToDeath</span>(<span style=color:#66d9ef>this</span>, 0);
</span></span><span style=display:flex><span>            mService <span style=color:#f92672>=</span> <span style=color:#66d9ef>null</span>;
</span></span><span style=display:flex><span>            <span style=color:#75715e>// 在这里执行重新连接服务的逻辑</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// 获取服务后</span>
</span></span><span style=display:flex><span>mService.<span style=color:#a6e22e>asBinder</span>().<span style=color:#a6e22e>linkToDeath</span>(mDeathRecipient, 0);
</span></span></code></pre></div></li></ul><h3 id=2-binder-事务缓冲区已满>2. Binder 事务缓冲区已满<a hidden class=anchor aria-hidden=true href=#2-binder-事务缓冲区已满>#</a></h3><p>Binder 通信依赖于一块内核管理的共享内存作为缓冲区。虽然 oneway 调用不需要等待 B 进程处理完，但它仍然需要将数据（方法标识符和参数）从 A 进程复制到这个内核缓冲区。</p><p>如果 B 进程非常繁忙，或者有大量的进程（包括 A 进程）在短时间内向 B 进程发送了大量的 Binder 消息（无论是 oneway 还是双向的），就可能导致分配给 B 进程的 Binder 缓冲区被占满。当 A 进程再次尝试发送 oneway 消息时，内核无法为其分配空间，调用就会失败。</p><ul><li><strong>返回错误</strong>: 底层返回 <code>BR_FAILED_REPLY</code> 或 <code>FAILED_TRANSACTION</code>。</li><li><strong>如何排查</strong>:<ul><li>使用 <code>adb shell dumpsys binder</code> 或 <code>adb shell cat /sys/kernel/debug/binder/stats</code> 查看 Binder 的统计信息，关注失败的事务（failed transactions）数量。</li><li>检查 B 进程是否有 ANR（Application Not Responding），如果 B 进程的主线程或 Binder 线程池被阻塞，它就无法及时处理收到的事务，导致缓冲区堆积。</li></ul></li></ul><h3 id=3-事务数据过大>3. 事务数据过大<a hidden class=anchor aria-hidden=true href=#3-事务数据过大>#</a></h3><p>Binder 事务能够承载的数据量是有限的，这个限制通常是 1MB 左右（实际上是整个 Binder 缓冲区的一部分）。如果你尝试在 oneway 调用中传递一个非常大的对象（例如一个巨大的 Bitmap 或 List），超过了这个限制，事务在发送阶段就会失败。</p><ul><li><strong>返回错误</strong>: 底层会返回 <code>TRANSACTION_TOO_LARGE</code> 错误，Java 层会抛出 <code>TransactionTooLargeException</code>。</li><li><strong>如何排查</strong>:<ul><li>在 Logcat 中直接搜索 <code>TransactionTooLargeException</code>。</li><li>检查你通过 oneway 调用传递的数据大小。如果是图片或文件，应考虑使用其他 IPC 方式，如共享内存（Ashmem）或文件描述符（File Descriptor）。</li></ul></li></ul><h3 id=4-目标进程b进程无响应-anr>4. 目标进程（B进程）无响应 (ANR)<a hidden class=anchor aria-hidden=true href=#4-目标进程b进程无响应-anr>#</a></h3><p>即使 B 进程还活着，但如果它的 Binder 线程池中的所有线程都被长时间运行的任务占用了，或者主线程发生了 ANR，那么它就无法处理新的 Binder 请求。这会间接导致第 2 点中提到的“Binder 事务缓冲区已满”问题。ANR问题可能导致线程阻塞，间接导致异步线程处理能力下降。</p><p>对于 oneway 调用来说，A 进程虽然不会被阻塞，但 Binder 驱动发现无法将消息派发给 B 进程的任何一个空闲线程时，可能会导致后续的调用失败（缓冲区满）。</p><ul><li><strong>如何排查</strong>: 在 <code>/data/anr/traces.txt</code> 文件中查找 B 进程的 ANR 日志，分析其线程堆栈，看 Binder 线程是否被卡住。</li></ul><h3 id=5-selinux-权限问题>5. SELinux 权限问题<a hidden class=anchor aria-hidden=true href=#5-selinux-权限问题>#</a></h3><p>在现代 Android 系统中，SELinux（安全增强型 Linux）对进程间的通信有严格的访问控制。如果 A 进程的 SELinux 上下文没有被授予向 B 进程的服务发起 Binder 调用的权限，那么这个调用在内核层面就会被拒绝。</p><ul><li><strong>返回错误</strong>: 通常是 <code>PERMISSION_DENIED</code>。</li><li><strong>如何排查</strong>: 在 Logcat 中过滤 <code>avc: denied</code> 关键字。SELinux 的拒绝日志会清晰地标明源上下文（source context, A进程）、目标上下文（target context, B进程）以及缺少的权限。</li></ul><h3 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h3><p>总的来说，oneway Binder 调用失败的原因可以归结为以下几点：</p><table><thead><tr><th style=text-align:left>失败原因</th><th style=text-align:left>关键错误/日志</th><th style=text-align:left>常见场景</th></tr></thead><tbody><tr><td style=text-align:left><strong>目标进程死亡</strong></td><td style=text-align:left><code>DeadObjectException</code></td><td style=text-align:left>B 进程崩溃或被系统杀死。</td></tr><tr><td style=text-align:left><strong>Binder 缓冲区满</strong></td><td style=text-align:left><code>FAILED_TRANSACTION</code></td><td style=text-align:left>B 进程处理慢，或短时请求过多。</td></tr><tr><td style=text-align:left><strong>传输数据过大</strong></td><td style=text-align:left><code>TransactionTooLargeException</code></td><td style=text-align:left>传递了超过 1MB 的大数据。</td></tr><tr><td style=text-align:left><strong>目标进程无响应</strong></td><td style=text-align:left>B 进程的 ANR 日志</td><td style=text-align:left>B 进程 Binder 线程池或主线程阻塞。</td></tr><tr><td style=text-align:left><strong>SELinux 权限不足</strong></td><td style=text-align:left><code>avc: denied</code></td><td style=text-align:left>缺少正确的 SELinux 策略规则。</td></tr></tbody></table><p>排查时，<strong>首先应该检查 Logcat</strong>，因为绝大多数问题（如进程死亡、数据过大、权限问题）都会在日志中留下明确的线索。如果日志没有直接线索，再考虑是否是由于 B 进程无响应导致的缓冲区溢出。</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ethen-cao.github.io/ethenslab/android-dev/debug/protolog/><span class=title>« Prev</span><br><span>深入解析 Android ProtoLog：高性能结构化日志系统</span>
</a><a class=next href=https://ethen-cao.github.io/ethenslab/android-dev/activitymanager/activity-pip/><span class=title>Next »</span><br><span>Android Internals: Activity Launch into PiP 流程深度解析</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://ethen-cao.github.io/ethenslab/>Ethen 的实验室</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0,theme:"default"})</script><script src=https://cdn.jsdelivr.net/npm/plantuml-encoder@1.4.0/dist/plantuml-encoder.min.js></script><script>(function(){const e=document.querySelectorAll("pre > code.language-plantuml, pre > code.language-planuml");e.forEach(e=>{const s=e.innerText,o=plantumlEncoder.encode(s),i="https://www.plantuml.com/plantuml/svg/"+o,t=document.createElement("img");t.src=i,t.alt="PlantUML Diagram",t.style.maxWidth="100%";const n=e.parentNode;n.parentNode.replaceChild(t,n)})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>