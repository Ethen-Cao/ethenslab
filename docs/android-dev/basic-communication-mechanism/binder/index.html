<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Android Binder 通信模型与内存管理机制详解 | Ethen 的实验室</title><meta name=keywords content><meta name=description content="Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。
本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。
1. 通信模型概述
Binder 驱动通过 ioctl 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。
1.1 同步通信 (Synchronous)
这是 Binder 的默认通信模式。

流程：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。
特性：
强一致性：Client 确切知道 Server 何时处理完毕。
优先级继承：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。
资源限制：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。

同步通信线程调度与优先级继承
在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。
核心机制

阻塞等待：Client 线程调用 ioctl 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。
优先级继承：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。
线程栈管理：Binder 驱动维护了 transaction_stack，用于追踪跨进程调用的嵌套关系（如 A->B->C），确保回复能正确层层返回。

同步通信时序图
下图展示了一个完整的同步调用闭环：Client 发送请求 -> Server 被唤醒处理 -> Server 返回结果 -> Client 被唤醒。"><meta name=author content><link rel=canonical href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/><link crossorigin=anonymous href=/ethenslab/assets/css/stylesheet.a1917769c3c78460b110da6d7905321bb53af4a56f22ba4cc0de824cf4d097ab.css integrity="sha256-oZF3acPHhGCxENpteQUyG7U69KVvIrpMwN6CTPTQl6s=" rel="preload stylesheet" as=style><link rel=icon href=https://ethen-cao.github.io/ethenslab/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethen-cao.github.io/ethenslab/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethen-cao.github.io/ethenslab/favicon-32x32.png><link rel=apple-touch-icon href=https://ethen-cao.github.io/ethenslab/apple-touch-icon.png><link rel=mask-icon href=https://ethen-cao.github.io/ethenslab/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"><meta property="og:site_name" content="Ethen 的实验室"><meta property="og:title" content="Android Binder 通信模型与内存管理机制详解"><meta property="og:description" content="Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。
本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。
1. 通信模型概述 Binder 驱动通过 ioctl 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。
1.1 同步通信 (Synchronous) 这是 Binder 的默认通信模式。
流程：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。 特性： 强一致性：Client 确切知道 Server 何时处理完毕。 优先级继承：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。 资源限制：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。 同步通信线程调度与优先级继承 在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。
核心机制 阻塞等待：Client 线程调用 ioctl 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。 优先级继承：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。 线程栈管理：Binder 驱动维护了 transaction_stack，用于追踪跨进程调用的嵌套关系（如 A->B->C），确保回复能正确层层返回。 同步通信时序图 下图展示了一个完整的同步调用闭环：Client 发送请求 -> Server 被唤醒处理 -> Server 返回结果 -> Client 被唤醒。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="android-dev"><meta property="article:published_time" content="2025-08-28T14:30:02+08:00"><meta property="article:modified_time" content="2025-08-28T14:30:02+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Android Binder 通信模型与内存管理机制详解"><meta name=twitter:description content="Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。
本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。
1. 通信模型概述
Binder 驱动通过 ioctl 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。
1.1 同步通信 (Synchronous)
这是 Binder 的默认通信模式。

流程：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。
特性：
强一致性：Client 确切知道 Server 何时处理完毕。
优先级继承：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。
资源限制：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。

同步通信线程调度与优先级继承
在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。
核心机制

阻塞等待：Client 线程调用 ioctl 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。
优先级继承：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。
线程栈管理：Binder 驱动维护了 transaction_stack，用于追踪跨进程调用的嵌套关系（如 A->B->C），确保回复能正确层层返回。

同步通信时序图
下图展示了一个完整的同步调用闭环：Client 发送请求 -> Server 被唤醒处理 -> Server 返回结果 -> Client 被唤醒。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Android系统开发","item":"https://ethen-cao.github.io/ethenslab/android-dev/"},{"@type":"ListItem","position":2,"name":"基础通信","item":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/"},{"@type":"ListItem","position":3,"name":"Android Binder 通信模型与内存管理机制详解","item":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Android Binder 通信模型与内存管理机制详解","name":"Android Binder 通信模型与内存管理机制详解","description":"Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。\n本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。\n1. 通信模型概述 Binder 驱动通过 ioctl 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。\n1.1 同步通信 (Synchronous) 这是 Binder 的默认通信模式。\n流程：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。 特性： 强一致性：Client 确切知道 Server 何时处理完毕。 优先级继承：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。 资源限制：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。 同步通信线程调度与优先级继承 在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。\n核心机制 阻塞等待：Client 线程调用 ioctl 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。 优先级继承：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。 线程栈管理：Binder 驱动维护了 transaction_stack，用于追踪跨进程调用的嵌套关系（如 A-\u0026gt;B-\u0026gt;C），确保回复能正确层层返回。 同步通信时序图 下图展示了一个完整的同步调用闭环：Client 发送请求 -\u0026gt; Server 被唤醒处理 -\u0026gt; Server 返回结果 -\u0026gt; Client 被唤醒。\n","keywords":[],"articleBody":"Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。\n本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。\n1. 通信模型概述 Binder 驱动通过 ioctl 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。\n1.1 同步通信 (Synchronous) 这是 Binder 的默认通信模式。\n流程：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。 特性： 强一致性：Client 确切知道 Server 何时处理完毕。 优先级继承：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。 资源限制：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。 同步通信线程调度与优先级继承 在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。\n核心机制 阻塞等待：Client 线程调用 ioctl 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。 优先级继承：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。 线程栈管理：Binder 驱动维护了 transaction_stack，用于追踪跨进程调用的嵌套关系（如 A-\u003eB-\u003eC），确保回复能正确层层返回。 同步通信时序图 下图展示了一个完整的同步调用闭环：Client 发送请求 -\u003e Server 被唤醒处理 -\u003e Server 返回结果 -\u003e Client 被唤醒。\n@startuml !theme plain autonumber \"[000]\" hide footbox title Binder Synchronous Transaction Flow \u0026 Priority Inheritance box \"Client Process\" #E1F5FE participant \"ClientThread\\n(High Priority)\" as CT end box box \"Kernel Space (Binder Driver)\" #F5F5F5 participant \"BinderDriver\" as BD participant \"ServerProc\\n(proc-\u003etodo)\" as P_TODO end box box \"Server Process\" #FFF3E0 participant \"ServerThread\\n(Low Priority)\" as ST end box == Phase 0: Server Thread Waiting == note over ST: Server Thread waiting in thread pool ST -\u003e BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u003e BD: binder_thread_read() BD -\u003e BD: binder_wait_for_work() note right of BD: ST sleeps: wait_event_freezable_exclusive deactivate BD ||| == Phase 1: Client Sends Request (BC_TRANSACTION) == note over CT: Client initiates Sync Call CT -\u003e BD: ioctl(BC_TRANSACTION, target=Node_A) activate BD note left of CT: Client BLOCKS here\\nWaiting for BR_REPLY BD -\u003e BD: binder_transaction() note right of BD 1. Find Target: Locate Node_A and ServerProc 2. Alloc Buffer: binder_alloc_new_buf() 3. Copy Data: copy_from_user() end note group Priority Inheritance (关键机制) BD -\u003e BD: Check Priorities note right of BD Client (High) \u003e Server (Low) Boost ServerThread Priority to match Client end note BD -\u003e ST: set_scheduler(ST, High_Prio) end BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn) BD -\u003e BD: binder_wakeup_thread_ilocked(proc, ST) BD -\u003e ST: wake_up_interruptible(\u0026ST-\u003ewait) deactivate BD ||| == Phase 2: Server Processes Request == activate ST ST -\u003e BD: (Wakes up) activate BD BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() BD --\u003e\u003e ST: return BR_TRANSACTION (Data) deactivate BD note over ST Processing... (Running with Client's High Priority) Executes implementation of Node_A end note ||| == Phase 3: Server Sends Reply (BC_REPLY) == ST -\u003e BD: ioctl(BC_REPLY, reply_data) activate BD BD -\u003e BD: binder_transaction() (is_reply=true) note right of BD 1. Pop Stack: Find original Client Txn 2. Copy Data: Copy reply to Client Buffer 3. Restore Priority: Restore ST priority to Low end note BD -\u003e ST: set_scheduler(ST, Low_Prio) BD -\u003e BD: binder_wakeup_thread_ilocked(ClientProc, CT) BD -\u003e CT: wake_up(\u0026CT-\u003ewait) BD --\u003e\u003e ST: return (BC_REPLY handled) deactivate BD note over ST: Server loop continues,\\nwaiting for next work ST -\u003e BD: ioctl(BINDER_WRITE_READ) activate BD deactivate ST BD -\u003e BD: binder_wait_for_work() deactivate BD ||| == Phase 4: Client Wakes Up == activate CT note right of BD: Client wakes up from ioctl BD --\u003e\u003e CT: return BR_REPLY (Result) deactivate CT note over CT: Client unblocks,\\nreceives return value @enduml 时序图逻辑解析 请求发送 (BC_TRANSACTION)： Client 线程调用 ioctl 进入内核。Binder 驱动根据 Handle 找到目标 Server 进程。 驱动执行 优先级继承：检测到 Client 优先级高于 Server，立即提升 Server 目标线程的优先级（在 binder_transaction_priority 函数中实现）。 Client 线程在内核中挂起，等待回复。 服务端处理 (BR_TRANSACTION)： Server 线程被唤醒，从内核缓冲区读取参数。 Server 线程在用户空间执行具体的业务逻辑。此时它持有 Client 的高优先级（如果有提升）。 结果回复 (BC_REPLY)： Server 处理完毕，调用 ioctl 发送 BC_REPLY。 驱动根据 transaction_stack 找到之前被挂起的 Client 线程。 驱动将 Server 线程的优先级恢复到原始状态。 驱动唤醒 Client 线程。 客户端恢复 (BR_REPLY)： Client 线程从 ioctl 返回，拿到 Server 的执行结果，继续执行后续代码。 1.2 异步通信 (Asynchronous / Oneway) 通过在 AIDL 中使用 oneway 关键字或在代码中设置 TF_ONE_WAY 标志触发。\n流程：Client 将数据写入驱动后立即返回，不等待 Server 处理，也不接收返回值。 特性： 高吞吐：适合发送通知、事件分发（如 Input 事件）等对时效性要求高但不需要回执的场景。 串行化保障：这是异步通信极其关键但常被忽略的特性（详见下文）。 无优先级继承：Server 线程以其自身优先级运行。 2. 异步线程管理机制 (核心机制) 很多开发者误认为 Binder 的异步消息是完全并发处理的。然而，深入内核源码 (binder.c) 可知，Binder 驱动对异步消息有着严格的**串行化（Serialization）**策略。\n2.1 调度策略 Binder 驱动在内核层维护了两种队列：\n全局队列 (proc-\u003etodo)：属于进程，用于存放发往该进程不同 Binder 实体的任务。 私有队列 (node-\u003easync_todo)：属于 Binder 实体（Node），用于暂存发往该特定实体的异步任务。 规则如下：\n不同实体，并发处理：如果 Client 分别向 Server 的 Node_A (如 Audio) 和 Node_B (如 Power) 发送异步消息，这两个任务会进入全局队列，Server 线程池会分配两个不同的线程并发处理。 同一实体，串行处理：如果 Client 连续向 Server 的 Node_A 发送多条异步消息，Binder 驱动会检查 Node_A 是否正如忙碌（has_async_transaction == 1）。如果是，后续消息会被强制放入 node-\u003easync_todo 排队，不会唤醒新线程。直到前一条消息处理完毕并释放 Buffer，下一条才会被移动到全局队列等待处理。 2.2 场景演示：串行 vs 并发 为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我们设计了以下场景：\nClient 向 Server 发送 Txn_1 (目标: Node_A)。 -\u003e 立即执行 (ST1) Client 向 Server 发送 Txn_2 (目标: Node_A)。 -\u003e 串行排队 (Serialization) Client 向 Server 发送 Txn_3 (目标: Node_B)。 -\u003e 并发执行 (ST2) ST1 完成 Txn_1，触发 Txn_2 执行。 这个图展示了 proc-\u003etodo（全局队列）与 node-\u003easync_todo（节点私有队列）的交互。\n@startuml !theme plain autonumber \"[000]\" hide footbox title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B) box \"Client Process\" #E1F5FE participant \"ClientThread\" as CT end box box \"Kernel Space (Binder Driver)\" #F5F5F5 participant \"BinderDriver\" as BD participant \"ServerProc\\n(proc-\u003etodo)\" as P_TODO participant \"Node_A\\n(node-\u003easync_todo)\" as NA_TODO participant \"Node_B\\n(node-\u003easync_todo)\" as NB_TODO end box box \"Server Process\" #FFF3E0 participant \"ServerThread_1\\n(ST1)\" as ST1 participant \"ServerThread_2\\n(ST2)\" as ST2 end box == Phase 0: Server Threads Enter Pool (Wait) == note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work ST1 -\u003e BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u003e BD: binder_ioctl_write_read()\\n -\u003e binder_thread_read() BD -\u003e BD: binder_wait_for_work() note right of BD: ST1 sleeps on\\nwait_event_freezable_exclusive(proc-\u003ewait) deactivate BD ST2 -\u003e BD: ioctl(BINDER_WRITE_READ, ...) activate BD BD -\u003e BD: binder_ioctl_write_read()\\n -\u003e binder_thread_read() BD -\u003e BD: binder_wait_for_work() note right of BD: ST2 sleeps on\\nwait_event_freezable_exclusive(proc-\u003ewait) deactivate BD ||| == Phase 1: Txn_1 to Node_A (Success: Wake ST1) == CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_A-\u003ehas_async_transaction == 0 end note BD -\u003e BD: Node_A-\u003ehas_async_transaction = 1 BD -\u003e BD: binder_alloc_new_buf() BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Since !pending_async: Enqueue to Process Global TODO end note BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_1) BD -\u003e BD: binder_select_thread_ilocked(proc) note right of BD Pops ST1 from proc-\u003ewaiting_threads end note BD -\u003e BD: binder_wakeup_thread_ilocked(proc, ST1) BD -\u003e ST1: wake_up_interruptible(\u0026ST1-\u003ewait) BD --\u003e\u003e CT: return (Async returns immediately) deactivate BD activate ST1 ST1 -\u003e BD: (Wakes up inside binder_thread_read) activate BD BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() note left of P_TODO: Txn_1 moved from proc-\u003etodo to ST1 BD --\u003e\u003e ST1: return BR_TRANSACTION (Txn_1 data) deactivate BD note over ST1: ST1 Processing Txn_1 (Node_A) deactivate ST1 ||| == Phase 2: Txn_2 to Node_A (Busy: Queued) == note left of CT: ST1 is still busy with Node_A.\\nClient sends another to Node_A. CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_A-\u003ehas_async_transaction == 1 (Because Txn_1 is active) end note BD -\u003e BD: pending_async = true BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Since pending_async == true: Enqueue to Node Private TODO NO THREAD WOKEN UP! end note BD -\u003e NA_TODO: binder_enqueue_work_ilocked(Txn_2) BD --\u003e\u003e CT: return deactivate BD ||| == Phase 3: Txn_3 to Node_B (Independent: Wake ST2) == note left of CT: ST1 busy with A, Txn_2 queued on A.\\nClient sends to Node_B. CT -\u003e BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B) activate BD BD -\u003e BD: binder_transaction() note right of BD Logic Check: Node_B-\u003ehas_async_transaction == 0 (Node_B is independent) end note BD -\u003e BD: Node_B-\u003ehas_async_transaction = 1 BD -\u003e BD: binder_proc_transaction(target_proc) note right of BD Enqueue to Process Global TODO end note BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_3) BD -\u003e BD: binder_select_thread_ilocked(proc) note right of BD ST1 is not in waiting_threads. Pops ST2 from waiting_threads. end note BD -\u003e BD: binder_wakeup_thread_ilocked(proc, ST2) BD -\u003e ST2: wake_up_interruptible(\u0026ST2-\u003ewait) BD --\u003e\u003e CT: return deactivate BD activate ST2 ST2 -\u003e BD: (Wakes up) activate BD BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() BD --\u003e\u003e ST2: return BR_TRANSACTION (Txn_3 data) deactivate BD note over ST2: ST2 Processing Txn_3 (Node_B) deactivate ST2 ||| == Phase 4: ST1 Finishes Txn_1 \u0026 Triggers Txn_2 == note over ST1: ST1 finishes work,\\ncalls free buffer ST1 -\u003e BD: ioctl(BC_FREE_BUFFER, buffer_ptr) activate BD BD -\u003e BD: binder_thread_write() -\u003e binder_free_buf() note right of BD Logic Check: buffer-\u003easync_transaction \u0026\u0026 buffer-\u003etarget_node end note BD -\u003e NA_TODO: binder_dequeue_work_head_ilocked() note right of NA_TODO Found Txn_2 waiting! end note alt Work Found in Async_Todo BD -\u003e P_TODO: binder_enqueue_work_ilocked(Txn_2) note right of P_TODO Move Txn_2 from Node_A private queue to Global Process queue end note BD -\u003e BD: binder_wakeup_proc_ilocked(proc) note right of BD Wake up any available thread. ST1 is currently in ioctl, so it will likely loop back and pick this up immediately in binder_thread_read. end note else No Work Found BD -\u003e BD: buf_node-\u003ehas_async_transaction = false end BD --\u003e\u003e ST1: return deactivate BD ST1 -\u003e BD: ioctl(BINDER_WRITE_READ, read...) activate BD BD -\u003e BD: binder_thread_read() BD -\u003e P_TODO: binder_dequeue_work_head_ilocked() note right of P_TODO: Picking up Txn_2 BD --\u003e\u003e ST1: return BR_TRANSACTION (Txn_2 data) deactivate BD note over ST1: ST1 Processing Txn_2 (Node_A) @enduml 2.3 两级队列机制详解 Binder 驱动之所以在异步通信（oneway）中引入两级队列机制（proc-\u003etodo 和 node-\u003easync_todo），是为了解决一对核心矛盾：\n多线程并发处理（高吞吐） VS 消息严格顺序性（逻辑正确性）\n如果只用一个全局队列，或者只用私有队列，都会导致严重的问题。下面详细拆解为什么要这么设计。\n1. 核心原因：保证“同一个Binder实体”的消息顺序 这是最根本的原因。异步消息（oneway）虽然发送端不等待，但在业务逻辑上通常要求**“发送顺序等于执行顺序”**。\n假设场景：你写了一个 View，先发送 Show()，紧接着发送 UpdateContent()。\n期望：先显示窗口，再更新内容。 如果没有 node-\u003easync_todo（只有全局队列 proc-\u003etodo）： Show() 进入全局队列。 UpdateContent() 进入全局队列。 Server 的 Thread A 抢到了 Show()。 Server 的 Thread B 抢到了 UpdateContent()。 由于线程调度是不确定的，Thread B 可能比 Thread A 先执行完。 后果：系统试图在一个还没显示的窗口上更新内容，导致 crash 或逻辑错误。 引入 node-\u003easync_todo 后： Binder 驱动强制规定：对于同一个 Node，同一时刻只能有一个异步任务在全局队列中。后续任务必须在 node-\u003easync_todo 中排队，直到前一个任务执行完释放。这就物理上杜绝了乱序执行的可能性。\n2. 次要原因：防止“队头阻塞”与资源独占 (公平性) 这是为了系统的稳定性和公平性考虑。\n假设场景：\nClient A 瞬间向 Service A（Node A）发送了 1000 个异步消息（例如疯狂打 Log 或刷屏）。 Client B 向 Service B（Node B）发送了 1 个重要的异步消息（例如“接听电话”）。 如果没有 node-\u003easync_todo（所有消息都进全局 proc-\u003etodo）：\n全局队列里塞满了 1000 个 Node A 的消息。 Node B 的消息排在第 1001 位。 Server 的线程池（假设 16 个线程）全部被唤醒去处理 Node A 的消息。 后果：Node B 的消息被严重延迟处理，导致“接电话”卡顿。这就是典型的**队头阻塞（Head-of-line blocking）或邻居噪声（Noisy Neighbor）**问题。 引入 node-\u003easync_todo 后：\nNode A 的第 1 个消息进入全局队列 proc-\u003etodo。 Node A 的第 2~1000 个消息进入私有队列 node-\u003easync_todo。 Node B 的消息进入全局队列 proc-\u003etodo。 结果：全局队列里只有 2 个任务（Node A 的任务1，Node B 的任务1）。 Server 线程池会分配两个线程，并发处理 Node A 和 Node B。 收益：Node A 的洪水攻击不会淹没 Node B 的正常请求，实现了进程级别的任务公平调度。 3. 形象比喻：银行柜台模型 为了方便理解，我们可以把 SystemServer 比作一个银行，Binder 线程是柜员。\nproc-\u003etodo (全局队列)：银行大厅的叫号屏幕。所有柜员看着屏幕，有号就叫。 node-\u003easync_todo (私有队列)：某个具体客户（Binder实体）手里的待办单据。 工作流程：\n客户 A (SurfaceFlinger) 拿了一叠单据（10 个 updateInput）进来了。 规则：银行规定，一个客户一次只能办一张单据，办完这张才能取下一张的号。 Client A 取了一个号（进入 proc-\u003etodo），手里捏着剩下 9 张单据（留在 node-\u003easync_todo）。 Client B (Audio) 进来，拿了一张单据，也取了一个号（进入 proc-\u003etodo）。 柜员 1 叫到了 Client A，开始办业务。 柜员 2 叫到了 Client B，开始办业务。（并发执行，互不影响） Client A 的第一张单据办完，柜员 1 问：“还有吗？” A 说：“有”。于是柜员 1 帮 A 重新取个号（把下一张单据从 async 移到 proc），等待下一个柜员叫号。 4. 总结：两级队列的各自职责 队列类型 proc-\u003etodo (全局队列) node-\u003easync_todo (私有队列) 层级 进程级 (Process Level) 对象级 (Binder Node Level) 可见性 所有 Binder 线程可见 仅内核驱动可见，线程不可直接获取 核心职责 分发任务：作为线程池的抢占池，实现跨服务的并发。 缓冲与排序：暂存特定服务的后续任务，保证顺序，防止洪水。 处理逻辑 FIFO（先进先出），任何空闲线程可取。 严格串行，必须等前一个任务 BC_FREE_BUFFER 后才释放。 解决痛点 解决“谁来干活”的问题。 解决“顺序错乱”和“资源独占”的问题。 正是因为这两个队列的配合，Binder 才能做到：既能让 Audio 和 Input 并行处理（走全局队列），又能保证 Input 的每一帧严格按顺序刷新（走私有队列）。\n2.4 结论与风险 Binder 并没有为异步消息分配“专用”线程，所有 Binder 线程都是通用的。但是，针对同一目标对象的连续异步调用会退化为单线程处理。\n风险：如果 Client（如 SurfaceFlinger）向 Server（如 SystemServer）的同一个接口疯狂发送大包，且 Server 端因锁竞争导致处理变慢，内核缓冲区会迅速积压，最终导致 binder_alloc 失败（-28 ENOSPC）。\n3. 内存管理机制 Binder 驱动通过 mmap 和页表修改实现了高效的“一次拷贝”内存管理，避免了传统 IPC 的两次拷贝开销。\n3.1 内存映射流程 建立 VMA (binder_mmap)： 进程启动时打开 /dev/binder，驱动为其分配一块虚拟地址空间（通常 1MB - 8MB），但不分配物理内存。 这块区域专门用于接收 Binder 数据。 分配与映射 (binder_alloc_buf)： 当事务发生时，驱动从内核通用物理内存池（vmalloc/alloc_page）申请物理页。 关键步骤：驱动修改目标进程的页表，将这块物理内存直接映射到目标进程步骤 1 中建立的虚拟地址空间中。 目标进程的用户空间线程可以直接读取这块内存。 释放 (BC_FREE_BUFFER)： Server 处理完数据后，必须通知驱动释放。驱动解除映射并归还物理页。 3.2 内存配额 (Quota) 为了防止异步消息滥用导致内存耗尽（DoS 攻击），Binder 实施了严格的配额管理：\n异步空间限制 (free_async_space)：\n每个进程的异步事务总内存被限制为 Binder 总 Buffer 大小的 50%。\n分配检查：binder_alloc_buf 时，如果是 oneway 调用，驱动会检查 current_async_size + new_size \u003c= total_buffer / 2。\n如果超过配额，分配失败，客户端收到错误（通常是静默丢弃或报错）。\n同步事务：\n不受 50% 配额限制，只受限于总缓冲区大小。\n依赖同步调用的阻塞特性形成天然的背压（Back-pressure）。\n4. 常见问题排查 (Debug) 在系统开发中，Binder 异常通常表现为 Crash 或 Log 报错。\n4.1 常见错误代码 错误类型 现象描述 可能原因 排查方向 DEAD_OBJECT DeadObjectException 目标进程（Server）已崩溃或被杀。 检查 Logcat 中 Server 进程的死亡日志。 FAILED_TRANSACTION 底层返回 -28 (ENOSPC) Binder 缓冲区耗尽。 1. 检查 Server 是否 ANR 或死锁。\n2. 检查是否有 Oneway Spam（如 SF 发送大包）。\n3. 检查 Server 线程池是否耗尽。 TRANSACTION_TOO_LARGE TransactionTooLargeException 单次传输数据超过限制（约 1MB/2）。 检查传输的数据结构（如大 Bitmap、长列表）。 BR_FAILED_REPLY IPC 调用失败 目标进程拒绝处理或处理出错。 检查 SELinux 权限 (avc: denied)。 4.2 深度案例：缓冲区满但 Server 无响应 现象：binder_alloc 报错 pid XXX spamming oneway?，缓冲区满，但 Server 进程看起来没有崩溃。\n深度分析逻辑：\n线程饥饿 (Starvation)：Server 的 Binder 线程池可能被其他高频、耗时的任务（如 checkPermission、ContentProvider 查询）占满。 锁竞争 (Lock Contention)：Server 线程虽然拿到了 Binder 任务，但卡在了内部锁（如 WindowManagerGlobalLock 或 InputDispatcher mLock）上，导致无法完成处理并释放 Buffer。 串行化阻塞：如本文 2.2 节所述，如果发给同一个对象，即使有空闲线程，驱动也不会调度，导致数据在内核积压。 解决方案：\n优化 Server 端锁的粒度。 Client 端避免短时间发送大量 Oneway 大包。 排查 Server 端是否存在高频调用占用了所有 Binder 线程。 ","wordCount":"1743","inLanguage":"en","datePublished":"2025-08-28T14:30:02+08:00","dateModified":"2025-08-28T14:30:02+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/binder/"},"publisher":{"@type":"Organization","name":"Ethen 的实验室","logo":{"@type":"ImageObject","url":"https://ethen-cao.github.io/ethenslab/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethen-cao.github.io/ethenslab/ accesskey=h title="Ethen 的实验室 (Alt + H)">Ethen 的实验室</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethen-cao.github.io/ethenslab/android-dev/ title=Android系统开发><span>Android系统开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/android-automotive-os-dev/ title="Android Automotive"><span>Android Automotive</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/qnx/ title=QNX开发><span>QNX开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/gunyah/ title=Gunyah><span>Gunyah</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/ivi-solution/ title=智能座舱方案><span>智能座舱方案</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/explore-ai title="Explore AI"><span>Explore AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethen-cao.github.io/ethenslab/>Home</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/android-dev/>Android系统开发</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/android-dev/basic-communication-mechanism/>基础通信</a></div><h1 class="post-title entry-hint-parent">Android Binder 通信模型与内存管理机制详解</h1><div class=post-meta><span title='2025-08-28 14:30:02 +0800 CST'>August 28, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1743 words</div></header><div class=post-content><p>Binder 是 Android 系统中进程间通信（IPC）的基石。不同于 Linux 传统的管道、Socket 或共享内存，Binder 提供了一种基于 C/S 架构、支持对象传输且内存高效的通信机制。</p><p>本文档将深入剖析 Binder 的同步/异步通信模型、线程调度策略以及内核层的内存管理机制。</p><h2 id=1-通信模型概述>1. 通信模型概述<a hidden class=anchor aria-hidden=true href=#1-通信模型概述>#</a></h2><p>Binder 驱动通过 <code>ioctl</code> 系统调用与用户空间交互。根据事务标志位（Flags），通信模式主要分为两类。</p><h3 id=11-同步通信-synchronous>1.1 同步通信 (Synchronous)<a hidden class=anchor aria-hidden=true href=#11-同步通信-synchronous>#</a></h3><p>这是 Binder 的默认通信模式。</p><ul><li><strong>流程</strong>：Client 发起调用后，其线程会被挂起（Blocked），进入等待状态。Binder 驱动唤醒 Server 端线程处理请求。Server 处理完毕并将结果写入驱动后，驱动唤醒 Client 线程，Client 获取返回值继续执行。</li><li><strong>特性</strong>：</li><li><strong>强一致性</strong>：Client 确切知道 Server 何时处理完毕。</li><li><strong>优先级继承</strong>：为防止优先级反转，驱动会将 Client 线程的优先级临时“借”给 Server 线程。</li><li><strong>资源限制</strong>：受限于调用栈深度和线程池大小，过多的同步链调用可能导致死锁或 watchdog 超时。</li></ul><h4 id=同步通信线程调度与优先级继承>同步通信线程调度与优先级继承<a hidden class=anchor aria-hidden=true href=#同步通信线程调度与优先级继承>#</a></h4><p>在同步通信（Synchronous IPC）中，Client 端发起请求后会进入阻塞状态（Blocked），直到 Server 端返回结果。Binder 驱动在此过程中扮演了“调度员”的角色，不仅负责数据的传递，还负责协调双方线程的执行状态和优先级。</p><h5 id=核心机制>核心机制<a hidden class=anchor aria-hidden=true href=#核心机制>#</a></h5><ol><li><strong>阻塞等待</strong>：Client 线程调用 <code>ioctl</code> 发送事务后，会在内核中休眠，等待 Server 的回复（Reply）。</li><li><strong>优先级继承</strong>：如果 Client 的优先级高于 Server 线程，Binder 驱动会将 Server 线程的优先级临时提升至与 Client 一致，以避免“优先级反转”问题。当 Server 处理完请求后，恢复原优先级。</li><li><strong>线程栈管理</strong>：Binder 驱动维护了 <code>transaction_stack</code>，用于追踪跨进程调用的嵌套关系（如 A->B->C），确保回复能正确层层返回。</li></ol><h5 id=同步通信时序图>同步通信时序图<a hidden class=anchor aria-hidden=true href=#同步通信时序图>#</a></h5><p>下图展示了一个完整的同步调用闭环：Client 发送请求 -> Server 被唤醒处理 -> Server 返回结果 -> Client 被唤醒。</p><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
autonumber &#34;&lt;b&gt;[000]&#34;
hide footbox

title Binder Synchronous Transaction Flow &amp; Priority Inheritance

box &#34;Client Process&#34; #E1F5FE
    participant &#34;ClientThread\n(High Priority)&#34; as CT
end box

box &#34;Kernel Space (Binder Driver)&#34; #F5F5F5
    participant &#34;BinderDriver&#34; as BD
    participant &#34;ServerProc\n(proc-&gt;todo)&#34; as P_TODO
end box

box &#34;Server Process&#34; #FFF3E0
    participant &#34;ServerThread\n(Low Priority)&#34; as ST
end box

== Phase 0: Server Thread Waiting ==

note over ST: Server Thread waiting in thread pool
ST -&gt; BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -&gt; BD: binder_thread_read()
BD -&gt; BD: binder_wait_for_work()
note right of BD: ST sleeps: wait_event_freezable_exclusive
deactivate BD

|||

== Phase 1: Client Sends Request (BC_TRANSACTION) ==

note over CT: Client initiates Sync Call
CT -&gt; BD: ioctl(BC_TRANSACTION, target=Node_A)
activate BD
note left of CT: Client BLOCKS here\nWaiting for BR_REPLY

BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;1. Find Target:&lt;/b&gt; Locate Node_A and ServerProc
  &lt;b&gt;2. Alloc Buffer:&lt;/b&gt; binder_alloc_new_buf()
  &lt;b&gt;3. Copy Data:&lt;/b&gt; copy_from_user()
end note

group Priority Inheritance (关键机制)
    BD -&gt; BD: Check Priorities
    note right of BD
      Client (High) &gt; Server (Low)
      &lt;b&gt;Boost ServerThread Priority&lt;/b&gt;
      to match Client
    end note
    BD -&gt; ST: set_scheduler(ST, High_Prio)
end

BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn)
BD -&gt; BD: binder_wakeup_thread_ilocked(proc, ST)
BD -&gt; ST: wake_up_interruptible(&amp;ST-&gt;wait)

deactivate BD

|||

== Phase 2: Server Processes Request ==

activate ST
ST -&gt; BD: (Wakes up)
activate BD
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
BD --&gt;&gt; ST: return BR_TRANSACTION (Data)
deactivate BD

note over ST
  &lt;b&gt;Processing...&lt;/b&gt;
  (Running with Client&#39;s High Priority)
  Executes implementation of Node_A
end note

|||

== Phase 3: Server Sends Reply (BC_REPLY) ==

ST -&gt; BD: ioctl(BC_REPLY, reply_data)
activate BD
BD -&gt; BD: binder_transaction() (is_reply=true)

note right of BD
  &lt;b&gt;1. Pop Stack:&lt;/b&gt; Find original Client Txn
  &lt;b&gt;2. Copy Data:&lt;/b&gt; Copy reply to Client Buffer
  &lt;b&gt;3. Restore Priority:&lt;/b&gt;
     Restore ST priority to Low
end note
BD -&gt; ST: set_scheduler(ST, Low_Prio)

BD -&gt; BD: binder_wakeup_thread_ilocked(ClientProc, CT)
BD -&gt; CT: wake_up(&amp;CT-&gt;wait)

BD --&gt;&gt; ST: return (BC_REPLY handled)
deactivate BD

note over ST: Server loop continues,\nwaiting for next work
ST -&gt; BD: ioctl(BINDER_WRITE_READ)
activate BD
deactivate ST
BD -&gt; BD: binder_wait_for_work()
deactivate BD

|||

== Phase 4: Client Wakes Up ==

activate CT
note right of BD: Client wakes up from ioctl
BD --&gt;&gt; CT: return BR_REPLY (Result)
deactivate CT

note over CT: Client unblocks,\nreceives return value
@enduml
</code></pre><h4 id=时序图逻辑解析>时序图逻辑解析<a hidden class=anchor aria-hidden=true href=#时序图逻辑解析>#</a></h4><ol><li><strong>请求发送 (<code>BC_TRANSACTION</code>)</strong>：</li></ol><ul><li>Client 线程调用 <code>ioctl</code> 进入内核。Binder 驱动根据 Handle 找到目标 Server 进程。</li><li>驱动执行 <strong>优先级继承</strong>：检测到 Client 优先级高于 Server，立即提升 Server 目标线程的优先级（在 <code>binder_transaction_priority</code> 函数中实现）。</li><li>Client 线程在内核中挂起，等待回复。</li></ul><ol start=2><li><strong>服务端处理 (<code>BR_TRANSACTION</code>)</strong>：</li></ol><ul><li>Server 线程被唤醒，从内核缓冲区读取参数。</li><li>Server 线程在用户空间执行具体的业务逻辑。此时它持有 Client 的高优先级（如果有提升）。</li></ul><ol start=3><li><strong>结果回复 (<code>BC_REPLY</code>)</strong>：</li></ol><ul><li>Server 处理完毕，调用 <code>ioctl</code> 发送 <code>BC_REPLY</code>。</li><li>驱动根据 <code>transaction_stack</code> 找到之前被挂起的 Client 线程。</li><li>驱动将 Server 线程的优先级<strong>恢复</strong>到原始状态。</li><li>驱动唤醒 Client 线程。</li></ul><ol start=4><li><strong>客户端恢复 (<code>BR_REPLY</code>)</strong>：</li></ol><ul><li>Client 线程从 <code>ioctl</code> 返回，拿到 Server 的执行结果，继续执行后续代码。</li></ul><h3 id=12-异步通信-asynchronous--oneway>1.2 异步通信 (Asynchronous / Oneway)<a hidden class=anchor aria-hidden=true href=#12-异步通信-asynchronous--oneway>#</a></h3><p>通过在 AIDL 中使用 <code>oneway</code> 关键字或在代码中设置 <code>TF_ONE_WAY</code> 标志触发。</p><ul><li><strong>流程</strong>：Client 将数据写入驱动后立即返回，<strong>不等待</strong> Server 处理，也<strong>不接收</strong>返回值。</li><li><strong>特性</strong>：</li><li><strong>高吞吐</strong>：适合发送通知、事件分发（如 Input 事件）等对时效性要求高但不需要回执的场景。</li><li><strong>串行化保障</strong>：这是异步通信极其关键但常被忽略的特性（详见下文）。</li><li><strong>无优先级继承</strong>：Server 线程以其自身优先级运行。</li></ul><hr><h2 id=2-异步线程管理机制-核心机制>2. 异步线程管理机制 (核心机制)<a hidden class=anchor aria-hidden=true href=#2-异步线程管理机制-核心机制>#</a></h2><p>很多开发者误认为 Binder 的异步消息是完全并发处理的。然而，深入内核源码 (<code>binder.c</code>) 可知，Binder 驱动对异步消息有着严格的**串行化（Serialization）**策略。</p><h3 id=21-调度策略>2.1 调度策略<a hidden class=anchor aria-hidden=true href=#21-调度策略>#</a></h3><p>Binder 驱动在内核层维护了两种队列：</p><ol><li><strong>全局队列 (<code>proc->todo</code>)</strong>：属于进程，用于存放发往该进程不同 Binder 实体的任务。</li><li><strong>私有队列 (<code>node->async_todo</code>)</strong>：属于 Binder 实体（Node），用于暂存发往该特定实体的异步任务。</li></ol><p><strong>规则如下：</strong></p><ul><li><strong>不同实体，并发处理</strong>：如果 Client 分别向 Server 的 <code>Node_A</code> (如 Audio) 和 <code>Node_B</code> (如 Power) 发送异步消息，这两个任务会进入全局队列，Server 线程池会分配<strong>两个不同的线程</strong>并发处理。</li><li><strong>同一实体，串行处理</strong>：如果 Client 连续向 Server 的 <code>Node_A</code> 发送多条异步消息，Binder 驱动会检查 <code>Node_A</code> 是否正如忙碌（<code>has_async_transaction == 1</code>）。如果是，后续消息会被强制放入 <code>node->async_todo</code> 排队，<strong>不会唤醒新线程</strong>。直到前一条消息处理完毕并释放 Buffer，下一条才会被移动到全局队列等待处理。</li></ul><h3 id=22-场景演示串行-vs-并发>2.2 场景演示：串行 vs 并发<a hidden class=anchor aria-hidden=true href=#22-场景演示串行-vs-并发>#</a></h3><p>为了清晰地展示 Binder 驱动的线程选择策略和并发/串行逻辑，我们设计了以下场景：</p><ol><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_1</strong> (目标: Node_A)。 -> <strong>立即执行 (ST1)</strong></li><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_2</strong> (目标: Node_A)。 -> <strong>串行排队 (Serialization)</strong></li><li><strong>Client</strong> 向 <strong>Server</strong> 发送 <strong>Txn_3</strong> (目标: Node_B)。 -> <strong>并发执行 (ST2)</strong></li><li><strong>ST1</strong> 完成 Txn_1，触发 Txn_2 执行。</li></ol><p>这个图展示了 <code>proc->todo</code>（全局队列）与 <code>node->async_todo</code>（节点私有队列）的交互。</p><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
autonumber &#34;&lt;b&gt;[000]&#34;
hide footbox

title Binder Async Threading Model: Serialization (Node_A) vs Concurrency (Node_B)

box &#34;Client Process&#34; #E1F5FE
    participant &#34;ClientThread&#34; as CT
end box

box &#34;Kernel Space (Binder Driver)&#34; #F5F5F5
    participant &#34;BinderDriver&#34; as BD
    participant &#34;ServerProc\n(proc-&gt;todo)&#34; as P_TODO
    participant &#34;Node_A\n(node-&gt;async_todo)&#34; as NA_TODO
    participant &#34;Node_B\n(node-&gt;async_todo)&#34; as NB_TODO
end box

box &#34;Server Process&#34; #FFF3E0
    participant &#34;ServerThread_1\n(ST1)&#34; as ST1
    participant &#34;ServerThread_2\n(ST2)&#34; as ST2
end box

== Phase 0: Server Threads Enter Pool (Wait) ==

note over ST1, ST2: ST1 and ST2 enter the thread pool to wait for work

ST1 -&gt; BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -&gt; BD: binder_ioctl_write_read()\n -&gt; binder_thread_read()
BD -&gt; BD: binder_wait_for_work()
note right of BD: ST1 sleeps on\nwait_event_freezable_exclusive(proc-&gt;wait)
deactivate BD

ST2 -&gt; BD: ioctl(BINDER_WRITE_READ, ...)
activate BD
BD -&gt; BD: binder_ioctl_write_read()\n -&gt; binder_thread_read()
BD -&gt; BD: binder_wait_for_work()
note right of BD: ST2 sleeps on\nwait_event_freezable_exclusive(proc-&gt;wait)
deactivate BD

|||

== Phase 1: Txn_1 to Node_A (Success: Wake ST1) ==

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_A-&gt;has_async_transaction == 0
end note

BD -&gt; BD: Node_A-&gt;has_async_transaction = 1
BD -&gt; BD: binder_alloc_new_buf()

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Since !pending_async:
  Enqueue to Process Global TODO
end note
BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_1)

BD -&gt; BD: binder_select_thread_ilocked(proc)
note right of BD
  Pops ST1 from 
  proc-&gt;waiting_threads
end note

BD -&gt; BD: binder_wakeup_thread_ilocked(proc, ST1)
BD -&gt; ST1: wake_up_interruptible(&amp;ST1-&gt;wait)

BD --&gt;&gt; CT: return (Async returns immediately)
deactivate BD

activate ST1
ST1 -&gt; BD: (Wakes up inside binder_thread_read)
activate BD
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
note left of P_TODO: Txn_1 moved from proc-&gt;todo to ST1
BD --&gt;&gt; ST1: return BR_TRANSACTION (Txn_1 data)
deactivate BD
note over ST1: &lt;b&gt;ST1 Processing Txn_1 (Node_A)&lt;/b&gt;
deactivate ST1

|||

== Phase 2: Txn_2 to Node_A (Busy: Queued) ==

note left of CT: ST1 is still busy with Node_A.\nClient sends another to Node_A.

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_A)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_A-&gt;has_async_transaction == 1
  (Because Txn_1 is active)
end note

BD -&gt; BD: pending_async = true

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Since pending_async == true:
  Enqueue to Node Private TODO
  &lt;b&gt;NO THREAD WOKEN UP!&lt;/b&gt;
end note
BD -&gt; NA_TODO: binder_enqueue_work_ilocked(Txn_2)

BD --&gt;&gt; CT: return
deactivate BD

|||

== Phase 3: Txn_3 to Node_B (Independent: Wake ST2) ==

note left of CT: ST1 busy with A, Txn_2 queued on A.\nClient sends to &lt;b&gt;Node_B&lt;/b&gt;.

CT -&gt; BD: ioctl(BC_TRANSACTION | TF_ONEWAY, target=Node_B)
activate BD
BD -&gt; BD: binder_transaction()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  Node_B-&gt;has_async_transaction == 0
  (Node_B is independent)
end note

BD -&gt; BD: Node_B-&gt;has_async_transaction = 1

BD -&gt; BD: binder_proc_transaction(target_proc)
note right of BD
  Enqueue to Process Global TODO
end note
BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_3)

BD -&gt; BD: binder_select_thread_ilocked(proc)
note right of BD
  ST1 is not in waiting_threads.
  Pops ST2 from waiting_threads.
end note

BD -&gt; BD: binder_wakeup_thread_ilocked(proc, ST2)
BD -&gt; ST2: wake_up_interruptible(&amp;ST2-&gt;wait)

BD --&gt;&gt; CT: return
deactivate BD

activate ST2
ST2 -&gt; BD: (Wakes up)
activate BD
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
BD --&gt;&gt; ST2: return BR_TRANSACTION (Txn_3 data)
deactivate BD
note over ST2: &lt;b&gt;ST2 Processing Txn_3 (Node_B)&lt;/b&gt;
deactivate ST2

|||

== Phase 4: ST1 Finishes Txn_1 &amp; Triggers Txn_2 ==

note over ST1: ST1 finishes work,\ncalls free buffer

ST1 -&gt; BD: ioctl(BC_FREE_BUFFER, buffer_ptr)
activate BD
BD -&gt; BD: binder_thread_write() -&gt; binder_free_buf()

note right of BD
  &lt;b&gt;Logic Check:&lt;/b&gt;
  buffer-&gt;async_transaction &amp;&amp; buffer-&gt;target_node
end note

BD -&gt; NA_TODO: binder_dequeue_work_head_ilocked()
note right of NA_TODO
  Found Txn_2 waiting!
end note

alt Work Found in Async_Todo
    BD -&gt; P_TODO: binder_enqueue_work_ilocked(Txn_2)
    note right of P_TODO
      Move Txn_2 from Node_A private queue
      to Global Process queue
    end note
    
    BD -&gt; BD: binder_wakeup_proc_ilocked(proc)
    note right of BD
      &lt;b&gt;Wake up any available thread.&lt;/b&gt;
      ST1 is currently in ioctl, so it will likely
      loop back and pick this up immediately
      in binder_thread_read.
    end note
else No Work Found
    BD -&gt; BD: buf_node-&gt;has_async_transaction = false
end

BD --&gt;&gt; ST1: return
deactivate BD

ST1 -&gt; BD: ioctl(BINDER_WRITE_READ, read...)
activate BD
BD -&gt; BD: binder_thread_read()
BD -&gt; P_TODO: binder_dequeue_work_head_ilocked()
note right of P_TODO: Picking up Txn_2
BD --&gt;&gt; ST1: return BR_TRANSACTION (Txn_2 data)
deactivate BD

note over ST1: &lt;b&gt;ST1 Processing Txn_2 (Node_A)&lt;/b&gt;

@enduml
</code></pre><h3 id=23-两级队列机制详解>2.3 两级队列机制详解<a hidden class=anchor aria-hidden=true href=#23-两级队列机制详解>#</a></h3><p>Binder 驱动之所以在异步通信（oneway）中引入<strong>两级队列机制</strong>（<code>proc->todo</code> 和 <code>node->async_todo</code>），是为了解决一对核心矛盾：</p><p><strong>多线程并发处理（高吞吐） VS 消息严格顺序性（逻辑正确性）</strong></p><p>如果只用一个全局队列，或者只用私有队列，都会导致严重的问题。下面详细拆解为什么要这么设计。</p><h4 id=1-核心原因保证同一个binder实体的消息顺序>1. 核心原因：保证“同一个Binder实体”的消息顺序<a hidden class=anchor aria-hidden=true href=#1-核心原因保证同一个binder实体的消息顺序>#</a></h4><p>这是最根本的原因。异步消息（oneway）虽然发送端不等待，但在业务逻辑上通常要求**“发送顺序等于执行顺序”**。</p><p><strong>假设场景</strong>：你写了一个 View，先发送 <code>Show()</code>，紧接着发送 <code>UpdateContent()</code>。</p><ul><li><strong>期望</strong>：先显示窗口，再更新内容。</li><li><strong>如果没有 <code>node->async_todo</code>（只有全局队列 <code>proc->todo</code>）</strong>：</li></ul><ol><li><code>Show()</code> 进入全局队列。</li><li><code>UpdateContent()</code> 进入全局队列。</li><li>Server 的 <strong>Thread A</strong> 抢到了 <code>Show()</code>。</li><li>Server 的 <strong>Thread B</strong> 抢到了 <code>UpdateContent()</code>。</li><li>由于线程调度是不确定的，<strong>Thread B 可能比 Thread A 先执行完</strong>。</li><li><strong>后果</strong>：系统试图在一个还没显示的窗口上更新内容，导致 crash 或逻辑错误。</li></ol><p><strong>引入 <code>node->async_todo</code> 后</strong>：
Binder 驱动强制规定：<strong>对于同一个 Node，同一时刻只能有一个异步任务在全局队列中</strong>。后续任务必须在 <code>node->async_todo</code> 中排队，直到前一个任务执行完释放。这就物理上杜绝了乱序执行的可能性。</p><h4 id=2-次要原因防止队头阻塞与资源独占-公平性>2. 次要原因：防止“队头阻塞”与资源独占 (公平性)<a hidden class=anchor aria-hidden=true href=#2-次要原因防止队头阻塞与资源独占-公平性>#</a></h4><p>这是为了系统的稳定性和公平性考虑。</p><p><strong>假设场景</strong>：</p><ul><li><strong>Client A</strong> 瞬间向 <strong>Service A</strong>（Node A）发送了 1000 个异步消息（例如疯狂打 Log 或刷屏）。</li><li><strong>Client B</strong> 向 <strong>Service B</strong>（Node B）发送了 1 个重要的异步消息（例如“接听电话”）。</li></ul><p><strong>如果没有 <code>node->async_todo</code>（所有消息都进全局 <code>proc->todo</code>）</strong>：</p><ol><li>全局队列里塞满了 1000 个 Node A 的消息。</li><li>Node B 的消息排在第 1001 位。</li><li>Server 的线程池（假设 16 个线程）全部被唤醒去处理 Node A 的消息。</li><li><strong>后果</strong>：Node B 的消息被严重延迟处理，导致“接电话”卡顿。这就是典型的**队头阻塞（Head-of-line blocking）<strong>或</strong>邻居噪声（Noisy Neighbor）**问题。</li></ol><p><strong>引入 <code>node->async_todo</code> 后</strong>：</p><ol><li>Node A 的第 1 个消息进入全局队列 <code>proc->todo</code>。</li><li>Node A 的第 2~1000 个消息进入私有队列 <code>node->async_todo</code>。</li><li>Node B 的消息进入全局队列 <code>proc->todo</code>。</li><li><strong>结果</strong>：全局队列里只有 2 个任务（Node A 的任务1，Node B 的任务1）。</li><li>Server 线程池会分配两个线程，<strong>并发处理</strong> Node A 和 Node B。</li><li><strong>收益</strong>：Node A 的洪水攻击不会淹没 Node B 的正常请求，实现了<strong>进程级别的任务公平调度</strong>。</li></ol><h4 id=3-形象比喻银行柜台模型>3. 形象比喻：银行柜台模型<a hidden class=anchor aria-hidden=true href=#3-形象比喻银行柜台模型>#</a></h4><p>为了方便理解，我们可以把 SystemServer 比作一个<strong>银行</strong>，Binder 线程是<strong>柜员</strong>。</p><ul><li><strong><code>proc->todo</code> (全局队列)</strong>：银行大厅的<strong>叫号屏幕</strong>。所有柜员看着屏幕，有号就叫。</li><li><strong><code>node->async_todo</code> (私有队列)</strong>：某个<strong>具体客户（Binder实体）手里的待办单据</strong>。</li></ul><p><strong>工作流程</strong>：</p><ol><li><strong>客户 A (SurfaceFlinger)</strong> 拿了一叠单据（10 个 updateInput）进来了。</li><li><strong>规则</strong>：银行规定，一个客户一次只能办一张单据，办完这张才能取下一张的号。</li><li><strong>Client A</strong> 取了一个号（进入 <code>proc->todo</code>），手里捏着剩下 9 张单据（留在 <code>node->async_todo</code>）。</li><li><strong>Client B (Audio)</strong> 进来，拿了一张单据，也取了一个号（进入 <code>proc->todo</code>）。</li><li><strong>柜员 1</strong> 叫到了 Client A，开始办业务。</li><li><strong>柜员 2</strong> 叫到了 Client B，开始办业务。<strong>（并发执行，互不影响）</strong></li><li><strong>Client A</strong> 的第一张单据办完，柜员 1 问：“还有吗？” A 说：“有”。于是柜员 1 帮 A 重新取个号（把下一张单据从 <code>async</code> 移到 <code>proc</code>），等待下一个柜员叫号。</li></ol><h4 id=4-总结两级队列的各自职责>4. 总结：两级队列的各自职责<a hidden class=anchor aria-hidden=true href=#4-总结两级队列的各自职责>#</a></h4><table><thead><tr><th>队列类型</th><th><code>proc->todo</code> (全局队列)</th><th><code>node->async_todo</code> (私有队列)</th></tr></thead><tbody><tr><td><strong>层级</strong></td><td>进程级 (Process Level)</td><td>对象级 (Binder Node Level)</td></tr><tr><td><strong>可见性</strong></td><td>所有 Binder 线程可见</td><td>仅内核驱动可见，线程不可直接获取</td></tr><tr><td><strong>核心职责</strong></td><td><strong>分发任务</strong>：作为线程池的抢占池，实现跨服务的并发。</td><td><strong>缓冲与排序</strong>：暂存特定服务的后续任务，保证顺序，防止洪水。</td></tr><tr><td><strong>处理逻辑</strong></td><td>FIFO（先进先出），任何空闲线程可取。</td><td>严格串行，必须等前一个任务 <code>BC_FREE_BUFFER</code> 后才释放。</td></tr><tr><td><strong>解决痛点</strong></td><td>解决“谁来干活”的问题。</td><td>解决“顺序错乱”和“资源独占”的问题。</td></tr></tbody></table><p>正是因为这两个队列的配合，Binder 才能做到：<strong>既能让 Audio 和 Input 并行处理（走全局队列），又能保证 Input 的每一帧严格按顺序刷新（走私有队列）。</strong></p><h3 id=24-结论与风险>2.4 结论与风险<a hidden class=anchor aria-hidden=true href=#24-结论与风险>#</a></h3><p>Binder 并没有为异步消息分配“专用”线程，所有 Binder 线程都是通用的。但是，<strong>针对同一目标对象的连续异步调用会退化为单线程处理</strong>。</p><p><strong>风险</strong>：如果 Client（如 SurfaceFlinger）向 Server（如 SystemServer）的同一个接口疯狂发送大包，且 Server 端因锁竞争导致处理变慢，内核缓冲区会迅速积压，最终导致 <code>binder_alloc</code> 失败（-28 ENOSPC）。</p><h2 id=3-内存管理机制>3. 内存管理机制<a hidden class=anchor aria-hidden=true href=#3-内存管理机制>#</a></h2><p>Binder 驱动通过 <code>mmap</code> 和页表修改实现了高效的“一次拷贝”内存管理，避免了传统 IPC 的两次拷贝开销。</p><h3 id=31-内存映射流程>3.1 内存映射流程<a hidden class=anchor aria-hidden=true href=#31-内存映射流程>#</a></h3><ol><li><strong>建立 VMA (<code>binder_mmap</code>)</strong>：</li></ol><ul><li>进程启动时打开 <code>/dev/binder</code>，驱动为其分配一块虚拟地址空间（通常 1MB - 8MB），但<strong>不分配物理内存</strong>。</li><li>这块区域专门用于接收 Binder 数据。</li></ul><ol start=2><li><strong>分配与映射 (<code>binder_alloc_buf</code>)</strong>：</li></ol><ul><li>当事务发生时，驱动从内核通用物理内存池（vmalloc/alloc_page）申请物理页。</li><li><strong>关键步骤</strong>：驱动修改目标进程的页表，将这块物理内存<strong>直接映射</strong>到目标进程步骤 1 中建立的虚拟地址空间中。</li><li>目标进程的用户空间线程可以直接读取这块内存。</li></ul><ol start=3><li><strong>释放 (<code>BC_FREE_BUFFER</code>)</strong>：</li></ol><ul><li>Server 处理完数据后，必须通知驱动释放。驱动解除映射并归还物理页。</li></ul><h3 id=32-内存配额-quota>3.2 内存配额 (Quota)<a hidden class=anchor aria-hidden=true href=#32-内存配额-quota>#</a></h3><p>为了防止异步消息滥用导致内存耗尽（DoS 攻击），Binder 实施了严格的配额管理：</p><ul><li><p><strong>异步空间限制 (<code>free_async_space</code>)</strong>：</p></li><li><p>每个进程的异步事务总内存被限制为 Binder 总 Buffer 大小的 <strong>50%</strong>。</p></li><li><p><strong>分配检查</strong>：<code>binder_alloc_buf</code> 时，如果是 <code>oneway</code> 调用，驱动会检查 <code>current_async_size + new_size &lt;= total_buffer / 2</code>。</p></li><li><p>如果超过配额，分配失败，客户端收到错误（通常是静默丢弃或报错）。</p></li><li><p><strong>同步事务</strong>：</p></li><li><p>不受 50% 配额限制，只受限于总缓冲区大小。</p></li><li><p>依赖同步调用的阻塞特性形成天然的背压（Back-pressure）。</p></li></ul><h2 id=4-常见问题排查-debug>4. 常见问题排查 (Debug)<a hidden class=anchor aria-hidden=true href=#4-常见问题排查-debug>#</a></h2><p>在系统开发中，Binder 异常通常表现为 Crash 或 Log 报错。</p><h3 id=41-常见错误代码>4.1 常见错误代码<a hidden class=anchor aria-hidden=true href=#41-常见错误代码>#</a></h3><table><thead><tr><th>错误类型</th><th>现象描述</th><th>可能原因</th><th>排查方向</th></tr></thead><tbody><tr><td><strong>DEAD_OBJECT</strong></td><td><code>DeadObjectException</code></td><td>目标进程（Server）已崩溃或被杀。</td><td>检查 Logcat 中 Server 进程的死亡日志。</td></tr><tr><td><strong>FAILED_TRANSACTION</strong></td><td>底层返回 -28 (ENOSPC)</td><td><strong>Binder 缓冲区耗尽</strong>。</td><td>1. 检查 Server 是否 ANR 或死锁。<br>2. 检查是否有 Oneway Spam（如 SF 发送大包）。<br>3. 检查 Server 线程池是否耗尽。</td></tr><tr><td><strong>TRANSACTION_TOO_LARGE</strong></td><td><code>TransactionTooLargeException</code></td><td>单次传输数据超过限制（约 1MB/2）。</td><td>检查传输的数据结构（如大 Bitmap、长列表）。</td></tr><tr><td><strong>BR_FAILED_REPLY</strong></td><td>IPC 调用失败</td><td>目标进程拒绝处理或处理出错。</td><td>检查 SELinux 权限 (<code>avc: denied</code>)。</td></tr></tbody></table><h3 id=42-深度案例缓冲区满但-server-无响应>4.2 深度案例：缓冲区满但 Server 无响应<a hidden class=anchor aria-hidden=true href=#42-深度案例缓冲区满但-server-无响应>#</a></h3><p><strong>现象</strong>：<code>binder_alloc</code> 报错 <code>pid XXX spamming oneway?</code>，缓冲区满，但 Server 进程看起来没有崩溃。</p><p><strong>深度分析逻辑</strong>：</p><ol><li><strong>线程饥饿 (Starvation)</strong>：Server 的 Binder 线程池可能被其他高频、耗时的任务（如 <code>checkPermission</code>、ContentProvider 查询）占满。</li><li><strong>锁竞争 (Lock Contention)</strong>：Server 线程虽然拿到了 Binder 任务，但卡在了内部锁（如 <code>WindowManagerGlobalLock</code> 或 <code>InputDispatcher mLock</code>）上，导致无法完成处理并释放 Buffer。</li><li><strong>串行化阻塞</strong>：如本文 2.2 节所述，如果发给同一个对象，即使有空闲线程，驱动也不会调度，导致数据在内核积压。</li></ol><p><strong>解决方案</strong>：</p><ul><li>优化 Server 端锁的粒度。</li><li>Client 端避免短时间发送大量 Oneway 大包。</li><li>排查 Server 端是否存在高频调用占用了所有 Binder 线程。</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ethen-cao.github.io/ethenslab/android-dev/debug/protolog/><span class=title>« Prev</span><br><span>深入解析 Android ProtoLog：高性能结构化日志系统</span>
</a><a class=next href=https://ethen-cao.github.io/ethenslab/android-dev/activitymanager/activity-pip/><span class=title>Next »</span><br><span>Android Internals: Activity Launch into PiP 流程深度解析</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://ethen-cao.github.io/ethenslab/>Ethen 的实验室</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0,theme:"default"})</script><script src=https://cdn.jsdelivr.net/npm/plantuml-encoder@1.4.0/dist/plantuml-encoder.min.js></script><script>(function(){const e=document.querySelectorAll("pre > code.language-plantuml, pre > code.language-planuml");e.forEach(e=>{const s=e.innerText,o=plantumlEncoder.encode(s),i="https://www.plantuml.com/plantuml/svg/"+o,t=document.createElement("img");t.src=i,t.alt="PlantUML Diagram",t.style.maxWidth="100%";const n=e.parentNode;n.parentNode.replaceChild(t,n)})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>