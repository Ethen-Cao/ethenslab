<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ethen 的实验室</title><meta name=keywords content><meta name=description content="Hotword Detection的流程
基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及底层硬件触发、系统服务路由、沙盒服务校验以及上层应用响应四个关键阶段。
以下是基于代码分析的时序流程：
第一阶段：底层硬件触发与系统层接收 (Hardware Trigger & System Layer)
当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。


HAL 层回调:
SoundTriggerHw3Compat（或其他版本的 Compat 层）收到驱动层的回调。

代码中 ModelCallbackAdaper.recognitionCallback 被调用，它将底层的事件封装为 RecognitionEventSys 并向上层传递 。



Middleware 层传递:
事件经过 SoundTriggerMiddlewareImpl 及一系列装饰器（Validation, Logging, Permission），最终到达 SoundTriggerService。


SoundTriggerService 处理:
SoundTriggerService 通过其内部的 SoundTriggerHelper 处理该事件。

SoundTriggerHelper 的 onRecognition 方法被触发，识别出这是 KeyphraseRecognitionEvent（关键短语识别事件）。
紧接着调用 onKeyphraseRecognitionLocked 。



回调 VoiceInteractionManagerService (VIMS):
SoundTriggerHelper 调用它持有的 IRecognitionStatusCallback。

在 VIMS 的上下文中，这个回调实际上是 HotwordDetectionConnection 内部定义的 SoundTriggerCallback 。



第二阶段：路由至沙盒服务 (Routing to Sandboxed Service)
VoiceInteractionManagerService 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 HotwordDetectionService 进行校验。"><meta name=author content><link rel=canonical href=https://ethen-cao.github.io/ethenslab/android-dev/visualquerydetector/visualquerydetector/><link crossorigin=anonymous href=/ethenslab/assets/css/stylesheet.a1917769c3c78460b110da6d7905321bb53af4a56f22ba4cc0de824cf4d097ab.css integrity="sha256-oZF3acPHhGCxENpteQUyG7U69KVvIrpMwN6CTPTQl6s=" rel="preload stylesheet" as=style><link rel=icon href=https://ethen-cao.github.io/ethenslab/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethen-cao.github.io/ethenslab/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethen-cao.github.io/ethenslab/favicon-32x32.png><link rel=apple-touch-icon href=https://ethen-cao.github.io/ethenslab/apple-touch-icon.png><link rel=mask-icon href=https://ethen-cao.github.io/ethenslab/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethen-cao.github.io/ethenslab/android-dev/visualquerydetector/visualquerydetector/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethen-cao.github.io/ethenslab/android-dev/visualquerydetector/visualquerydetector/"><meta property="og:site_name" content="Ethen 的实验室"><meta property="og:title" content="Ethen 的实验室"><meta property="og:description" content="Hotword Detection的流程 基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及底层硬件触发、系统服务路由、沙盒服务校验以及上层应用响应四个关键阶段。
以下是基于代码分析的时序流程：
第一阶段：底层硬件触发与系统层接收 (Hardware Trigger & System Layer) 当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。
HAL 层回调: SoundTriggerHw3Compat（或其他版本的 Compat 层）收到驱动层的回调。
代码中 ModelCallbackAdaper.recognitionCallback 被调用，它将底层的事件封装为 RecognitionEventSys 并向上层传递 。 Middleware 层传递: 事件经过 SoundTriggerMiddlewareImpl 及一系列装饰器（Validation, Logging, Permission），最终到达 SoundTriggerService。
SoundTriggerService 处理: SoundTriggerService 通过其内部的 SoundTriggerHelper 处理该事件。
SoundTriggerHelper 的 onRecognition 方法被触发，识别出这是 KeyphraseRecognitionEvent（关键短语识别事件）。 紧接着调用 onKeyphraseRecognitionLocked 。 回调 VoiceInteractionManagerService (VIMS): SoundTriggerHelper 调用它持有的 IRecognitionStatusCallback。
在 VIMS 的上下文中，这个回调实际上是 HotwordDetectionConnection 内部定义的 SoundTriggerCallback 。 第二阶段：路由至沙盒服务 (Routing to Sandboxed Service) VoiceInteractionManagerService 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 HotwordDetectionService 进行校验。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="android-dev"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Hotword Detection的流程
基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及底层硬件触发、系统服务路由、沙盒服务校验以及上层应用响应四个关键阶段。
以下是基于代码分析的时序流程：
第一阶段：底层硬件触发与系统层接收 (Hardware Trigger & System Layer)
当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。


HAL 层回调:
SoundTriggerHw3Compat（或其他版本的 Compat 层）收到驱动层的回调。

代码中 ModelCallbackAdaper.recognitionCallback 被调用，它将底层的事件封装为 RecognitionEventSys 并向上层传递 。



Middleware 层传递:
事件经过 SoundTriggerMiddlewareImpl 及一系列装饰器（Validation, Logging, Permission），最终到达 SoundTriggerService。


SoundTriggerService 处理:
SoundTriggerService 通过其内部的 SoundTriggerHelper 处理该事件。

SoundTriggerHelper 的 onRecognition 方法被触发，识别出这是 KeyphraseRecognitionEvent（关键短语识别事件）。
紧接着调用 onKeyphraseRecognitionLocked 。



回调 VoiceInteractionManagerService (VIMS):
SoundTriggerHelper 调用它持有的 IRecognitionStatusCallback。

在 VIMS 的上下文中，这个回调实际上是 HotwordDetectionConnection 内部定义的 SoundTriggerCallback 。



第二阶段：路由至沙盒服务 (Routing to Sandboxed Service)
VoiceInteractionManagerService 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 HotwordDetectionService 进行校验。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Android系统开发","item":"https://ethen-cao.github.io/ethenslab/android-dev/"},{"@type":"ListItem","position":2,"name":"","item":"https://ethen-cao.github.io/ethenslab/android-dev/visualquerydetector/visualquerydetector/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"Hotword Detection的流程 基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及底层硬件触发、系统服务路由、沙盒服务校验以及上层应用响应四个关键阶段。\n以下是基于代码分析的时序流程：\n第一阶段：底层硬件触发与系统层接收 (Hardware Trigger \u0026amp; System Layer) 当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。\nHAL 层回调: SoundTriggerHw3Compat（或其他版本的 Compat 层）收到驱动层的回调。\n代码中 ModelCallbackAdaper.recognitionCallback 被调用，它将底层的事件封装为 RecognitionEventSys 并向上层传递 。 Middleware 层传递: 事件经过 SoundTriggerMiddlewareImpl 及一系列装饰器（Validation, Logging, Permission），最终到达 SoundTriggerService。\nSoundTriggerService 处理: SoundTriggerService 通过其内部的 SoundTriggerHelper 处理该事件。\nSoundTriggerHelper 的 onRecognition 方法被触发，识别出这是 KeyphraseRecognitionEvent（关键短语识别事件）。 紧接着调用 onKeyphraseRecognitionLocked 。 回调 VoiceInteractionManagerService (VIMS): SoundTriggerHelper 调用它持有的 IRecognitionStatusCallback。\n在 VIMS 的上下文中，这个回调实际上是 HotwordDetectionConnection 内部定义的 SoundTriggerCallback 。 第二阶段：路由至沙盒服务 (Routing to Sandboxed Service) VoiceInteractionManagerService 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 HotwordDetectionService 进行校验。\n","keywords":[],"articleBody":"Hotword Detection的流程 基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及底层硬件触发、系统服务路由、沙盒服务校验以及上层应用响应四个关键阶段。\n以下是基于代码分析的时序流程：\n第一阶段：底层硬件触发与系统层接收 (Hardware Trigger \u0026 System Layer) 当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。\nHAL 层回调: SoundTriggerHw3Compat（或其他版本的 Compat 层）收到驱动层的回调。\n代码中 ModelCallbackAdaper.recognitionCallback 被调用，它将底层的事件封装为 RecognitionEventSys 并向上层传递 。 Middleware 层传递: 事件经过 SoundTriggerMiddlewareImpl 及一系列装饰器（Validation, Logging, Permission），最终到达 SoundTriggerService。\nSoundTriggerService 处理: SoundTriggerService 通过其内部的 SoundTriggerHelper 处理该事件。\nSoundTriggerHelper 的 onRecognition 方法被触发，识别出这是 KeyphraseRecognitionEvent（关键短语识别事件）。 紧接着调用 onKeyphraseRecognitionLocked 。 回调 VoiceInteractionManagerService (VIMS): SoundTriggerHelper 调用它持有的 IRecognitionStatusCallback。\n在 VIMS 的上下文中，这个回调实际上是 HotwordDetectionConnection 内部定义的 SoundTriggerCallback 。 第二阶段：路由至沙盒服务 (Routing to Sandboxed Service) VoiceInteractionManagerService 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 HotwordDetectionService 进行校验。\nHotwordDetectionConnection 介入: 在 HotwordDetectionConnection.SoundTriggerCallback.onKeyphraseDetected 中：\n系统检查是否已建立 HotwordDetectionConnection（即是否使用了 HotwordDetectionService）。 如果是（“Hi Google”通常是这种情况），它会记录 metrics 并调用 mHotwordDetectionConnection.detectFromDspSource 。 注意：如果没使用 HDS，它会走另一条路直接回调给 App，但现在主流流程都包含 HDS 。 会话分发: HotwordDetectionConnection 找到对应的检测器会话（对于 DSP 触发，是 DspTrustedHotwordDetectorSession），并调用其 detectFromDspSourceLocked 方法 。\n发起 IPC 调用: 在 DspTrustedHotwordDetectorSession.detectFromDspSourceLocked 中：\n系统设置一个超时计时器（默认 3-4秒），防止沙盒服务无响应 。 调用 service.detectFromDspSource(...)。这里的 service 是 ISandboxedDetectionService 的 Binder 代理，指向运行在隔离进程中的 HotwordDetectionService 。 系统将硬件识别事件 (recognitionEvent) 和音频格式传递给沙盒服务。 第三阶段：沙盒服务校验 (Sandboxed Validation) 这一阶段发生在隔离进程中（代码逻辑由 Google App 提供，但运行环境受限）。\n算法校验: HotwordDetectionService（未在AOSP代码中，属于 App 侧实现）接收到音频数据，运行高精度的软件算法进行二次确认。\n返回结果:\n校验通过: 沙盒服务调用回调接口的 onDetected 方法。 校验失败: 沙盒服务调用 onRejected。 第四阶段：结果处理与分发 (Result Processing \u0026 Dispatch) 系统服务（System Server）收到沙盒服务的验证结果后，进行安全检查和数据封装，最后发给主应用。\n接收校验结果: DspTrustedHotwordDetectorSession 内部的匿名回调类收到 onDetected 调用 。\n安全检查:\n超时检查: 检查是否已超时 。 权限检查: 调用 enforcePermissionsForDataDelivery 确保主应用有录音权限 。 Keyphrase ID 检查: 调用 enforceExtraKeyphraseIdNotLeaked 确保沙盒服务没有伪造硬件未检测到的短语 ID 。 音频流拷贝: 系统使用 mHotwordAudioStreamCopier.startCopyingAudioStreams(result)。这会将沙盒服务传递过来的音频流（Pipe）进行处理，以便主应用可以读取音频数据 。\n回调主应用 (Google App): 最后，DspTrustedHotwordDetectorSession 调用 externalCallback.onKeyphraseDetected(recognitionEvent, newResult) 。\n这里的 externalCallback 是 Google App 主进程中 AlwaysOnHotwordDetector 注册的 Binder 回调。 UI 展示: Google App 的主进程收到回调后，确认唤醒成功，随即调用 VoiceInteractionService.showSession()，请求 VIMS 显示语音助手界面（如底部的彩色条）。\n时序图总结 @startuml !theme plain autonumber \"[0]\" skinparam backgroundColor white skinparam sequence { ArrowColor Black ActorBorderColor Black LifeLineBorderColor Black ParticipantBorderColor Black ParticipantBackgroundColor White } participant \"Hardware/HAL\\n(DSP)\" as HAL participant \"SoundTrigger\\nService\" as STS participant \"HotwordDetection\\nConnection\" as HDC participant \"DspTrusted\\nSession\" as DSP_Session participant \"HotwordDetection\\nService\\n(Isolated Process)\" as HDS participant \"Google App\\n(Host Process)\" as App == 阶段一：硬件触发 == HAL -\u003e STS: onRecognition(Event) note right: 硬件检测到声学特征 STS -\u003e HDC: onKeyphraseDetected(Event) note right: 路由至 VoiceInteractionManagerService 层 == 阶段二：发起校验 == HDC -\u003e DSP_Session: detectFromDspSource(Event) activate DSP_Session DSP_Session -\u003e HDS: detectFromDspSource(Event, Audio) note right: 跨进程调用至隔离沙盒 == 阶段三：沙盒校验 == activate HDS HDS -\u003e HDS: 运行软件算法\\n(二次确认) HDS -\u003e DSP_Session: onDetected(Result) deactivate HDS == 阶段四：结果分发 == DSP_Session -\u003e DSP_Session: 权限检查 \u0026 音频流拷贝 DSP_Session -\u003e App: onKeyphraseDetected(Result) note right: 最终回调给主应用 deactivate DSP_Session App -\u003e App: 播放提示音/显示UI @enduml 通过这个流程，Android 既利用了 DSP 的低功耗特性（第一阶段），又利用了隔离进程的安全性来保护隐私（第三阶段），最后才唤醒主应用进行交互。\n注视唤醒 “注视唤醒”（Visual Query Detection / Gaze Wake-up）的实现流程是利用 VisualQueryDetectionService (VQDS) 结合摄像头数据来触发语音交互。 这个过程利用了 Android 的沙盒机制来保护隐私，确保只有在检测到明确的“注视”意图后，数据才会流向主助理应用。 以下是基于代码分析的详细实现流程：\n核心组件角色 Host App (Assistant): 语音助手主应用（如 Google App），运行 VisualQueryDetector。 System Server (VIMS): 系统服务，负责权限管控和路由。具体由 VoiceInteractionManagerService 和 HotwordDetectionConnection 管理。 Isolated Process (VQDS): 运行 VisualQueryDetectionService，负责处理摄像头数据并检测注视行为。 详细时序流程 1. 初始化与绑定 (Initialization) 在功能启用时，主应用需要初始化检测器并传递模型数据。\n创建检测器: Host App 调用 VoiceInteractionService.createVisualQueryDetector() 。 系统绑定: VoiceInteractionManagerService (VIMS) 接收请求，通过 HotwordDetectionConnection 绑定到 App 声明的 VisualQueryDetectionService（必须配置为隔离进程） 。 传递配置: 系统调用 onUpdateState 将配置参数 (PersistableBundle) 和共享内存 (SharedMemory) 传递给隔离服务，用于加载视觉模型 。 2. 启动感知 (Start Perceiving) 当设备屏幕点亮或进入特定状态时，Host App 请求开启检测。\n发起请求: Host App 调用 VisualQueryDetector.startRecognition() 。 系统路由: 该调用通过 Binder 传递给 VIMS 的 startPerceiving 方法 。 连接层处理: VIMS 委托给 HotwordDetectionConnection，后者找到 VisualQueryDetectorSession 并调用 startPerceivingLocked 。 启动沙盒服务: Session 通过 AIDL 调用隔离服务的 detectWithVisualSignals(callback) 方法 。 服务响应: 在隔离进程中，VisualQueryDetectionService 的 onStartDetection() 被触发。开发者需在此处开启摄像头流 。 3. 注视检测与注意力捕获 (Attention Detection) 这是“注视唤醒”的核心逻辑，发生在隔离进程中。\n视觉处理: VisualQueryDetectionService 分析每一帧图像（使用 App 自带的算法库）。 获得注意 (Attention Gained): 当算法判断用户正在注视屏幕并有交互意图时，服务调用 gainedAttention(VisualQueryAttentionResult) 。 系统反馈: 系统服务端的 VisualQueryDetectorSession 收到回调。 它会通知系统的 IVisualQueryDetectionAttentionListener（通常是 SystemUI），在屏幕上显示“正在聆听”的视觉动效（如角落的光效） 。 关键点: 这一步并不会将音频或视频发给主 App，仅通知系统“用户在看”。 4. 查询流式传输 (Query Streaming) 在获得注意力后，服务开始将识别到的意图（语音或多模态数据）传回主应用。\n流式传输: 隔离服务调用 streamQuery(String partialQuery) 或 streamQuery(VisualQueryDetectedResult) 。 权限检查: 系统服务（System Server）在转发数据前进行严格的权限检查： 如果包含音频查询，检查 RECORD_AUDIO 权限 。 如果包含视觉辅助数据，检查 CAMERA 权限 。 数据分发: 检查通过后，VisualQueryDetectorSession 调用 onQueryDetected 。 Host App 响应: 主应用中的 VisualQueryDetector.Callback 收到查询文本，开始执行搜索或指令 。 5. 结束交互 (Completion) 完成: 当用户停止说话，服务调用 finishQuery() 。 失去注意: 当用户移开视线，服务调用 lostAttention()，系统会取消 UI 动效并停止接收流 。 时序图解 @startuml !theme plain autonumber \"[0]\" skinparam backgroundColor white skinparam sequence { ArrowColor Black ActorBorderColor Black LifeLineBorderColor Black ParticipantBorderColor Black ParticipantBackgroundColor White } participant \"Host App\\n(Assistant)\" as App participant \"System Server\\n(VIMS/Session)\" as System participant \"Isolated Service\\n(VQDS)\" as Service == 阶段一：启动 == App -\u003e System: startRecognition() System -\u003e Service: detectWithVisualSignals() Service -\u003e Service: 打开摄像头\\n开始分析 == 阶段二：注视检测 (Gaze) == Service -\u003e Service: 算法检测到用户注视 Service -\u003e System: gainedAttention(AttentionResult) System -\u003e System: 通知 SystemUI 显示光效\\n(用户感知到设备已唤醒) == 阶段三：唤醒与流传输 == Service -\u003e Service: 结合唇语/语音识别 Service -\u003e System: streamQuery(\"打开相册\") activate System System -\u003e System: 检查 AppOps/权限 (Record Audio) System -\u003e App: onQueryDetected(\"打开相册\") deactivate System App -\u003e App: 执行指令 == 阶段四：结束 == Service -\u003e System: lostAttention() / finishQuery() System -\u003e App: onQueryFinished() App -\u003e System: stopRecognition() System -\u003e Service: stopDetection() @enduml 总结 “注视唤醒”的关键在于 VisualQueryDetectionService。它允许语音助手在不泄露摄像头原始数据的前提下（因为在隔离进程），实时判断用户的注视行为。只有当 gainedAttention 被调用后，系统才允许流式传输查询内容，从而实现了隐私与便利性的平衡。\nVoiceInteractionManagerService VoiceInteractionManagerService (VIMS) 是 Android 系统中管理语音交互的核心系统服务。它作为系统服务运行在 system_server 进程中，负责协调用户选定的语音助手应用（如 Google Assistant）与系统其他组件（如 ActivityManager, WindowManager, SoundTrigger 等）之间的交互。\n以下是基于源代码对 VoiceInteractionManagerService 的详细设计讲解：\n1. 职责 (Responsibilities) VoiceInteractionManagerService 的主要职责可以归纳为以下几点：\n语音助手生命周期管理:\n负责管理当前激活的语音交互服务（VoiceInteractionService）。 处理服务的绑定与解绑，特别是在用户切换、设置变更或应用更新时 。 确保同一时间只有一个活跃的语音助手服务 。 会话 (Session) 管理:\n处理来自语音助手的 showSession 请求，协调显示语音交互界面（Session UI）。 管理 Session 的创建、显示、隐藏和销毁。 协助传递上下文数据（Assist Data），如屏幕截图和视图结构，给语音助手 。 热词检测 (Hotword Detection) 协调:\n作为上层应用与底层 SoundTrigger 硬件之间的桥梁。 管理 AlwaysOnHotwordDetector 和 HotwordDetectionService（在隔离进程中运行的热词检测服务）。 处理 DSP 模型的加载、更新和卸载 。 验证和管理 HotwordDetectionService 的身份和权限 。 视觉查询 (Visual Query) 支持:\n支持 VisualQueryDetectionService，允许助手利用摄像头数据进行检测（如注视唤醒）。 权限与安全性:\n强制执行权限检查，例如 BIND_VOICE_INTERACTION，确保只有拥有正确权限的应用才能成为语音助手 。 管理临时权限授予，允许助手访问启动的 Activity 数据 。 多用户支持:\n维护每个用户的语音助手配置，并在用户切换时重新加载相应的服务实现 。 2. 实现 (Implementation) VIMS 的实现采用了典型的 Android 系统服务模式，分为服务入口、Binder 桩（Stub）和每用户实现（Impl）。\n2.1 核心类结构 VoiceInteractionManagerService:\n继承自 SystemService，是服务的入口点。 负责发布 Binder 服务和本地服务 (VoiceInteractionManagerInternal) 。 监听用户生命周期事件 (onUserStarting, onUserSwitching) 。 VoiceInteractionManagerServiceStub (内部类):\n继承自 IVoiceInteractionManagerService.Stub。 实现了跨进程通信（IPC）接口，供外部应用（如 Settings, SystemUI, 语音助手 App）调用。 主要负责接收请求，进行权限检查，然后转发给当前用户的 VoiceInteractionManagerServiceImpl 。 VoiceInteractionManagerServiceImpl:\n核心逻辑所在。每个 Impl 实例对应一个特定的用户和该用户选定的语音助手组件。 维护与 VoiceInteractionService 的 ServiceConnection 。 持有 HotwordDetectionConnection 来管理热词检测 。 持有 VoiceInteractionSessionConnection 来管理 UI 会话 。 2.2 启动时序 (Startup Sequence) VIMS 的启动流程如下：\nonStart():\n由 SystemServer 调用。 发布 Binder 服务 voiceinteraction。 发布本地服务 VoiceInteractionManagerInternal，供系统进程内其他服务调用。 向 ActivityManagerInternal 注册监听器，以便在 Activity 销毁时收到通知 。 onBootPhase(int phase):\nPHASE_SYSTEM_SERVICES_READY: 获取依赖的系统服务，如 ShortcutServiceInternal 和 SoundTriggerInternal 。 PHASE_THIRD_PARTY_APPS_CAN_START: 调用 mServiceStub.systemRunning(isSafeMode())，开始真正的初始化 。 systemRunning() (在 Stub 中):\n注册 PackageMonitor 监听包的变化（如语音助手被卸载或更新）。 注册 SettingsObserver 监听 Settings.Secure.VOICE_INTERACTION_SERVICE 的变化。 调用 initForUser() 初始化当前用户的状态 。 调用 switchImplementationIfNeeded() 尝试绑定服务 。 initForUser(int userHandle):\n从 Settings.Secure 读取当前用户的语音交互服务配置。 如果未设置，尝试根据配置或系统默认值找到一个可用的语音助手，并更新设置 。 初始化语音识别器组件信息 。 switchImplementationIfNeeded():\n这是连接建立的关键点。 它读取当前的设置，解析出组件名 (ComponentName)。 创建一个新的 VoiceInteractionManagerServiceImpl 实例，传入上下文和组件信息 。 调用 mImpl.startLocked() 。 VoiceInteractionManagerServiceImpl.startLocked():\n创建 Intent 指向语音助手组件。 调用 mContext.bindServiceAsUser() 绑定服务。这里使用了 BIND_FOREGROUND_SERVICE 等标志来确保服务优先级 。 onServiceConnected() (在 Impl 的 Connection 中):\n当服务绑定成功后，获取 IVoiceInteractionService 接口。 调用远程服务的 ready() 方法，通知语音助手应用系统已准备就绪 。 至此，VIMS 启动完成，并与当前用户的语音助手建立了双向通信通道。当用户触发语音助手时，VIMS 将通过这个通道发送请求；反之，当语音助手需要显示 UI 或访问硬件时，也会通过这个通道回调 VIMS。\nVoiceInteractionManagerServiceImpl VoiceInteractionManagerServiceImpl 是 Android 语音交互框架中负责具体业务实现的核心类。每个实例对应一个特定的用户和一个选定的语音交互服务组件（例如 Google Assistant）。它全权负责管理该语音助手在当前用户下的生命周期、会话连接、硬件绑定以及业务逻辑转发。\n以下是该类的详细职责描述：\n1. 语音助手服务的生命周期管理 该类负责维护与实际语音助手应用（VoiceInteractionService）的绑定关系。\n服务绑定与验证：在构造函数中，它会解析 VoiceInteractionServiceInfo 来验证服务的有效性 。通过 startLocked 方法，它使用 bindServiceAsUser 绑定到目标服务组件 。 连接状态监控：通过 ServiceConnection (mConnection) 监听服务的连接状态。 连接成功：在 onServiceConnected 中获取 IVoiceInteractionService 接口代理并调用 ready() 。 连接断开：在 onServiceDisconnected 中清理引用并重置 Hotword 连接 。 意外死亡：在 onBindingDied 中处理服务意外退出的情况，例如检测是否由用户强制停止 。 资源清理：在 shutdownLocked 中负责解绑服务，取消当前会话，并断开 Hotword 检测连接 。 2. 会话 (Session) 管理 它管理着语音助手的 UI 界面（即“会话”），是系统与助手 UI 之间的桥梁。\n会话创建与显示：showSessionLocked 方法负责创建 VoiceInteractionSessionConnection 对象（若尚不存在），这代表了助手应用显示的 UI 层 。它还会计算唯一的 KEY_SHOW_SESSION_ID 并传递给服务 。 可见性控制：提供 hideSessionLocked 方法来隐藏当前会话 ，以及 finishLocked 方法来销毁会话 。 窗口交互：处理关闭系统对话框 (closeSystemDialogsLocked) 和保持屏幕常亮 (setKeepAwakeLocked) 的请求。 3. 检测器管理 (Hotword \u0026 Visual Query) 它是上层应用与底层检测硬件/隔离服务之间的枢纽，持有一个 HotwordDetectionConnection 对象来具体执行这些任务。\n连接持有：通过 mHotwordDetectionConnection 变量持有对热词检测连接的引用 。 初始化检测器：initAndVerifyDetectorLocked 方法负责校验权限、配置隔离进程，并初始化 Hotword 或 Visual Query 检测器 。 状态与数据更新：updateStateLocked 负责将配置参数 (PersistableBundle) 和模型数据 (SharedMemory) 更新到底层检测服务 。 麦克风与视觉控制：提供 startListeningFromMicLocked 、startPerceivingLocked 等方法来控制音频和视觉信号的捕获。 4. 辅助功能与 Activity 交互 它充当了系统 Activity 管理器 (ActivityTaskManager) 和语音助手之间的代理，赋予助手操作 App 的能力。\n启动 Activity：实现了 startVoiceActivityLocked 和 startAssistantActivityLocked ，允许语音助手在特定模式（如隐藏模式或不激活模式）下启动 Activity。 Direct Actions：通过 requestDirectActionsLocked 和 performDirectActionLocked ，允许语音助手查询和执行当前前台 App 的应用内动作（App Slices/Intents）。 权限授予：通过 grantImplicitAccessLocked 方法，向语音助手授予对特定 Activity 的隐式访问权限 。 5. 系统设置与监听 无障碍设置监听：内部类 AccessibilitySettingsContentObserver 负责监听 VISUAL_QUERY_ACCESSIBILITY_DETECTION_ENABLED 设置的变化，并通知注册的监听器 。 可见性监听：支持启动和停止监听可见 Activity 的变化 (startListeningVisibleActivityChangedLocked)，以便助手能对屏幕内容做出反应 。 总结 VoiceInteractionManagerServiceImpl 是 Android 语音交互服务的具体执行者。它在一个特定用户的上下文中，协调了服务绑定、UI 会话展示、底层硬件检测以及与其他系统服务（如 ATM/AM）的交互。\n在 VoiceInteractionManagerServiceImpl 类中，mComponent 是一个关键的成员变量。以下是关于它的详细解释：\n1. mComponent 是什么？ 类型: ComponentName 含义: 它代表了当前选定的语音交互服务（Voice Interaction Service）的组件名称。 简单来说，就是用户在系统设置中选择的那个“默认数字助理应用”的具体服务组件（例如 Google Assistant 的 com.google.android.googlequicksearchbox/com.google.android.voiceinteraction.GsaVoiceInteractionService）。 作用: 这个变量用于在系统内部唯一标识当前正在为用户服务的那个语音助手组件。后续的绑定服务（bindService）、启动 Activity 等操作都需要使用这个组件名来指定目标 。 2. 它是如何被设置的？ 它是通过 VoiceInteractionManagerServiceImpl 的构造函数 进行初始化的。\n代码位置: 在 VoiceInteractionManagerServiceImpl.java 的构造函数中： VoiceInteractionManagerServiceImpl(Context context, Handler handler, VoiceInteractionManagerService.VoiceInteractionManagerServiceStub stub, int userHandle, ComponentName service) { // \u003c--- 这里的 service 参数 // ... mComponent = service; // \u003c--- 在这里赋值 // ... } 3. 谁负责传入这个值？（谁调用了构造函数？） 是由系统服务的主控类 VoiceInteractionManagerService (VIMS) 负责创建并传入的。\n具体流程如下：\n读取设置: 在 VoiceInteractionManagerService.java 中，方法 switchImplementationIfNeededNoTracingLocked 会从系统安全设置中读取当前用户选定的语音服务字符串：\nString curService = Settings.Secure.getStringForUser( mResolver, Settings.Secure.VOICE_INTERACTION_SERVICE, mCurUser); 解析组件名: 将读取到的字符串解析为 ComponentName 对象（代码中命名为 serviceComponent）：\nserviceComponent = ComponentName.unflattenFromString(curService); 实例化 Impl 并传入: VIMS 实例化 VoiceInteractionManagerServiceImpl，并将 serviceComponent 作为参数传进去：\nsetImplLocked(new VoiceInteractionManagerServiceImpl(mContext, UiThread.getHandler(), this, mCurUser, serviceComponent)); // \u003c--- 传入 mComponent 总结 是什么: 当前激活的语音助手服务的组件名（包名+类名）。 谁设置: 由 VoiceInteractionManagerService 读取系统设置 (Settings.Secure.VOICE_INTERACTION_SERVICE) 后，在创建 VoiceInteractionManagerServiceImpl 实例时通过构造函数传入的。 VoiceInteractionServiceInfo VoiceInteractionServiceInfo 的构造过程主要发生在它的构造函数中。它通过 PackageManager 获取服务的 ServiceInfo，然后读取该服务在 AndroidManifest.xml 中声明的 所指向的 XML 资源文件。\n以下是具体的解析步骤：\n1. 入口与权限检查 构造函数接收 PackageManager 和 ServiceInfo 作为参数 。\n权限验证：首先检查该服务是否声明了 android.permission.BIND_VOICE_INTERACTION 权限。如果没有，解析过程会终止并设置错误信息 。 2. 加载 XML 元数据 代码调用 si.loadXmlMetaData 方法加载元数据 。\n元数据名称：它查找的键值是 VoiceInteractionService.SERVICE_META_DATA ，即 \"android.voice_interaction\" 。 这对应于 AndroidManifest.xml 中服务声明内的 。 3. 解析 XML 标签 使用 XmlResourceParser 解析加载的 XML 资源：\n根标签验证：解析器跳过开始文档事件，检查根标签（Root Tag）的名称是否为 \"voice-interaction-service\"。如果不是，则报错返回 。 4. 读取属性 (Attributes) 一旦验证了根标签，代码会获取应用程序的资源 (Resources)，并使用 obtainAttributes 方法读取 XML 中的属性值 。这些属性被映射到 VoiceInteractionServiceInfo 的成员变量中：\nsessionService (android:sessionService): 定义会话服务的组件名 。 recognitionService (android:recognitionService): 定义语音识别服务的组件名 。 settingsActivity (android:settingsActivity): 定义设置页面的 Activity 。 supportsAssist (android:supportsAssist): 布尔值，是否支持辅助功能 。 supportsLaunchFromKeyguard (android:supportsLaunchVoiceAssistFromKeyguard): 布尔值，是否支持从锁屏启动 。 supportsLocalInteraction (android:supportsLocalInteraction): 布尔值，是否支持本地交互 。 hotwordDetectionService (android:hotwordDetectionService): 定义 Hotword 检测服务的组件名 。 visualQueryDetectionService (android:visualQueryDetectionService): 定义视觉查询检测服务的组件名 。 5. 强制性字段检查 在资源回收 (array.recycle()) 后，代码会检查两个必须存在的属性：\nsessionService 必须存在，否则报错 。 recognitionService 必须存在，否则报错 。 总结 VoiceInteractionServiceInfo 的构造本质上是对开发者在 res/xml/ 目录下定义的配置文件的反序列化过程。系统通过这个过程知道了语音助手由哪些组件构成（如会话服务、热词服务、视觉服务等）以及它支持哪些特性。\nVoiceInteractionService 根据 frameworks/base/core/java/android/service/voice/VoiceInteractionService.java 文件的定义和注释，VoiceInteractionService (VIS) 是当前被选中的语音交互应用的顶层服务（Top-level service）。它由 Android 系统保持长期运行，充当语音助手应用的“大脑”或“入口”。\n以下是它的主要职责：\n1. 充当常驻后台的“管家” 始终运行：它是当前用户选定的语音交互器，由系统保持始终运行（Always Running）。 轻量级设计：由于它始终运行，设计原则要求它必须尽可能轻量 。它不应处理繁重的 UI 渲染或复杂的业务逻辑，这些应交给独立的 VoiceInteractionSessionService 处理 。 2. 管理热词与检测器 (Detection Management) 它是创建和管理各种唤醒检测机制的入口：\nDSP 热词检测：通过 createAlwaysOnHotwordDetector 创建基于硬件 DSP 的低功耗热词检测器（如 “Hey Google”）。 软件/麦克风检测：通过 createHotwordDetector 创建基于软件或麦克风输入的检测器 。 视觉查询检测：通过 createVisualQueryDetector 创建视觉查询检测器，利用摄像头信号进行唤醒（如“注视唤醒”）。 模型管理：处理声学模型的更新通知 (onSoundModelsChangedInternal) 。 3. 发起交互会话 (Session Initiation) 虽然它自己不显示 UI，但它负责决定何时显示 UI：\n启动会话：通过调用 showSession() 方法，请求系统启动与用户交互的 VoiceInteractionSession 。这通常发生在检测到热词或收到其他触发信号时。 传递参数：它可以通过 Bundle 向 Session 传递参数 。 4. 处理系统生命周期回调 它接收来自系统的关键状态通知：\n初始化：onReady() 方法在系统准备好接收交互时被调用，这是服务进行初始化的时机 。 销毁：onShutdown() 方法在服务被关闭时调用 。 配置变更：处理支持的语音动作查询 (onGetSupportedVoiceActions) 。 5. 锁屏交互支持 锁屏唤醒：通过 onLaunchVoiceAssistFromKeyguard() 方法，处理用户在锁屏状态下触发语音助手的操作 。这允许助手在锁屏之上显示特定的 UI。 6. 设置与上下文控制 上下文限制：通过 setDisabledShowContext() 方法，可以控制在启动会话时是否禁用获取屏幕截图或辅助数据（Assist Data）。 UI 提示：通过 setUiHints() 向系统 UI 提供提示信息 。 总结： VoiceInteractionService 是语音助手应用的后台指挥官。它负责监听唤醒词（通过管理 Detector），并在需要时“叫醒”负责显示 UI 和处理复杂交互的 VoiceInteractionSessionService（前台执行者）。\nVisualQueryDetectionService 是的，VisualQueryDetectionService 也是在语音助理 App（例如 Google App）中实现的，并且其运行机制与 HotwordDetectionService 高度相似。\n它虽然是语音助理 APK 的一部分，但受到 Android 系统严格的沙盒限制。\n1. 代码位置：在助理 App 内 根据源代码注释，VisualQueryDetectionService 是“由想要提供视觉查询检测功能的应用程序实现的” 。\n开发者需要在语音助理 App 的工程中继承 VisualQueryDetectionService 并实现其抽象方法（如 onStartDetection）。 2. 配置方式：元数据声明 系统通过读取 VoiceInteractionService 的 XML 元数据来找到这个服务。\n在 VoiceInteractionServiceInfo.java 中可以看到，系统会解析 android:visualQueryDetectionService 属性 。 这意味着开发者必须在 res/xml/ 下的配置文件中明确指定这个类的路径。 3. 运行环境：强制隔离进程 与 HotwordDetectionService 一样，VisualQueryDetectionService 必须配置为在隔离进程中运行。\n在系统绑定该服务之前，HotwordDetectionConnection 会进行严格检查：verifyDetectorForVisualQueryDetectionLocked 方法会验证该服务是否设置了 android:isolatedProcess=\"true\" 。 如果没有配置隔离进程，系统会抛出 IllegalStateException 并拒绝启动服务。 4. 进程共享 (Process Sharing) 值得注意的是，Android 允许 VisualQueryDetectionService 和 HotwordDetectionService 运行在同一个隔离进程中。\n在 verifyProcessSharingLocked 方法中，系统会检查这两个服务是否都配置了 android:allowSharedIsolatedProcess=\"true\" 。 如果配置允许，它们可以共享同一个沙盒环境，从而更方便地共享内存或状态（例如共享某些底层推理引擎资源），同时依然与主应用进程隔离。 总结 VisualQueryDetectionService 是语音助理 App 提供的代码，但被 Android 系统强制“关押”在一个独立的、受限的沙盒进程中运行，目的是在处理摄像头数据（视觉信号）时保护用户隐私。\nHotwordDetectionService HotwordDetectionService 虽然代码包含在语音助理 App（如 Google App）的 APK 中，但它能够运行在隔离进程中，完全依赖于 Android 系统服务（System Server）的特殊绑定机制。\n简单来说：代码是 App 提供的，但启动方式和运行环境是由 Android 系统强制指定的。\n以下是其运行在隔离进程中的具体技术实现步骤：\n1. App 侧的强制声明 (Manifest Declaration) 首先，语音助理 App 必须在其 AndroidManifest.xml 中声明该服务，并显式设置 android:isolatedProcess=\"true\"。 虽然我们无法直接看到 Manifest 文件，但在 VoiceInteractionManagerServiceImpl.java 中，系统会严格检查这一属性。\n检查逻辑：在绑定服务前，系统会调用 isIsolatedProcessLocked 方法，检查该服务的 ServiceInfo.flags 是否包含 FLAG_ISOLATED_PROCESS。 强制执行：如果开发者忘记在 Manifest 中声明这一属性，系统会抛出 IllegalStateException 并拒绝启动该服务。 2. 系统侧的特殊绑定 (System Binding) 普通的 Service 绑定使用 bindService，但对于 HotwordDetectionService，系统使用的是 bindIsolatedService。\n在 HotwordDetectionConnection.java 的 ServiceConnection 内部类中可以看到这一关键调用：\nboolean bindResult = mContext.bindIsolatedService( mIntent, Context.BIND_AUTO_CREATE | Context.BIND_FOREGROUND_SERVICE | mBindingFlags, \"hotword_detector_\" + mInstanceNumber, // 实例名称 mExecutor, serviceConnection); bindIsolatedService 的作用：这个 API 指示 Activity Manager (AMS) 启动一个新的进程。这个进程是临时的、匿名的，并且被分配了一个特殊的 Isolated UID。 独立的 UID：这个 Isolated UID 与主 App 的 UID 不同（例如主 App 是 u0_a123，隔离进程可能是 u0_i99）。Linux 内核的权限管理机制会基于 UID 隔离资源，使得这个进程无法访问主 App 的私有数据目录，也没有网络权限。 3. 实例区分 (Instance Name) 在调用 bindIsolatedService 时，系统还会传入一个 instanceName（例如 \"hotword_detector_0\"）。\n这允许系统区分同一个服务的不同隔离实例。如果服务崩溃重启，系统可以通过计数器 (mInstanceNumber) 创建一个新的、干净的隔离进程实例。 4. 共享隔离进程 (Shared Isolated Process) 如果系统属性 ro.hotword.visual_query_service_enabled 为真，且应用声明了支持，HotwordDetectionService 还可以与 VisualQueryDetectionService 共享同一个隔离进程。\n代码中会检查 Context.BIND_SHARED_ISOLATED_PROCESS 标志。这允许两个相关的服务在同一个沙盒中运行，以便高效共享内存数据，但依然与主 App 隔离。 总结 HotwordDetectionService 之所以能运行在隔离进程，是因为：\nApp 声明：App 自愿放弃了该 Service 的权限，声明其为 isolatedProcess。 系统执行：System Server 使用 bindIsolatedService API 启动它，操作系统内核为其分配了受限的 UID，从而在此“牢笼”中执行 App 提供的代码。 ","wordCount":"1607","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ethen-cao.github.io/ethenslab/android-dev/visualquerydetector/visualquerydetector/"},"publisher":{"@type":"Organization","name":"Ethen 的实验室","logo":{"@type":"ImageObject","url":"https://ethen-cao.github.io/ethenslab/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethen-cao.github.io/ethenslab/ accesskey=h title="Ethen 的实验室 (Alt + H)">Ethen 的实验室</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethen-cao.github.io/ethenslab/android-dev/ title=Android系统开发><span>Android系统开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/android-automotive-os-dev/ title="Android Automotive"><span>Android Automotive</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/qnx/ title=QNX开发><span>QNX开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/gunyah/ title=Gunyah><span>Gunyah</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/ivi-solution/ title=智能座舱方案><span>智能座舱方案</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/explore-ai title="Explore AI"><span>Explore AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethen-cao.github.io/ethenslab/>Home</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/android-dev/>Android系统开发</a></div><h1 class="post-title entry-hint-parent"></h1><div class=post-meta>8 min&nbsp;·&nbsp;1607 words</div></header><div class=post-content><h2 id=hotword-detection的流程>Hotword Detection的流程<a hidden class=anchor aria-hidden=true href=#hotword-detection的流程>#</a></h2><p>基于提供AOSP的代码，以“Hi Google”为例，语音唤醒（Hotword Detection）的完整流程是一个跨进程、跨层级的复杂协作过程。它主要涉及<strong>底层硬件触发</strong>、<strong>系统服务路由</strong>、<strong>沙盒服务校验</strong>以及<strong>上层应用响应</strong>四个关键阶段。</p><p>以下是基于代码分析的时序流程：</p><h3 id=第一阶段底层硬件触发与系统层接收-hardware-trigger--system-layer>第一阶段：底层硬件触发与系统层接收 (Hardware Trigger & System Layer)<a hidden class=anchor aria-hidden=true href=#第一阶段底层硬件触发与系统层接收-hardware-trigger--system-layer>#</a></h3><p>当用户说出“Hi Google”时，底层 DSP 芯片（SoundTrigger HAL）检测到声学特征，并向系统发送中断信号。</p><ol><li><p><strong>HAL 层回调</strong>:
<code>SoundTriggerHw3Compat</code>（或其他版本的 Compat 层）收到驱动层的回调。</p><ul><li>代码中 <code>ModelCallbackAdaper.recognitionCallback</code> 被调用，它将底层的事件封装为 <code>RecognitionEventSys</code> 并向上层传递 。</li></ul></li><li><p><strong>Middleware 层传递</strong>:
事件经过 <code>SoundTriggerMiddlewareImpl</code> 及一系列装饰器（Validation, Logging, Permission），最终到达 <code>SoundTriggerService</code>。</p></li><li><p><strong>SoundTriggerService 处理</strong>:
<code>SoundTriggerService</code> 通过其内部的 <code>SoundTriggerHelper</code> 处理该事件。</p><ul><li><code>SoundTriggerHelper</code> 的 <code>onRecognition</code> 方法被触发，识别出这是 <code>KeyphraseRecognitionEvent</code>（关键短语识别事件）。</li><li>紧接着调用 <code>onKeyphraseRecognitionLocked</code> 。</li></ul></li><li><p><strong>回调 VoiceInteractionManagerService (VIMS)</strong>:
<code>SoundTriggerHelper</code> 调用它持有的 <code>IRecognitionStatusCallback</code>。</p><ul><li>在 VIMS 的上下文中，这个回调实际上是 <code>HotwordDetectionConnection</code> 内部定义的 <code>SoundTriggerCallback</code> 。</li></ul></li></ol><h3 id=第二阶段路由至沙盒服务-routing-to-sandboxed-service>第二阶段：路由至沙盒服务 (Routing to Sandboxed Service)<a hidden class=anchor aria-hidden=true href=#第二阶段路由至沙盒服务-routing-to-sandboxed-service>#</a></h3><p><code>VoiceInteractionManagerService</code> 接收到硬件事件后，不会直接发给 Google App，而是先路由给运行在隔离进程中的 <code>HotwordDetectionService</code> 进行校验。</p><ol><li><p><strong>HotwordDetectionConnection 介入</strong>:
在 <code>HotwordDetectionConnection.SoundTriggerCallback.onKeyphraseDetected</code> 中：</p><ul><li>系统检查是否已建立 <code>HotwordDetectionConnection</code>（即是否使用了 <code>HotwordDetectionService</code>）。</li><li>如果是（“Hi Google”通常是这种情况），它会记录 metrics 并调用 <code>mHotwordDetectionConnection.detectFromDspSource</code> 。</li><li><em>注意</em>：如果没使用 HDS，它会走另一条路直接回调给 App，但现在主流流程都包含 HDS 。</li></ul></li><li><p><strong>会话分发</strong>:
<code>HotwordDetectionConnection</code> 找到对应的检测器会话（对于 DSP 触发，是 <code>DspTrustedHotwordDetectorSession</code>），并调用其 <code>detectFromDspSourceLocked</code> 方法 。</p></li><li><p><strong>发起 IPC 调用</strong>:
在 <code>DspTrustedHotwordDetectorSession.detectFromDspSourceLocked</code> 中：</p><ul><li>系统设置一个超时计时器（默认 3-4秒），防止沙盒服务无响应 。</li><li>调用 <code>service.detectFromDspSource(...)</code>。这里的 <code>service</code> 是 <code>ISandboxedDetectionService</code> 的 Binder 代理，指向运行在隔离进程中的 <code>HotwordDetectionService</code> 。</li><li>系统将硬件识别事件 (<code>recognitionEvent</code>) 和音频格式传递给沙盒服务。</li></ul></li></ol><h3 id=第三阶段沙盒服务校验-sandboxed-validation>第三阶段：沙盒服务校验 (Sandboxed Validation)<a hidden class=anchor aria-hidden=true href=#第三阶段沙盒服务校验-sandboxed-validation>#</a></h3><p>这一阶段发生在<strong>隔离进程</strong>中（代码逻辑由 Google App 提供，但运行环境受限）。</p><ol><li><p><strong>算法校验</strong>:
<code>HotwordDetectionService</code>（未在AOSP代码中，属于 App 侧实现）接收到音频数据，运行高精度的软件算法进行二次确认。</p></li><li><p><strong>返回结果</strong>:</p><ul><li><strong>校验通过</strong>: 沙盒服务调用回调接口的 <code>onDetected</code> 方法。</li><li><strong>校验失败</strong>: 沙盒服务调用 <code>onRejected</code>。</li></ul></li></ol><h3 id=第四阶段结果处理与分发-result-processing--dispatch>第四阶段：结果处理与分发 (Result Processing & Dispatch)<a hidden class=anchor aria-hidden=true href=#第四阶段结果处理与分发-result-processing--dispatch>#</a></h3><p>系统服务（System Server）收到沙盒服务的验证结果后，进行安全检查和数据封装，最后发给主应用。</p><ol><li><p><strong>接收校验结果</strong>:
<code>DspTrustedHotwordDetectorSession</code> 内部的匿名回调类收到 <code>onDetected</code> 调用 。</p></li><li><p><strong>安全检查</strong>:</p><ul><li><strong>超时检查</strong>: 检查是否已超时 。</li><li><strong>权限检查</strong>: 调用 <code>enforcePermissionsForDataDelivery</code> 确保主应用有录音权限 。</li><li><strong>Keyphrase ID 检查</strong>: 调用 <code>enforceExtraKeyphraseIdNotLeaked</code> 确保沙盒服务没有伪造硬件未检测到的短语 ID 。</li></ul></li><li><p><strong>音频流拷贝</strong>:
系统使用 <code>mHotwordAudioStreamCopier.startCopyingAudioStreams(result)</code>。这会将沙盒服务传递过来的音频流（Pipe）进行处理，以便主应用可以读取音频数据 。</p></li><li><p><strong>回调主应用 (Google App)</strong>:
最后，<code>DspTrustedHotwordDetectorSession</code> 调用 <code>externalCallback.onKeyphraseDetected(recognitionEvent, newResult)</code> 。</p><ul><li>这里的 <code>externalCallback</code> 是 Google App 主进程中 <code>AlwaysOnHotwordDetector</code> 注册的 Binder 回调。</li></ul></li><li><p><strong>UI 展示</strong>:
Google App 的主进程收到回调后，确认唤醒成功，随即调用 <code>VoiceInteractionService.showSession()</code>，请求 VIMS 显示语音助手界面（如底部的彩色条）。</p></li></ol><h3 id=时序图总结>时序图总结<a hidden class=anchor aria-hidden=true href=#时序图总结>#</a></h3><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
autonumber &#34;&lt;b&gt;[0]&#34;
skinparam backgroundColor white
skinparam sequence {
    ArrowColor Black
    ActorBorderColor Black
    LifeLineBorderColor Black
    ParticipantBorderColor Black
    ParticipantBackgroundColor White
}

participant &#34;Hardware/HAL\n(DSP)&#34; as HAL
participant &#34;SoundTrigger\nService&#34; as STS
participant &#34;HotwordDetection\nConnection&#34; as HDC
participant &#34;DspTrusted\nSession&#34; as DSP_Session
participant &#34;HotwordDetection\nService\n(Isolated Process)&#34; as HDS
participant &#34;Google App\n(Host Process)&#34; as App

== 阶段一：硬件触发 ==
HAL -&gt; STS: onRecognition(Event)
note right: 硬件检测到声学特征

STS -&gt; HDC: onKeyphraseDetected(Event)
note right: 路由至 VoiceInteractionManagerService 层

== 阶段二：发起校验 ==
HDC -&gt; DSP_Session: detectFromDspSource(Event)
activate DSP_Session

DSP_Session -&gt; HDS: detectFromDspSource(Event, Audio)
note right: 跨进程调用至隔离沙盒

== 阶段三：沙盒校验 ==
activate HDS
HDS -&gt; HDS: 运行软件算法\n(二次确认)
HDS -&gt; DSP_Session: onDetected(Result)
deactivate HDS

== 阶段四：结果分发 ==
DSP_Session -&gt; DSP_Session: 权限检查 &amp; 音频流拷贝

DSP_Session -&gt; App: onKeyphraseDetected(Result)
note right: 最终回调给主应用
deactivate DSP_Session

App -&gt; App: 播放提示音/显示UI
@enduml
</code></pre><p>通过这个流程，Android 既利用了 DSP 的低功耗特性（第一阶段），又利用了隔离进程的安全性来保护隐私（第三阶段），最后才唤醒主应用进行交互。</p><h2 id=注视唤醒>注视唤醒<a hidden class=anchor aria-hidden=true href=#注视唤醒>#</a></h2><p>&ldquo;注视唤醒&rdquo;（Visual Query Detection / Gaze Wake-up）的实现流程是利用 <strong><code>VisualQueryDetectionService</code></strong> (VQDS) 结合摄像头数据来触发语音交互。
这个过程利用了 Android 的沙盒机制来保护隐私，确保只有在检测到明确的“注视”意图后，数据才会流向主助理应用。
以下是基于代码分析的详细实现流程：</p><h3 id=核心组件角色>核心组件角色<a hidden class=anchor aria-hidden=true href=#核心组件角色>#</a></h3><ol><li><strong>Host App (Assistant)</strong>: 语音助手主应用（如 Google App），运行 <code>VisualQueryDetector</code>。</li><li><strong>System Server (VIMS)</strong>: 系统服务，负责权限管控和路由。具体由 <code>VoiceInteractionManagerService</code> 和 <code>HotwordDetectionConnection</code> 管理。</li><li><strong>Isolated Process (VQDS)</strong>: 运行 <code>VisualQueryDetectionService</code>，负责处理摄像头数据并检测注视行为。</li></ol><h3 id=详细时序流程>详细时序流程<a hidden class=anchor aria-hidden=true href=#详细时序流程>#</a></h3><h4 id=1-初始化与绑定-initialization>1. 初始化与绑定 (Initialization)<a hidden class=anchor aria-hidden=true href=#1-初始化与绑定-initialization>#</a></h4><p>在功能启用时，主应用需要初始化检测器并传递模型数据。</p><ul><li><strong>创建检测器</strong>: Host App 调用 <code>VoiceInteractionService.createVisualQueryDetector()</code> 。</li><li><strong>系统绑定</strong>: <code>VoiceInteractionManagerService</code> (VIMS) 接收请求，通过 <code>HotwordDetectionConnection</code> 绑定到 App 声明的 <code>VisualQueryDetectionService</code>（必须配置为隔离进程） 。</li><li><strong>传递配置</strong>: 系统调用 <code>onUpdateState</code> 将配置参数 (<code>PersistableBundle</code>) 和共享内存 (<code>SharedMemory</code>) 传递给隔离服务，用于加载视觉模型 。</li></ul><h4 id=2-启动感知-start-perceiving>2. 启动感知 (Start Perceiving)<a hidden class=anchor aria-hidden=true href=#2-启动感知-start-perceiving>#</a></h4><p>当设备屏幕点亮或进入特定状态时，Host App 请求开启检测。</p><ul><li><strong>发起请求</strong>: Host App 调用 <code>VisualQueryDetector.startRecognition()</code> 。</li><li><strong>系统路由</strong>: 该调用通过 Binder 传递给 VIMS 的 <code>startPerceiving</code> 方法 。</li><li><strong>连接层处理</strong>: VIMS 委托给 <code>HotwordDetectionConnection</code>，后者找到 <code>VisualQueryDetectorSession</code> 并调用 <code>startPerceivingLocked</code> 。</li><li><strong>启动沙盒服务</strong>: Session 通过 AIDL 调用隔离服务的 <code>detectWithVisualSignals(callback)</code> 方法 。</li><li><strong>服务响应</strong>: 在隔离进程中，<code>VisualQueryDetectionService</code> 的 <code>onStartDetection()</code> 被触发。开发者需在此处开启摄像头流 。</li></ul><h4 id=3-注视检测与注意力捕获-attention-detection>3. 注视检测与注意力捕获 (Attention Detection)<a hidden class=anchor aria-hidden=true href=#3-注视检测与注意力捕获-attention-detection>#</a></h4><p>这是“注视唤醒”的核心逻辑，发生在隔离进程中。</p><ul><li><strong>视觉处理</strong>: <code>VisualQueryDetectionService</code> 分析每一帧图像（使用 App 自带的算法库）。</li><li><strong>获得注意 (Attention Gained)</strong>: 当算法判断用户正在注视屏幕并有交互意图时，服务调用 <code>gainedAttention(VisualQueryAttentionResult)</code> 。</li><li><strong>系统反馈</strong>:<ul><li>系统服务端的 <code>VisualQueryDetectorSession</code> 收到回调。</li><li>它会通知系统的 <code>IVisualQueryDetectionAttentionListener</code>（通常是 SystemUI），在屏幕上显示“正在聆听”的视觉动效（如角落的光效） 。</li><li><strong>关键点</strong>: 这一步并不会将音频或视频发给主 App，仅通知系统“用户在看”。</li></ul></li></ul><h4 id=4-查询流式传输-query-streaming>4. 查询流式传输 (Query Streaming)<a hidden class=anchor aria-hidden=true href=#4-查询流式传输-query-streaming>#</a></h4><p>在获得注意力后，服务开始将识别到的意图（语音或多模态数据）传回主应用。</p><ul><li><strong>流式传输</strong>: 隔离服务调用 <code>streamQuery(String partialQuery)</code> 或 <code>streamQuery(VisualQueryDetectedResult)</code> 。</li><li><strong>权限检查</strong>: 系统服务（System Server）在转发数据前进行严格的权限检查：<ul><li>如果包含音频查询，检查 <code>RECORD_AUDIO</code> 权限 。</li><li>如果包含视觉辅助数据，检查 <code>CAMERA</code> 权限 。</li></ul></li><li><strong>数据分发</strong>: 检查通过后，<code>VisualQueryDetectorSession</code> 调用 <code>onQueryDetected</code> 。</li><li><strong>Host App 响应</strong>: 主应用中的 <code>VisualQueryDetector.Callback</code> 收到查询文本，开始执行搜索或指令 。</li></ul><h4 id=5-结束交互-completion>5. 结束交互 (Completion)<a hidden class=anchor aria-hidden=true href=#5-结束交互-completion>#</a></h4><ul><li><strong>完成</strong>: 当用户停止说话，服务调用 <code>finishQuery()</code> 。</li><li><strong>失去注意</strong>: 当用户移开视线，服务调用 <code>lostAttention()</code>，系统会取消 UI 动效并停止接收流 。</li></ul><h3 id=时序图解>时序图解<a hidden class=anchor aria-hidden=true href=#时序图解>#</a></h3><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
autonumber &#34;&lt;b&gt;[0]&#34;
skinparam backgroundColor white
skinparam sequence {
    ArrowColor Black
    ActorBorderColor Black
    LifeLineBorderColor Black
    ParticipantBorderColor Black
    ParticipantBackgroundColor White
}

participant &#34;Host App\n(Assistant)&#34; as App
participant &#34;System Server\n(VIMS/Session)&#34; as System
participant &#34;Isolated Service\n(VQDS)&#34; as Service

== 阶段一：启动 ==
App -&gt; System: startRecognition()
System -&gt; Service: detectWithVisualSignals()
Service -&gt; Service: 打开摄像头\n开始分析

== 阶段二：注视检测 (Gaze) ==
Service -&gt; Service: 算法检测到用户注视
Service -&gt; System: gainedAttention(AttentionResult)
System -&gt; System: 通知 SystemUI 显示光效\n(用户感知到设备已唤醒)

== 阶段三：唤醒与流传输 ==
Service -&gt; Service: 结合唇语/语音识别
Service -&gt; System: streamQuery(&#34;打开相册&#34;)
activate System
System -&gt; System: 检查 AppOps/权限 (Record Audio)
System -&gt; App: onQueryDetected(&#34;打开相册&#34;)
deactivate System
App -&gt; App: 执行指令

== 阶段四：结束 ==
Service -&gt; System: lostAttention() / finishQuery()
System -&gt; App: onQueryFinished()
App -&gt; System: stopRecognition()
System -&gt; Service: stopDetection()
@enduml
</code></pre><h3 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h3><p>“注视唤醒”的关键在于 <strong><code>VisualQueryDetectionService</code></strong>。它允许语音助手在<strong>不泄露摄像头原始数据</strong>的前提下（因为在隔离进程），实时判断用户的注视行为。只有当 <code>gainedAttention</code> 被调用后，系统才允许流式传输查询内容，从而实现了隐私与便利性的平衡。</p><h2 id=voiceinteractionmanagerservice>VoiceInteractionManagerService<a hidden class=anchor aria-hidden=true href=#voiceinteractionmanagerservice>#</a></h2><p><code>VoiceInteractionManagerService</code> (VIMS) 是 Android 系统中管理语音交互的核心系统服务。它作为系统服务运行在 <code>system_server</code> 进程中，负责协调用户选定的语音助手应用（如 Google Assistant）与系统其他组件（如 ActivityManager, WindowManager, SoundTrigger 等）之间的交互。</p><p>以下是基于源代码对 <code>VoiceInteractionManagerService</code> 的详细设计讲解：</p><h3 id=1-职责-responsibilities>1. 职责 (Responsibilities)<a hidden class=anchor aria-hidden=true href=#1-职责-responsibilities>#</a></h3><p><code>VoiceInteractionManagerService</code> 的主要职责可以归纳为以下几点：</p><ol><li><p><strong>语音助手生命周期管理</strong>:</p><ul><li>负责管理当前激活的语音交互服务（<code>VoiceInteractionService</code>）。</li><li>处理服务的绑定与解绑，特别是在用户切换、设置变更或应用更新时 。</li><li>确保同一时间只有一个活跃的语音助手服务 。</li></ul></li><li><p><strong>会话 (Session) 管理</strong>:</p><ul><li>处理来自语音助手的 <code>showSession</code> 请求，协调显示语音交互界面（Session UI）。</li><li>管理 Session 的创建、显示、隐藏和销毁。</li><li>协助传递上下文数据（Assist Data），如屏幕截图和视图结构，给语音助手 。</li></ul></li><li><p><strong>热词检测 (Hotword Detection) 协调</strong>:</p><ul><li>作为上层应用与底层 <code>SoundTrigger</code> 硬件之间的桥梁。</li><li>管理 <code>AlwaysOnHotwordDetector</code> 和 <code>HotwordDetectionService</code>（在隔离进程中运行的热词检测服务）。</li><li>处理 DSP 模型的加载、更新和卸载 。</li><li>验证和管理 <code>HotwordDetectionService</code> 的身份和权限 。</li></ul></li><li><p><strong>视觉查询 (Visual Query) 支持</strong>:</p><ul><li>支持 <code>VisualQueryDetectionService</code>，允许助手利用摄像头数据进行检测（如注视唤醒）。</li></ul></li><li><p><strong>权限与安全性</strong>:</p><ul><li>强制执行权限检查，例如 <code>BIND_VOICE_INTERACTION</code>，确保只有拥有正确权限的应用才能成为语音助手 。</li><li>管理临时权限授予，允许助手访问启动的 Activity 数据 。</li></ul></li><li><p><strong>多用户支持</strong>:</p><ul><li>维护每个用户的语音助手配置，并在用户切换时重新加载相应的服务实现 。</li></ul></li></ol><h3 id=2-实现-implementation>2. 实现 (Implementation)<a hidden class=anchor aria-hidden=true href=#2-实现-implementation>#</a></h3><p>VIMS 的实现采用了典型的 Android 系统服务模式，分为服务入口、Binder 桩（Stub）和每用户实现（Impl）。</p><h4 id=21-核心类结构>2.1 核心类结构<a hidden class=anchor aria-hidden=true href=#21-核心类结构>#</a></h4><ol><li><p><strong><code>VoiceInteractionManagerService</code></strong>:</p><ul><li>继承自 <code>SystemService</code>，是服务的入口点。</li><li>负责发布 Binder 服务和本地服务 (<code>VoiceInteractionManagerInternal</code>) 。</li><li>监听用户生命周期事件 (<code>onUserStarting</code>, <code>onUserSwitching</code>) 。</li></ul></li><li><p><strong><code>VoiceInteractionManagerServiceStub</code></strong> (内部类):</p><ul><li>继承自 <code>IVoiceInteractionManagerService.Stub</code>。</li><li>实现了跨进程通信（IPC）接口，供外部应用（如 Settings, SystemUI, 语音助手 App）调用。</li><li>主要负责接收请求，进行权限检查，然后转发给当前用户的 <code>VoiceInteractionManagerServiceImpl</code> 。</li></ul></li><li><p><strong><code>VoiceInteractionManagerServiceImpl</code></strong>:</p><ul><li><strong>核心逻辑所在</strong>。每个 <code>Impl</code> 实例对应一个特定的用户和该用户选定的语音助手组件。</li><li>维护与 <code>VoiceInteractionService</code> 的 <code>ServiceConnection</code> 。</li><li>持有 <code>HotwordDetectionConnection</code> 来管理热词检测 。</li><li>持有 <code>VoiceInteractionSessionConnection</code> 来管理 UI 会话 。</li></ul></li></ol><h4 id=22-启动时序-startup-sequence>2.2 启动时序 (Startup Sequence)<a hidden class=anchor aria-hidden=true href=#22-启动时序-startup-sequence>#</a></h4><p>VIMS 的启动流程如下：</p><ol><li><p><strong><code>onStart()</code></strong>:</p><ul><li>由 <code>SystemServer</code> 调用。</li><li>发布 Binder 服务 <code>voiceinteraction</code>。</li><li>发布本地服务 <code>VoiceInteractionManagerInternal</code>，供系统进程内其他服务调用。</li><li>向 <code>ActivityManagerInternal</code> 注册监听器，以便在 Activity 销毁时收到通知 。</li></ul></li><li><p><strong><code>onBootPhase(int phase)</code></strong>:</p><ul><li><strong><code>PHASE_SYSTEM_SERVICES_READY</code></strong>: 获取依赖的系统服务，如 <code>ShortcutServiceInternal</code> 和 <code>SoundTriggerInternal</code> 。</li><li><strong><code>PHASE_THIRD_PARTY_APPS_CAN_START</code></strong>: 调用 <code>mServiceStub.systemRunning(isSafeMode())</code>，开始真正的初始化 。</li></ul></li><li><p><strong><code>systemRunning()</code> (在 Stub 中)</strong>:</p><ul><li>注册 <code>PackageMonitor</code> 监听包的变化（如语音助手被卸载或更新）。</li><li>注册 <code>SettingsObserver</code> 监听 <code>Settings.Secure.VOICE_INTERACTION_SERVICE</code> 的变化。</li><li>调用 <code>initForUser()</code> 初始化当前用户的状态 。</li><li>调用 <code>switchImplementationIfNeeded()</code> 尝试绑定服务 。</li></ul></li><li><p><strong><code>initForUser(int userHandle)</code></strong>:</p><ul><li>从 <code>Settings.Secure</code> 读取当前用户的语音交互服务配置。</li><li>如果未设置，尝试根据配置或系统默认值找到一个可用的语音助手，并更新设置 。</li><li>初始化语音识别器组件信息 。</li></ul></li><li><p><strong><code>switchImplementationIfNeeded()</code></strong>:</p><ul><li>这是连接建立的关键点。</li><li>它读取当前的设置，解析出组件名 (<code>ComponentName</code>)。</li><li>创建一个新的 <code>VoiceInteractionManagerServiceImpl</code> 实例，传入上下文和组件信息 。</li><li>调用 <code>mImpl.startLocked()</code> 。</li></ul></li><li><p><strong><code>VoiceInteractionManagerServiceImpl.startLocked()</code></strong>:</p><ul><li>创建 <code>Intent</code> 指向语音助手组件。</li><li>调用 <code>mContext.bindServiceAsUser()</code> 绑定服务。这里使用了 <code>BIND_FOREGROUND_SERVICE</code> 等标志来确保服务优先级 。</li></ul></li><li><p><strong><code>onServiceConnected()</code></strong> (在 Impl 的 Connection 中):</p><ul><li>当服务绑定成功后，获取 <code>IVoiceInteractionService</code> 接口。</li><li>调用远程服务的 <code>ready()</code> 方法，通知语音助手应用系统已准备就绪 。</li></ul></li></ol><p>至此，VIMS 启动完成，并与当前用户的语音助手建立了双向通信通道。当用户触发语音助手时，VIMS 将通过这个通道发送请求；反之，当语音助手需要显示 UI 或访问硬件时，也会通过这个通道回调 VIMS。</p><h2 id=voiceinteractionmanagerserviceimpl>VoiceInteractionManagerServiceImpl<a hidden class=anchor aria-hidden=true href=#voiceinteractionmanagerserviceimpl>#</a></h2><p><code>VoiceInteractionManagerServiceImpl</code> 是 Android 语音交互框架中负责<strong>具体业务实现</strong>的核心类。每个实例对应<strong>一个特定的用户</strong>和<strong>一个选定的语音交互服务组件</strong>（例如 Google Assistant）。它全权负责管理该语音助手在当前用户下的生命周期、会话连接、硬件绑定以及业务逻辑转发。</p><p>以下是该类的详细职责描述：</p><h3 id=1-语音助手服务的生命周期管理>1. 语音助手服务的生命周期管理<a hidden class=anchor aria-hidden=true href=#1-语音助手服务的生命周期管理>#</a></h3><p>该类负责维护与实际语音助手应用（<code>VoiceInteractionService</code>）的绑定关系。</p><ul><li><strong>服务绑定与验证</strong>：在构造函数中，它会解析 <code>VoiceInteractionServiceInfo</code> 来验证服务的有效性 。通过 <code>startLocked</code> 方法，它使用 <code>bindServiceAsUser</code> 绑定到目标服务组件 。</li><li><strong>连接状态监控</strong>：通过 <code>ServiceConnection</code> (<code>mConnection</code>) 监听服务的连接状态。<ul><li><strong>连接成功</strong>：在 <code>onServiceConnected</code> 中获取 <code>IVoiceInteractionService</code> 接口代理并调用 <code>ready()</code> 。</li><li><strong>连接断开</strong>：在 <code>onServiceDisconnected</code> 中清理引用并重置 Hotword 连接 。</li><li><strong>意外死亡</strong>：在 <code>onBindingDied</code> 中处理服务意外退出的情况，例如检测是否由用户强制停止 。</li></ul></li><li><strong>资源清理</strong>：在 <code>shutdownLocked</code> 中负责解绑服务，取消当前会话，并断开 Hotword 检测连接 。</li></ul><h3 id=2-会话-session-管理>2. 会话 (Session) 管理<a hidden class=anchor aria-hidden=true href=#2-会话-session-管理>#</a></h3><p>它管理着语音助手的 UI 界面（即“会话”），是系统与助手 UI 之间的桥梁。</p><ul><li><strong>会话创建与显示</strong>：<code>showSessionLocked</code> 方法负责创建 <code>VoiceInteractionSessionConnection</code> 对象（若尚不存在），这代表了助手应用显示的 UI 层 。它还会计算唯一的 <code>KEY_SHOW_SESSION_ID</code> 并传递给服务 。</li><li><strong>可见性控制</strong>：提供 <code>hideSessionLocked</code> 方法来隐藏当前会话 ，以及 <code>finishLocked</code> 方法来销毁会话 。</li><li><strong>窗口交互</strong>：处理关闭系统对话框 (<code>closeSystemDialogsLocked</code>) 和保持屏幕常亮 (<code>setKeepAwakeLocked</code>) 的请求。</li></ul><h3 id=3-检测器管理-hotword--visual-query>3. 检测器管理 (Hotword & Visual Query)<a hidden class=anchor aria-hidden=true href=#3-检测器管理-hotword--visual-query>#</a></h3><p>它是上层应用与底层检测硬件/隔离服务之间的枢纽，持有一个 <code>HotwordDetectionConnection</code> 对象来具体执行这些任务。</p><ul><li><strong>连接持有</strong>：通过 <code>mHotwordDetectionConnection</code> 变量持有对热词检测连接的引用 。</li><li><strong>初始化检测器</strong>：<code>initAndVerifyDetectorLocked</code> 方法负责校验权限、配置隔离进程，并初始化 Hotword 或 Visual Query 检测器 。</li><li><strong>状态与数据更新</strong>：<code>updateStateLocked</code> 负责将配置参数 (<code>PersistableBundle</code>) 和模型数据 (<code>SharedMemory</code>) 更新到底层检测服务 。</li><li><strong>麦克风与视觉控制</strong>：提供 <code>startListeningFromMicLocked</code> 、<code>startPerceivingLocked</code> 等方法来控制音频和视觉信号的捕获。</li></ul><h3 id=4-辅助功能与-activity-交互>4. 辅助功能与 Activity 交互<a hidden class=anchor aria-hidden=true href=#4-辅助功能与-activity-交互>#</a></h3><p>它充当了系统 Activity 管理器 (<code>ActivityTaskManager</code>) 和语音助手之间的代理，赋予助手操作 App 的能力。</p><ul><li><strong>启动 Activity</strong>：实现了 <code>startVoiceActivityLocked</code> 和 <code>startAssistantActivityLocked</code> ，允许语音助手在特定模式（如隐藏模式或不激活模式）下启动 Activity。</li><li><strong>Direct Actions</strong>：通过 <code>requestDirectActionsLocked</code> 和 <code>performDirectActionLocked</code> ，允许语音助手查询和执行当前前台 App 的应用内动作（App Slices/Intents）。</li><li><strong>权限授予</strong>：通过 <code>grantImplicitAccessLocked</code> 方法，向语音助手授予对特定 Activity 的隐式访问权限 。</li></ul><h3 id=5-系统设置与监听>5. 系统设置与监听<a hidden class=anchor aria-hidden=true href=#5-系统设置与监听>#</a></h3><ul><li><strong>无障碍设置监听</strong>：内部类 <code>AccessibilitySettingsContentObserver</code> 负责监听 <code>VISUAL_QUERY_ACCESSIBILITY_DETECTION_ENABLED</code> 设置的变化，并通知注册的监听器 。</li><li><strong>可见性监听</strong>：支持启动和停止监听可见 Activity 的变化 (<code>startListeningVisibleActivityChangedLocked</code>)，以便助手能对屏幕内容做出反应 。</li></ul><h3 id=总结-1>总结<a hidden class=anchor aria-hidden=true href=#总结-1>#</a></h3><p><code>VoiceInteractionManagerServiceImpl</code> 是 Android 语音交互服务的<strong>具体执行者</strong>。它在一个特定用户的上下文中，协调了<strong>服务绑定</strong>、<strong>UI 会话展示</strong>、<strong>底层硬件检测</strong>以及<strong>与其他系统服务（如 ATM/AM）的交互</strong>。</p><p>在 <code>VoiceInteractionManagerServiceImpl</code> 类中，<code>mComponent</code> 是一个关键的成员变量。以下是关于它的详细解释：</p><h3 id=1-mcomponent-是什么>1. <code>mComponent</code> 是什么？<a hidden class=anchor aria-hidden=true href=#1-mcomponent-是什么>#</a></h3><ul><li><strong>类型</strong>: <code>ComponentName</code></li><li><strong>含义</strong>: 它代表了<strong>当前选定的语音交互服务（Voice Interaction Service）的组件名称</strong>。<ul><li>简单来说，就是用户在系统设置中选择的那个“默认数字助理应用”的具体服务组件（例如 Google Assistant 的 <code>com.google.android.googlequicksearchbox/com.google.android.voiceinteraction.GsaVoiceInteractionService</code>）。</li></ul></li><li><strong>作用</strong>: 这个变量用于在系统内部唯一标识当前正在为用户服务的那个语音助手组件。后续的绑定服务（bindService）、启动 Activity 等操作都需要使用这个组件名来指定目标 。</li></ul><h3 id=2-它是如何被设置的>2. 它是如何被设置的？<a hidden class=anchor aria-hidden=true href=#2-它是如何被设置的>#</a></h3><p>它是通过 <strong><code>VoiceInteractionManagerServiceImpl</code> 的构造函数</strong> 进行初始化的。</p><ul><li><strong>代码位置</strong>:
在 <code>VoiceInteractionManagerServiceImpl.java</code> 的构造函数中：<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-java data-lang=java><span style=display:flex><span>VoiceInteractionManagerServiceImpl(Context context, Handler handler,
</span></span><span style=display:flex><span>        VoiceInteractionManagerService.<span style=color:#a6e22e>VoiceInteractionManagerServiceStub</span> stub,
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>int</span> userHandle, ComponentName service) { <span style=color:#75715e>// &lt;--- 这里的 service 参数</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ...</span>
</span></span><span style=display:flex><span>    mComponent <span style=color:#f92672>=</span> service; <span style=color:#75715e>// &lt;--- 在这里赋值</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// ...</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ul><h3 id=3-谁负责传入这个值谁调用了构造函数>3. 谁负责传入这个值？（谁调用了构造函数？）<a hidden class=anchor aria-hidden=true href=#3-谁负责传入这个值谁调用了构造函数>#</a></h3><p>是由系统服务的主控类 <strong><code>VoiceInteractionManagerService</code></strong> (VIMS) 负责创建并传入的。</p><p>具体流程如下：</p><ol><li><p><strong>读取设置</strong>:
在 <code>VoiceInteractionManagerService.java</code> 中，方法 <code>switchImplementationIfNeededNoTracingLocked</code> 会从系统安全设置中读取当前用户选定的语音服务字符串：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-java data-lang=java><span style=display:flex><span>String curService <span style=color:#f92672>=</span> Settings.<span style=color:#a6e22e>Secure</span>.<span style=color:#a6e22e>getStringForUser</span>(
</span></span><span style=display:flex><span>        mResolver, Settings.<span style=color:#a6e22e>Secure</span>.<span style=color:#a6e22e>VOICE_INTERACTION_SERVICE</span>, mCurUser);
</span></span></code></pre></div></li><li><p><strong>解析组件名</strong>:
将读取到的字符串解析为 <code>ComponentName</code> 对象（代码中命名为 <code>serviceComponent</code>）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-java data-lang=java><span style=display:flex><span>serviceComponent <span style=color:#f92672>=</span> ComponentName.<span style=color:#a6e22e>unflattenFromString</span>(curService);
</span></span></code></pre></div></li><li><p><strong>实例化 Impl 并传入</strong>:
VIMS 实例化 <code>VoiceInteractionManagerServiceImpl</code>，并将 <code>serviceComponent</code> 作为参数传进去：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-java data-lang=java><span style=display:flex><span>setImplLocked(<span style=color:#66d9ef>new</span> VoiceInteractionManagerServiceImpl(mContext,
</span></span><span style=display:flex><span>        UiThread.<span style=color:#a6e22e>getHandler</span>(), <span style=color:#66d9ef>this</span>, mCurUser, serviceComponent)); <span style=color:#75715e>// &lt;--- 传入 mComponent</span>
</span></span></code></pre></div></li></ol><h3 id=总结-2>总结<a hidden class=anchor aria-hidden=true href=#总结-2>#</a></h3><ul><li><strong>是什么</strong>: 当前激活的语音助手服务的组件名（包名+类名）。</li><li><strong>谁设置</strong>: 由 <strong><code>VoiceInteractionManagerService</code></strong> 读取系统设置 (<code>Settings.Secure.VOICE_INTERACTION_SERVICE</code>) 后，在创建 <code>VoiceInteractionManagerServiceImpl</code> 实例时通过构造函数传入的。</li></ul><h2 id=voiceinteractionserviceinfo>VoiceInteractionServiceInfo<a hidden class=anchor aria-hidden=true href=#voiceinteractionserviceinfo>#</a></h2><p><code>VoiceInteractionServiceInfo</code> 的构造过程主要发生在它的构造函数中。它通过 <code>PackageManager</code> 获取服务的 <code>ServiceInfo</code>，然后读取该服务在 <code>AndroidManifest.xml</code> 中声明的 <code>&lt;meta-data></code> 所指向的 XML 资源文件。</p><p>以下是具体的解析步骤：</p><h3 id=1-入口与权限检查>1. 入口与权限检查<a hidden class=anchor aria-hidden=true href=#1-入口与权限检查>#</a></h3><p>构造函数接收 <code>PackageManager</code> 和 <code>ServiceInfo</code> 作为参数 。</p><ul><li><strong>权限验证</strong>：首先检查该服务是否声明了 <code>android.permission.BIND_VOICE_INTERACTION</code> 权限。如果没有，解析过程会终止并设置错误信息 。</li></ul><h3 id=2-加载-xml-元数据>2. 加载 XML 元数据<a hidden class=anchor aria-hidden=true href=#2-加载-xml-元数据>#</a></h3><p>代码调用 <code>si.loadXmlMetaData</code> 方法加载元数据 。</p><ul><li><strong>元数据名称</strong>：它查找的键值是 <code>VoiceInteractionService.SERVICE_META_DATA</code> ，即 <code>"android.voice_interaction"</code> 。</li><li>这对应于 AndroidManifest.xml 中服务声明内的 <code>&lt;meta-data android:name="android.voice_interaction" android:resource="@xml/..." /></code>。</li></ul><h3 id=3-解析-xml-标签>3. 解析 XML 标签<a hidden class=anchor aria-hidden=true href=#3-解析-xml-标签>#</a></h3><p>使用 <code>XmlResourceParser</code> 解析加载的 XML 资源：</p><ul><li><strong>根标签验证</strong>：解析器跳过开始文档事件，检查根标签（Root Tag）的名称是否为 <code>"voice-interaction-service"</code>。如果不是，则报错返回 。</li></ul><h3 id=4-读取属性-attributes>4. 读取属性 (Attributes)<a hidden class=anchor aria-hidden=true href=#4-读取属性-attributes>#</a></h3><p>一旦验证了根标签，代码会获取应用程序的资源 (<code>Resources</code>)，并使用 <code>obtainAttributes</code> 方法读取 XML 中的属性值 。这些属性被映射到 <code>VoiceInteractionServiceInfo</code> 的成员变量中：</p><ul><li><strong>sessionService</strong> (<code>android:sessionService</code>): 定义会话服务的组件名 。</li><li><strong>recognitionService</strong> (<code>android:recognitionService</code>): 定义语音识别服务的组件名 。</li><li><strong>settingsActivity</strong> (<code>android:settingsActivity</code>): 定义设置页面的 Activity 。</li><li><strong>supportsAssist</strong> (<code>android:supportsAssist</code>): 布尔值，是否支持辅助功能 。</li><li><strong>supportsLaunchFromKeyguard</strong> (<code>android:supportsLaunchVoiceAssistFromKeyguard</code>): 布尔值，是否支持从锁屏启动 。</li><li><strong>supportsLocalInteraction</strong> (<code>android:supportsLocalInteraction</code>): 布尔值，是否支持本地交互 。</li><li><strong>hotwordDetectionService</strong> (<code>android:hotwordDetectionService</code>): 定义 Hotword 检测服务的组件名 。</li><li><strong>visualQueryDetectionService</strong> (<code>android:visualQueryDetectionService</code>): 定义视觉查询检测服务的组件名 。</li></ul><h3 id=5-强制性字段检查>5. 强制性字段检查<a hidden class=anchor aria-hidden=true href=#5-强制性字段检查>#</a></h3><p>在资源回收 (<code>array.recycle()</code>) 后，代码会检查两个必须存在的属性：</p><ul><li><strong>sessionService</strong> 必须存在，否则报错 。</li><li><strong>recognitionService</strong> 必须存在，否则报错 。</li></ul><h3 id=总结-3>总结<a hidden class=anchor aria-hidden=true href=#总结-3>#</a></h3><p><code>VoiceInteractionServiceInfo</code> 的构造本质上是对开发者在 <code>res/xml/</code> 目录下定义的配置文件的<strong>反序列化过程</strong>。系统通过这个过程知道了语音助手由哪些组件构成（如会话服务、热词服务、视觉服务等）以及它支持哪些特性。</p><h2 id=voiceinteractionservice>VoiceInteractionService<a hidden class=anchor aria-hidden=true href=#voiceinteractionservice>#</a></h2><p>根据 <code>frameworks/base/core/java/android/service/voice/VoiceInteractionService.java</code> 文件的定义和注释，<code>VoiceInteractionService</code> (VIS) 是当前被选中的语音交互应用的<strong>顶层服务（Top-level service）</strong>。它由 Android 系统保持长期运行，充当语音助手应用的“大脑”或“入口”。</p><p>以下是它的主要职责：</p><h3 id=1-充当常驻后台的管家>1. 充当常驻后台的“管家”<a hidden class=anchor aria-hidden=true href=#1-充当常驻后台的管家>#</a></h3><ul><li><strong>始终运行</strong>：它是当前用户选定的语音交互器，由系统保持始终运行（Always Running）。</li><li><strong>轻量级设计</strong>：由于它始终运行，设计原则要求它必须<strong>尽可能轻量</strong> 。它不应处理繁重的 UI 渲染或复杂的业务逻辑，这些应交给独立的 <code>VoiceInteractionSessionService</code> 处理 。</li></ul><h3 id=2-管理热词与检测器-detection-management>2. 管理热词与检测器 (Detection Management)<a hidden class=anchor aria-hidden=true href=#2-管理热词与检测器-detection-management>#</a></h3><p>它是创建和管理各种唤醒检测机制的入口：</p><ul><li><strong>DSP 热词检测</strong>：通过 <code>createAlwaysOnHotwordDetector</code> 创建基于硬件 DSP 的低功耗热词检测器（如 &ldquo;Hey Google&rdquo;）。</li><li><strong>软件/麦克风检测</strong>：通过 <code>createHotwordDetector</code> 创建基于软件或麦克风输入的检测器 。</li><li><strong>视觉查询检测</strong>：通过 <code>createVisualQueryDetector</code> 创建视觉查询检测器，利用摄像头信号进行唤醒（如“注视唤醒”）。</li><li><strong>模型管理</strong>：处理声学模型的更新通知 (<code>onSoundModelsChangedInternal</code>) 。</li></ul><h3 id=3-发起交互会话-session-initiation>3. 发起交互会话 (Session Initiation)<a hidden class=anchor aria-hidden=true href=#3-发起交互会话-session-initiation>#</a></h3><p>虽然它自己不显示 UI，但它负责决定<strong>何时</strong>显示 UI：</p><ul><li><strong>启动会话</strong>：通过调用 <code>showSession()</code> 方法，请求系统启动与用户交互的 <code>VoiceInteractionSession</code> 。这通常发生在检测到热词或收到其他触发信号时。</li><li><strong>传递参数</strong>：它可以通过 <code>Bundle</code> 向 Session 传递参数 。</li></ul><h3 id=4-处理系统生命周期回调>4. 处理系统生命周期回调<a hidden class=anchor aria-hidden=true href=#4-处理系统生命周期回调>#</a></h3><p>它接收来自系统的关键状态通知：</p><ul><li><strong>初始化</strong>：<code>onReady()</code> 方法在系统准备好接收交互时被调用，这是服务进行初始化的时机 。</li><li><strong>销毁</strong>：<code>onShutdown()</code> 方法在服务被关闭时调用 。</li><li><strong>配置变更</strong>：处理支持的语音动作查询 (<code>onGetSupportedVoiceActions</code>) 。</li></ul><h3 id=5-锁屏交互支持>5. 锁屏交互支持<a hidden class=anchor aria-hidden=true href=#5-锁屏交互支持>#</a></h3><ul><li><strong>锁屏唤醒</strong>：通过 <code>onLaunchVoiceAssistFromKeyguard()</code> 方法，处理用户在锁屏状态下触发语音助手的操作 。这允许助手在锁屏之上显示特定的 UI。</li></ul><h3 id=6-设置与上下文控制>6. 设置与上下文控制<a hidden class=anchor aria-hidden=true href=#6-设置与上下文控制>#</a></h3><ul><li><strong>上下文限制</strong>：通过 <code>setDisabledShowContext()</code> 方法，可以控制在启动会话时是否禁用获取屏幕截图或辅助数据（Assist Data）。</li><li><strong>UI 提示</strong>：通过 <code>setUiHints()</code> 向系统 UI 提供提示信息 。</li></ul><p><strong>总结</strong>：
<code>VoiceInteractionService</code> 是语音助手应用的<strong>后台指挥官</strong>。它负责监听唤醒词（通过管理 Detector），并在需要时“叫醒”负责显示 UI 和处理复杂交互的 <code>VoiceInteractionSessionService</code>（前台执行者）。</p><h2 id=visualquerydetectionservice>VisualQueryDetectionService<a hidden class=anchor aria-hidden=true href=#visualquerydetectionservice>#</a></h2><p>是的，<code>VisualQueryDetectionService</code> <strong>也是在语音助理 App（例如 Google App）中实现的</strong>，并且其运行机制与 <code>HotwordDetectionService</code> 高度相似。</p><p>它虽然是语音助理 APK 的一部分，但受到 Android 系统严格的沙盒限制。</p><h3 id=1-代码位置在助理-app-内>1. 代码位置：在助理 App 内<a hidden class=anchor aria-hidden=true href=#1-代码位置在助理-app-内>#</a></h3><p>根据源代码注释，<code>VisualQueryDetectionService</code> 是“由想要提供视觉查询检测功能的<strong>应用程序实现</strong>的” 。</p><ul><li>开发者需要在语音助理 App 的工程中继承 <code>VisualQueryDetectionService</code> 并实现其抽象方法（如 <code>onStartDetection</code>）。</li></ul><h3 id=2-配置方式元数据声明>2. 配置方式：元数据声明<a hidden class=anchor aria-hidden=true href=#2-配置方式元数据声明>#</a></h3><p>系统通过读取 <code>VoiceInteractionService</code> 的 XML 元数据来找到这个服务。</p><ul><li>在 <code>VoiceInteractionServiceInfo.java</code> 中可以看到，系统会解析 <code>android:visualQueryDetectionService</code> 属性 。</li><li>这意味着开发者必须在 <code>res/xml/</code> 下的配置文件中明确指定这个类的路径。</li></ul><h3 id=3-运行环境强制隔离进程>3. 运行环境：强制隔离进程<a hidden class=anchor aria-hidden=true href=#3-运行环境强制隔离进程>#</a></h3><p>与 <code>HotwordDetectionService</code> 一样，<code>VisualQueryDetectionService</code> <strong>必须</strong>配置为在隔离进程中运行。</p><ul><li>在系统绑定该服务之前，<code>HotwordDetectionConnection</code> 会进行严格检查：<code>verifyDetectorForVisualQueryDetectionLocked</code> 方法会验证该服务是否设置了 <code>android:isolatedProcess="true"</code> 。</li><li>如果没有配置隔离进程，系统会抛出 <code>IllegalStateException</code> 并拒绝启动服务。</li></ul><h3 id=4-进程共享-process-sharing>4. 进程共享 (Process Sharing)<a hidden class=anchor aria-hidden=true href=#4-进程共享-process-sharing>#</a></h3><p>值得注意的是，Android 允许 <code>VisualQueryDetectionService</code> 和 <code>HotwordDetectionService</code> 运行在<strong>同一个</strong>隔离进程中。</p><ul><li>在 <code>verifyProcessSharingLocked</code> 方法中，系统会检查这两个服务是否都配置了 <code>android:allowSharedIsolatedProcess="true"</code> 。</li><li>如果配置允许，它们可以共享同一个沙盒环境，从而更方便地共享内存或状态（例如共享某些底层推理引擎资源），同时依然与主应用进程隔离。</li></ul><h3 id=总结-4>总结<a hidden class=anchor aria-hidden=true href=#总结-4>#</a></h3><p><code>VisualQueryDetectionService</code> 是语音助理 App 提供的代码，但被 Android 系统强制“关押”在一个独立的、受限的沙盒进程中运行，目的是在处理摄像头数据（视觉信号）时保护用户隐私。</p><h2 id=hotworddetectionservice>HotwordDetectionService<a hidden class=anchor aria-hidden=true href=#hotworddetectionservice>#</a></h2><p><code>HotwordDetectionService</code> 虽然代码包含在语音助理 App（如 Google App）的 APK 中，但它能够运行在隔离进程中，完全依赖于 <strong>Android 系统服务（System Server）的特殊绑定机制</strong>。</p><p>简单来说：<strong>代码是 App 提供的，但启动方式和运行环境是由 Android 系统强制指定的。</strong></p><p>以下是其运行在隔离进程中的具体技术实现步骤：</p><h3 id=1-app-侧的强制声明-manifest-declaration>1. App 侧的强制声明 (Manifest Declaration)<a hidden class=anchor aria-hidden=true href=#1-app-侧的强制声明-manifest-declaration>#</a></h3><p>首先，语音助理 App 必须在其 <code>AndroidManifest.xml</code> 中声明该服务，并显式设置 <code>android:isolatedProcess="true"</code>。
虽然我们无法直接看到 Manifest 文件，但在 <code>VoiceInteractionManagerServiceImpl.java</code> 中，系统会严格检查这一属性。</p><ul><li><strong>检查逻辑</strong>：在绑定服务前，系统会调用 <code>isIsolatedProcessLocked</code> 方法，检查该服务的 <code>ServiceInfo.flags</code> 是否包含 <code>FLAG_ISOLATED_PROCESS</code>。</li><li><strong>强制执行</strong>：如果开发者忘记在 Manifest 中声明这一属性，系统会抛出 <code>IllegalStateException</code> 并拒绝启动该服务。</li></ul><h3 id=2-系统侧的特殊绑定-system-binding>2. 系统侧的特殊绑定 (System Binding)<a hidden class=anchor aria-hidden=true href=#2-系统侧的特殊绑定-system-binding>#</a></h3><p>普通的 Service 绑定使用 <code>bindService</code>，但对于 <code>HotwordDetectionService</code>，系统使用的是 <strong><code>bindIsolatedService</code></strong>。</p><p>在 <code>HotwordDetectionConnection.java</code> 的 <code>ServiceConnection</code> 内部类中可以看到这一关键调用：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#66d9ef>boolean</span> bindResult <span style=color:#f92672>=</span> mContext.<span style=color:#a6e22e>bindIsolatedService</span>(
</span></span><span style=display:flex><span>    mIntent,
</span></span><span style=display:flex><span>    Context.<span style=color:#a6e22e>BIND_AUTO_CREATE</span> <span style=color:#f92672>|</span> Context.<span style=color:#a6e22e>BIND_FOREGROUND_SERVICE</span> <span style=color:#f92672>|</span> mBindingFlags,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;hotword_detector_&#34;</span> <span style=color:#f92672>+</span> mInstanceNumber, <span style=color:#75715e>// 实例名称</span>
</span></span><span style=display:flex><span>    mExecutor,
</span></span><span style=display:flex><span>    serviceConnection);
</span></span></code></pre></div><ul><li><strong><code>bindIsolatedService</code> 的作用</strong>：这个 API 指示 Activity Manager (AMS) 启动一个新的进程。这个进程是<strong>临时的、匿名的</strong>，并且被分配了一个特殊的 <strong>Isolated UID</strong>。</li><li><strong>独立的 UID</strong>：这个 Isolated UID 与主 App 的 UID 不同（例如主 App 是 u0_a123，隔离进程可能是 u0_i99）。Linux 内核的权限管理机制会基于 UID 隔离资源，使得这个进程无法访问主 App 的私有数据目录，也没有网络权限。</li></ul><h3 id=3-实例区分-instance-name>3. 实例区分 (Instance Name)<a hidden class=anchor aria-hidden=true href=#3-实例区分-instance-name>#</a></h3><p>在调用 <code>bindIsolatedService</code> 时，系统还会传入一个 <code>instanceName</code>（例如 <code>"hotword_detector_0"</code>）。</p><ul><li>这允许系统区分同一个服务的不同隔离实例。如果服务崩溃重启，系统可以通过计数器 (<code>mInstanceNumber</code>) 创建一个新的、干净的隔离进程实例。</li></ul><h3 id=4-共享隔离进程-shared-isolated-process>4. 共享隔离进程 (Shared Isolated Process)<a hidden class=anchor aria-hidden=true href=#4-共享隔离进程-shared-isolated-process>#</a></h3><p>如果系统属性 <code>ro.hotword.visual_query_service_enabled</code> 为真，且应用声明了支持，<code>HotwordDetectionService</code> 还可以与 <code>VisualQueryDetectionService</code> 共享同一个隔离进程。</p><ul><li>代码中会检查 <code>Context.BIND_SHARED_ISOLATED_PROCESS</code> 标志。这允许两个相关的服务在同一个沙盒中运行，以便高效共享内存数据，但依然与主 App 隔离。</li></ul><h3 id=总结-5>总结<a hidden class=anchor aria-hidden=true href=#总结-5>#</a></h3><p><code>HotwordDetectionService</code> 之所以能运行在隔离进程，是因为：</p><ol><li><strong>App 声明</strong>：App 自愿放弃了该 Service 的权限，声明其为 <code>isolatedProcess</code>。</li><li><strong>系统执行</strong>：System Server 使用 <code>bindIsolatedService</code> API 启动它，操作系统内核为其分配了受限的 UID，从而在此“牢笼”中执行 App 提供的代码。</li></ol></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ethen-cao.github.io/ethenslab/android-dev/display/hwc_setbrightness/><span class=title>« Prev</span><br><span></span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://ethen-cao.github.io/ethenslab/>Ethen 的实验室</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0,theme:"default"})</script><script src=https://cdn.jsdelivr.net/npm/plantuml-encoder@1.4.0/dist/plantuml-encoder.min.js></script><script>(function(){const e=document.querySelectorAll("pre > code.language-plantuml, pre > code.language-planuml");e.forEach(e=>{const s=e.innerText,o=plantumlEncoder.encode(s),i="https://www.plantuml.com/plantuml/svg/"+o,t=document.createElement("img");t.src=i,t.alt="PlantUML Diagram",t.style.maxWidth="100%";const n=e.parentNode;n.parentNode.replaceChild(t,n)})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>