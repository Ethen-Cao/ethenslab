<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ethen 的实验室</title><meta name=keywords content><meta name=description content="智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档
版本信息

  
      
          序号
          版本
          修订内容
          状态
          修订人
          日期
      
  
  
      
          1
          0.1
          First draft
          
          操权力
          2025/12/31
      
  

文档目的
本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：

管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。
跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。
工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。

背景与问题定义
背景
当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：

应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。
平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。
系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。

当前痛点

  
      
          痛点
          描述
          业务影响
      
  
  
      
          跨端故障排查成本较高
          当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。
          研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。
      
      
          性能量化数据覆盖不足
          现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。
          版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。
      
      
          偶发异常现场回溯困难
          对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。
          闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。
      
      
          资源效能优化缺乏支撑
          缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。
          成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。
      
  

目标与范围
项目目标
本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标："><meta name=author content><link rel=canonical href=https://ethen-cao.github.io/ethenslab/others/%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%E7%AB%AF%E4%BA%91%E4%B8%80%E4%BD%93%E6%80%A7%E8%83%BD%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/><link crossorigin=anonymous href=/ethenslab/assets/css/stylesheet.a1917769c3c78460b110da6d7905321bb53af4a56f22ba4cc0de824cf4d097ab.css integrity="sha256-oZF3acPHhGCxENpteQUyG7U69KVvIrpMwN6CTPTQl6s=" rel="preload stylesheet" as=style><link rel=icon href=https://ethen-cao.github.io/ethenslab/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethen-cao.github.io/ethenslab/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethen-cao.github.io/ethenslab/favicon-32x32.png><link rel=apple-touch-icon href=https://ethen-cao.github.io/ethenslab/apple-touch-icon.png><link rel=mask-icon href=https://ethen-cao.github.io/ethenslab/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethen-cao.github.io/ethenslab/others/%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%E7%AB%AF%E4%BA%91%E4%B8%80%E4%BD%93%E6%80%A7%E8%83%BD%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethen-cao.github.io/ethenslab/others/%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%E7%AB%AF%E4%BA%91%E4%B8%80%E4%BD%93%E6%80%A7%E8%83%BD%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/"><meta property="og:site_name" content="Ethen 的实验室"><meta property="og:title" content="Ethen 的实验室"><meta property="og:description" content="智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档 版本信息 序号 版本 修订内容 状态 修订人 日期 1 0.1 First draft 操权力 2025/12/31 文档目的 本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：
管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。 跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。 工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。 背景与问题定义 背景 当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：
应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。 平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。 系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。 当前痛点 痛点 描述 业务影响 跨端故障排查成本较高 当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。 研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。 性能量化数据覆盖不足 现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。 版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。 偶发异常现场回溯困难 对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。 闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。 资源效能优化缺乏支撑 缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。 成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。 目标与范围 项目目标 本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标："><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="others"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档
版本信息

  
      
          序号
          版本
          修订内容
          状态
          修订人
          日期
      
  
  
      
          1
          0.1
          First draft
          
          操权力
          2025/12/31
      
  

文档目的
本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：

管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。
跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。
工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。

背景与问题定义
背景
当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：

应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。
平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。
系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。

当前痛点

  
      
          痛点
          描述
          业务影响
      
  
  
      
          跨端故障排查成本较高
          当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。
          研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。
      
      
          性能量化数据覆盖不足
          现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。
          版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。
      
      
          偶发异常现场回溯困难
          对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。
          闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。
      
      
          资源效能优化缺乏支撑
          缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。
          成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。
      
  

目标与范围
项目目标
本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"杂记","item":"https://ethen-cao.github.io/ethenslab/others/"},{"@type":"ListItem","position":2,"name":"","item":"https://ethen-cao.github.io/ethenslab/others/%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%E7%AB%AF%E4%BA%91%E4%B8%80%E4%BD%93%E6%80%A7%E8%83%BD%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档 版本信息 序号 版本 修订内容 状态 修订人 日期 1 0.1 First draft 操权力 2025/12/31 文档目的 本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：\n管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。 跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。 工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。 背景与问题定义 背景 当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：\n应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。 平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。 系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。 当前痛点 痛点 描述 业务影响 跨端故障排查成本较高 当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。 研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。 性能量化数据覆盖不足 现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。 版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。 偶发异常现场回溯困难 对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。 闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。 资源效能优化缺乏支撑 缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。 成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。 目标与范围 项目目标 本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标：\n","keywords":[],"articleBody":"智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档 版本信息 序号 版本 修订内容 状态 修订人 日期 1 0.1 First draft 操权力 2025/12/31 文档目的 本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：\n管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。 跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。 工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。 背景与问题定义 背景 当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：\n应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。 平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。 系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。 当前痛点 痛点 描述 业务影响 跨端故障排查成本较高 当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。 研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。 性能量化数据覆盖不足 现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。 版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。 偶发异常现场回溯困难 对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。 闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。 资源效能优化缺乏支撑 缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。 成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。 目标与范围 项目目标 本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标：\n全链路可观测：打破 Android、Linux Host、MCU 的数据孤岛，建立统一的 全局事件标准 (Global Event ID)，将分散在不同系统的故障与状态数据聚合至同一平台，实现跨端事件的追踪，为后续的可视化链路分析奠定数据基础。 故障现场自动聚合与关联：突破现有“日志碎片化”及“事后拉取不全”的局限。建立 “事件驱动” 的现场快照机制，在异常发生瞬间，自动聚合与该事件强相关的全维度上下文信息（如 Trace、系统 Log、进程状态等）并生成完整的故障证据包。这不仅实现了 Event 与 Log 的精准索引，更确保了现场信息的完整性，能有效解决因关键日志缺失导致无法定位的难题。 数据驱动治理：建立系统级的性能与稳定性基线，通过量化数据驱动版本质量改善与硬件资源优化，将质量管理从“定性”转向“定量”。 核心 KPI 指标 维度 指标名称 目标值 (示例) 说明 稳定性 故障主动捕获率 \u003e 90% 指系统发生崩溃（如 SurfaceFlinger Crash）、重启后，平台能自动感知识别，无需依赖用户反馈。 诊断效率 故障取证自动化率 100% 针对严重异常事件，实现 Event 与 Log 的自动关联，确保研发直接获取现场数据。 性能体验 核心页面流畅度达标率 \u003e 95% 监测核心 App 在全量车辆中的掉帧情况，量化用户感知的卡顿程度。 资源能效 高功耗场景识别产出 TOP 5/季度 识别并输出高资源占用的异常场景，支撑性能治理。 项目范围 范围内 端侧全栈感知体系：\nAndroid 深度探针：构建系统级监控服务 PolarisAgentService，实现对应用生命周期、核心服务状态、底层资源（CPU/Memory/IO/Binder）的全维度深度监听。 Linux/MCU 异构覆盖：建设 Linux Host 侧的系统健康守护进程，负责关键服务（Service）存活检测与系统指标采集；适配 MCU 通信协议，实现异构芯片间的故障透传。 边缘智能处理：在端侧实现数据的预处理与清洗，包含事件聚合、流控防爆、日志现场的智能截取与压缩，减轻车云带宽压力。 标准化基础设施：建立《全局事件注册表》及自动化工具链，统一多端的数据定义与协议标准。 云端分析能力需求:\n元数据管理能力：要求云端支持同步《全局事件注册表》，实现对上报事件的自动化解析、分类与标签化管理。 自动化关联引擎：要求云端具备 “事件-日志” 自动匹配能力，将结构化的 Event 数据与非结构化的 Log 文件在存储层自动关联，形成完整的故障证据包。 趋势与模式识别：要求云端支持基于时间窗口的聚合计算，能够显示异常爆发趋势及性能指标（CPU/内存）的长期演进趋势。 可视化与运营平台\n数字化质量驾驶舱：建设多维度的质量仪表盘（Dashboard），支持按版本、车型、时间段分析千车故障率、性能基线达标率。 智能排查工作台：提供“一站式”问题分析界面，支持通过 EventID/TraceID 检索故障，直接浏览关联的日志及设备状态，支持远程诊断指令的下发与结果展示。 范围外 可视化的全链路拓扑分析：1.0 阶段聚焦于跨端链路数据的 标准化采集与逻辑串联，优先夯实数据底座能力；全链路图形化的调用链拓扑展示规划在后续版本迭代中实现。 业务代码修复：Polaris 平台负责精准“定位”并“指派”问题，不负责 具体业务 APP 内部的代码逻辑修复。 交互体验设计：本项目专注于性能数据的量化，不包含 HMI 界面（UI/UE）的主观交互设计与优化。 业务流程与核心场景 角色定义 角色 职责描述 关注点 研发工程师 接收告警，分析堆栈与日志，修复 Bug；针对疑难客诉问题，远程下发特定诊断指令 故障堆栈的完整性，日志关联的准确性，是否需要补充更多运行时信息。 质量工程师 配置告警阈值，监控线上大盘水位，识别版本质量风险 故障率趋势是否劣化，性能指标是否符合预期。 产品经理 查看应用活跃度与性能体验趋势 核心功能的响应速度趋势，用户使用过程中的卡顿频率。 核心作业流程图 故障主动发现闭环流程 描述从异常发生到研发接入的处理路径\n捕获 (Capture): Polaris Agent 监听到异常（如 ANR），记录运行时状态，抓取 Trace/Logcat，并生成唯一 EventID。 处理 (Process): 端侧进行流量控制检查，通过 logf 索引将 Event 与 Log 文件进行逻辑组合。 上报 (Report): Event 数据实时上报，大文件 Log 在网络空闲时段异步上传（支持云端按需拉取）。 通知 (Notify): 云端检测到异常数据，向 责任模块负责人 发送通知。 分析 (Analyze): 研发工程师查看通知，进入平台查看关联的上下文数据，确认问题根因并修复。 疑难问题排查流程 描述针对复杂客诉处理路径\n检索: 研发工程师在平台输入车辆 VIN 码或 EventID 检索相关记录。 查看: 系统展示该事件的发生时间、设备信息、以及已自动关联的 Log 文件下载链接。 诊断: 针对区域技术支持无法处理的复杂客诉，若现有日志不足以定位，研发工程师 通过控制台下发诊断指令，端侧执行后回传结果，以获取更深度的运行时信息。 典型用户故事 场景一：风险预警 背景: 某车型灰度推送 v1.5 OTA 版本。 事件: 上线 24 小时内，Polaris 平台监测到 GVM_SYS_STORAGE_LOW（磁盘空间不足）事件在特定批次车辆上的上报。 行动:\n平台自动触发 风险预警，即时通知研发负责人。 研发工程师通过平台获取存储分布数据，精准定位到某应用私有目录占用空间急剧膨胀。 分析: 结合自动关联采样的 Log，确认该应用在特定异常分支下陷入数据库高频重复写入死循环。 结果: 研发团队在磁盘被完全耗尽导致系统挂死（System Hang）前，紧急输出修复补丁并推送 OTA，成功拦截了批量重大事故。 场景二：稳定性治理 背景: 某应用发布 v2.0 灰度版本。 事件: 灰度发布期间，平台监测到应用出现偶发性 GVM_APP_ANR（无响应）告警，且线下测试难以复现。 行动:\n研发工程师点击告警详情，查看聚合后的故障样本。 系统已通过 logf 字段自动关联了故障时刻 perflog (性能日志)。 分析: 工程师通过日志文件发现应用主线程阻塞在 Binder IPC 调用中；进一步联合分析 perflog，定位到是对端 Service 在高并发场景下因锁竞争导致处理耗时过长，拖累了客户端。 结果: 确认根因为服务端卡顿。研发工程师针对服务端逻辑进行异步化优化，彻底解决了这一隐蔽的跨进程阻塞问题。 场景三：性能监控 背景: 某版本上线后，产品经理关注核心应用在复杂交互场景下的滑动流畅度。 事件: Polaris 仪表盘显示 GVM_APP_JANK (掉帧/卡顿) 指标在特定列表滑动场景下出现劣化趋势。 行动:\n查询掉帧期间主线程的MessageQueue事件。 发现: 在卡顿发生的时间段内，主线程 MessageQueue 待处理消息数量显著激增。 分析: 研发工程师通过分析采集到的 Looper 统计数据，发现是一次性加载过多列表项导致并在主线程频繁 Post UI 刷新消息，引发主线程消息队列积压，从而阻塞了渲染信号（Vsync）的处理。 结果: 研发工程师引入消息合并与节流机制，消除了主线程拥堵，恢复了滑动流畅性。 场景四：远程指令下发 背景: 用户反馈方控按键（下一曲）失效，或错误地控制了不显示在屏幕上的后台音乐应用，常规 Logcat 无法体现系统内部的分发逻辑。 行动:\n研发工程师怀疑是 MediaSession 焦点抢占或状态同步异常。 工程师通过 Polaris 控制台，向目标车辆下发 dumpsys media_session 指令。 分析: 回传的诊断结果显示，Media button session 仍被后台应用 com.reachauto.clouddesk 占用（尽管其状态为 active=false），导致按键事件未正确分发给前台亮屏的 com.tencent.wecarflow。 结果: 确认根因是后台应用未正确释放焦点，研发工程师将 Bug 准确指派给相关应用团队，无需现场抓包。 需求拆解 本章节将 Polaris 1.0 平台的核心需求拆解为四大关键能力域。这些能力定义了系统的边界与核心价值，是后续详细功能设计的基础。 本章节采用能力域拆解方法，以系统应具备的核心能力为中心，而非具体功能或实现方式。每一能力域仅定义目标、适用范围与责任边界，不涉及接口设计、数据结构或技术选型细节。具体功能点将在后续《功能性需求》中展开，质量与约束要求将在《非功能性需求》中统一定义。\n稳定性全栈感知能力 目标：构建覆盖 Android 应用层、系统框架层以及 Linux Host/MCU 异构计算单元的异常捕获体系，实现全栈、全维度的故障感知与现场数据留存。\n能力名称 能力描述与目标 适用范围 责任边界 应用层稳定性监控 描述：具备对 Android 应用层 (APK) 致命异常的实时监测能力。涵盖 Java Crash、App Native Crash (JNI)、ANR 及 App OOM；在异常触发时同步执行现场冻结与堆栈抓取。\n目标：确保应用级崩溃能被捕捉和上报。 Android Framework\nThird-party Apps\nSystem Apps (Launcher等) 负责：捕获应用堆栈、页面栈及进程状态；\n不负责：分析应用内部具体的业务逻辑错误。 系统框架稳定性监控 描述：具备对 Android 核心服务及关键守护进程的存活状态监测能力；识别系统级资源耗尽风险（如 Binder 耗尽、句柄泄漏、LMK）。\n目标：准确识别 SystemServer 死锁 (Watchdog)、核心服务崩溃、系统异常重启及严重资源拥堵事件。 SystemServer\nBinder Driver\nNative Daemons (SurfaceFlinger等) 负责：识别导致系统不稳定的服务异常和资源瓶颈；\n不负责：介入 Linux Kernel 内部调度机制的调试。 异构运行环境监控\n(Heterogeneous Env Monitoring) 描述：具备对 Linux Host (PVM) 及 MCU 运行状态的独立监测能力。通过 Native Daemon 标准化采集 Linux 侧服务状态、系统重启事件以及 MCU 侧的心跳与硬件故障码。\n目标：实现对底层虚拟化环境与硬件外设健康状况的统一视图监控。 Linux Host (PVM)\nMCU\nHypervisor 负责：异构数据的标准化接入、协议对齐及状态监测；\n不负责：异构系统内部具体业务逻辑的监控实现。 性能与资源度量能力 目标：建立可量化的性能基线，从“主观体验”转向“客观数据”，实现对计算资源（CPU/Mem/IO）及系统启动效率的精细化审计。\n能力名称 能力描述与目标 适用范围 责任边界 交互体验量化 描述：具备对用户核心交互体验的监测能力。重点涵盖应用启动耗时、主线程健康度 (MessageQueue) 以及界面流畅度 (FPS/Jank)。\n目标：量化 App 启动速度，识别主线程阻塞与掉帧现象。 Top 核心应用\nLauncher\nSystemUI 负责：采集启动耗时、MessageQueue 调度延迟及绘制帧率；\n不负责：应用页面内部的数据加载逻辑。 资源水位画像 描述：具备对进程级资源消耗（CPU使用率、内存占用、IO吞吐量、句柄数）的周期性采样与 TOP 排行识别能力。\n目标：识别高资源消耗进程与内存/句柄泄漏，绘制 24h 资源趋势图。 Android GVM 进程\nNative 守护进程 负责：资源数据的统计、归因与异常阈值判定；\n不负责：操作系统的资源调度策略 (Scheduler)。 系统级性能监控 描述：具备对操作系统级关键性能指标的监测能力，重点涵盖系统启动耗时及磁盘整体负载。\n目标：确保座舱冷启动（Cold Boot）时间达标，监控存储设备寿命与性能衰减。 Android Boot Process\nStorage Device (UFS/eMMC) 负责：各启动阶段（BootLoader/Kernel/UserSpace）的耗时分解；\n不负责：缩短硬件初始化时间。 现场还原与协同能力 目标：解决“有报警无日志”的痛点，构建端云协同的自动化取证与远程诊断通道。\n能力名称 能力描述与目标 适用范围 责任边界 标准化事件协议体系 描述：基于《全局事件注册表》构建统一的事件定义、序列化与解析能力。\n目标：确保端侧上报数据与云端解析引擎的语义一致性，支持协议动态扩展。 端侧 Agent, 车云 SDK, 云端解析服务 负责协议的定义与维护工具链；不限制业务 Payload 的具体内容。 智能现场快照 描述：具备“事件驱动”的自动化日志聚合能力，在异常发生瞬间关联并打包 Trace、Logcat 及系统状态信息。\n目标：实现 Event 与 Log 文件的 1:1 精准索引。 本地日志系统, 文件系统 负责日志的定位、截取与压缩；不负责日志内容的语义分析。 远程诊断执行 描述：具备安全可控的云端指令接收与本地执行能力，支持下发 Shell 脚本或调试命令。\n目标：在不打扰用户的前提下获取更深度的运行时信息。 Shell 环境, Debug 接口 负责指令通道的建立与执行结果回传；严禁执行未授权的高危写操作，不支持批量执行、默认灰度单车、需显式授权、强审计。 数据智能与运营能力 目标：将海量原始数据转化为可行动的洞察（Actionable Insights），支撑研发与质量团队的决策。\n能力名称 能力描述与目标 适用范围 责任边界 端云数据自动关联 描述：具备在海量存储中，根据索引自动将结构化事件与非结构化日志文件进行绑定的能力。\n目标：消除人工查找日志的成本。 云端存储层 负责数据的逻辑关联与存储生命周期管理。 实时风险预警 描述：具备基于时间窗口的流式计算能力，识别线上故障的爆发趋势并触发告警。\n目标：实现故障感知。 计算引擎 负责告警策略的计算与推送；不负责告警后的工单流转。 多维质量可视化 描述：具备多维度（版本/车型/时间/地区）的数据聚合与图表展示能力。\n目标：提供从“宏观大盘”到“微观个案”的钻取分析视图。 数据仓库, 可视化前端 负责数据的可视化呈现。 系统总体方案 总体设计概述 Polaris 1.0 基于 Hypervisor 虚拟化架构 设计，旨在构建跨越 GVM (Guest VM - Android)、PVM (Primary VM - Linux) 及 MCU 的端云一体化全栈监控系统。 系统采用 分层架构 与 模块化服务设计。在控制面上，通过 注册表驱动（Registry-Driven） 机制实现业务埋点定义与底层采集逻辑的解耦；在数据面上，通过 双守护进程（Dual Daemon） 机制打通异构芯片与系统的通信壁垒。系统将计算能力前置至端侧，通过 PolarisAgentService 实现数据的实时清洗、流控与现场关联，仅将高价值的结构化数据与诊断日志同步至云端。\n系统总体架构图 架构分层详解 业务应用与接口层 本层负责定义数据采集的标准接口，通过自动代码生成技术屏蔽底层通信细节：\nPolaris SDK: 面向上层业务应用（如 Launcher, Maps）。该组件由《全局事件注册表》编译生成，提供强类型的事件对象封装与校验逻辑，负责将业务数据序列化并传递给 Framework 层。 System Internal SDK: 面向 SystemServer 内部服务（如 AMS, WMS）。与 Polaris SDK 同源生成，专门用于系统关键服务内部的插桩（Instrumentation），以捕获服务级异常与状态变更。 Host SDK: 面向 PVM 侧的 Linux 应用程序（如 Cluster HMI）、系统核心服务，提供 C++ 标准上报接口，负责将 PVM 侧业务数据发送至 Host Daemon。 框架传输与核心服务层 本层位于 Android GVM，是数据汇聚、策略执行与处理的核心区域：\nPolaris SDK (Framework API): 部署于 AppFramework API 层。作为系统级的传输接口实现，它承接来自上层业务的调用请求，并维护与*PolarisAgentService的 IPC 通信链路，确保数据的可靠投递。\nPolarisAgentService: 常驻系统服务，内部包含五个核心功能模块：\nEventCollector: 统一接入模块。作为 Binder 服务端接收 Polaris SDK 请求；同时作为 LocalSocket 客户端，在服务启动时主动连接 Native Daemon 建立长连接通道，并通过后台线程实时拉取 Native 侧上报的事件流。 ConfigManager: 配置管理模块。负责加载本地注册表文件的配置，解析采样率、阈值及采集开关策略。 FlowController: 流量控制模块。对输入事件进行频率限制，防止异常爆发导致系统资源耗尽。 ContextEngine: 现场聚合模块。在事件通过流控后，负责生成唯一 EventID，挂载系统时间戳，并根据事件类型关联 Logcat、Trace 文件及进程快照，生成索引信息。 DiagnoseHandler: 诊断执行模块。负责校验并执行来自云端的诊断指令（Shell Command），并管理执行结果的回传。 原生与异构跨域层 本层负责 Android Runtime 之外的底层环境监控及跨虚拟机通信：\nPolaris Native Daemon (GVM):\n本地采集: 负责监控 Native 进程崩溃（Tombstone）、系统资源、及 HAL 层状态。\n跨域网关: 作为 GVM 侧的通信端点，维护与 PVM/MCU 的连接，接收跨域透传的数据。\nPolaris Host Daemon (PVM):\n宿主监控: 负责监控 PVM 侧的 systemd 服务状态、关键驱动状态及虚拟机管理服务（qcrosvm/VMM）。\n传输通道与云平台 VlmAgent: 统一传输网关。作为端侧唯一的数据出口，负责接收来自 PolarisAgentService 的结构化事件,以及日志文件（按需拉取），执行断点续传、数据压缩与网络流量调度。 Cloud Platform: 负责数据的计算、存储与可视化。 核心设计原则 进程级隔离与服务化：PolarisAgentService 设计为独立系统进程，而非 SystemServer 的内部线程。这种设计带来了两大优势： 稳定性：监控服务的异常（如 OOM）不会导致系统核心服务（SystemServer）崩溃，反之亦然。 高性能：独立的进程空间避免了与 AMS/WMS 争抢主线程资源，确保了监控逻辑的独立调度。 系统核心即客户端：确立 SystemServer 在监控体系中的 Client 身份。AMS、WMS 等核心服务通过 System Internal SDK，以跨进程调用（IPC）的方式向 Polaris 上报数据。这种“旁路监控”模式确保了对系统原有逻辑的最小侵入。 异构接入抽象化：针对 MCU 等异构单元，系统采用 “HAL 驱动适配 + Daemon 统一采集” 的接入原则。不强依赖特定的物理连接方式（如直连或透传），而是通过 Native 层的适配层（Adapter/HAL）屏蔽硬件连接差异，确保架构在不同车型硬件拓扑下的通用性与兼容性。 功能性需求 稳定性全栈感知能力 应用层稳定性监控 FR-STAB-001 应用 Java 崩溃 (Java Crash) 捕获 属性 内容 优先级 P0 前置条件 1.系统层已部署全局监控探针。\n2. 监控功能的配置开关处于开启状态。 输入 触发源：\n应用运行环境（Android Runtime）抛出的未捕获异常信号（Uncaught Exception）。\n数据：\n1. 异常堆栈信息（StackTrace）。\n2. 异常类型与描述消息（Exception Message）。 处理逻辑 1. 异常拦截：\n在应用进程因异常即将终止前，拦截异常信号，挂起当前线程以确保有足够的 CPU 时间片执行数据采集。\n2. 上下文捕获：\n提取崩溃时刻的运行时环境信息，包括但不限于：\n- 进程名称、线程名称及 ID。\n- 应用的前后台状态。\n- 当前 Activity 页面栈信息（用于还原用户路径）。\n3. 流控策略：\n执行本地频次控制策略。检查该进程在设定时间窗口（如 10 分钟）内的崩溃次数，若超限则降级处理（仅记录统计计数，不抓取详细堆栈），防止日志写入引发 IO 阻塞。\n4. 透传退出：\n数据采集完成后，必须将异常信号交还给系统默认处理程序，确保应用能够按照 Android 系统规范正常退出，防止应用界面假死或进程僵滞。 输出 1. 结构化事件：生成包含完整上下文信息的 GVM_APP_CRASH 事件对象。2. 本地日志：在本地持久化存储区保留一份异常日志备份（作为兜底）。 3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-002 应用无响应 (ANR) 捕获 属性 内容 优先级 P0 前置条件 1. 系统层已部署全局监控探针。\n2. 监控功能的配置开关处于开启状态。 输入 触发源：系统框架层（Framework）识别到的应用无响应信号（AppNotResponding）。\n数据：\n1. 目标应用进程标识（PID/ProcessName）。\n2. 系统生成的堆栈跟踪文件（Trace File，通常位于 /data/anr/ 目录）。 处理逻辑 1. 信号识别：\n实时接收系统 ActivityManagerService 发出的 ANR 通知。\n2. 目标过滤：\n根据配置白名单判断是否采集该进程，过滤非关注应用的 ANR 事件。3. 堆栈截取：\n读取系统生成的 Trace 文件，根据目标 PID 精准截取该进程及其子线程的堆栈片段（需剔除文件中的其他无关进程数据，以减少数据体积）。\n4. 快照关联：\n获取 ANR 发生时刻的系统负载信息（Load Avg / CPU Usage / IO Wait）并与堆栈信息打包。\n5. 流控策略：\n执行本地频次控制策略。检查该进程在设定时间窗口（如 10 分钟）内的 ANR 次数，若超限则仅记录计数，不再执行堆栈截取操作。 输出 1. 结构化事件：生成包含 Trace 附件索引（Reference）的 GVM_APP_ANR 事件对象。\n2. 本地日志：在本地持久化存储区生成关联的证据包（包含截取的 Trace 片段与系统负载快照）。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-003 应用 Native 库崩溃 (App JNI Crash) 捕获 属性 内容 优先级 P0 前置条件 1. 系统层已部署全局监控探针（Native Daemon）。\n2. 监控功能的配置开关处于开启状态。 输入 触发源：\n应用进程（APP）加载的 JNI 动态库触发致命信号（SIGSEGV/SIGABRT）。\n数据：\n1. 系统生成的 Tombstone 崩溃文件（通常位于 /data/tombstones/）。\n2. 进程退出信号（Signal Code）。 处理逻辑 1. 监听与解析：\n实时监听系统 Tombstone 文件的生成事件，读取文件头部信息。\n2. 身份识别：\n检查崩溃进程的 UID 或进程名称。若属于非系统核心进程（即普通 App），则执行应用级采集逻辑；若为系统服务则忽略（交由系统框架监控处理）。\n3. 指纹去重：\n基于“应用名称 + 崩溃堆栈关键帧”生成唯一指纹，在端侧聚合重复的崩溃事件，防止日志风暴。\n4. 事件生成：\n将非结构化的 Tombstone 数据转换为标准化的事件对象。 输出 1. 结构化事件：生成 GVM_APP_NATIVE_CRASH 事件对象。\n2. 本地日志：建立事件 ID 与原始 Tombstone 文件的索引关联。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-004 应用 OOM (App OOM) 事件监控 属性 内容 优先级 P0 前置条件 1. 系统层已部署全局监控探针。\n2. 监控功能的配置开关处于开启状态。\n3. 具备获取应用进程退出详细原因的能力（如 ApplicationExitInfo 或类似机制）。 输入 触发源：\n应用进程意外终止信号。\n数据：\n1. 进程退出原因描述（Exit Reason，需区分系统回收/异常崩溃）。\n2. 进程终止前的内存使用统计数据（如 PSS/RSS/VSS）。 处理逻辑 1. 原因甄别：\n在进程退出后，识别退出原因。准确区分是系统低内存查杀 (LMK)（通常表现为 REASON_LOW_MEMORY）还是Java 堆内存耗尽（通常表现为 OutOfMemoryError 导致的 Crash）引发的异常。\n2. 内存快照回溯：\n尝试关联该进程在终止前最近一次采集的内存统计数据（如 PSS/RSS），以辅助判断是否存在内存泄漏。\n3. 风暴抑制：\n针对前台应用因 OOM 导致的反复重启进行检测。若同一应用在短时间内（如 5 分钟）连续触发 OOM，则实施指数退避策略，减少上报频次。\n4. 事件生成：\n组装 OOM 事件负载，标记明确的 OOM 类型（System LMK / Java OOM）。 输出 1. 结构化事件：生成包含内存快照信息的 GVM_APP_OOM 事件对象。\n2. 本地日志：记录关联的系统内存水位信息（MemInfo）。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 系统框架稳定性监控 FR-STAB-005 SystemServer Watchdog (死锁) 监控 属性 内容 优先级 P0 前置条件 1. 监控探针已植入系统看门狗（Watchdog）模块或具备监听能力。\n2. 监控功能的配置开关处于开启状态。 输入 触发源：\n系统关键锁或核心线程（如 UI Thread, IoThread）等待超时信号（通常阈值为 60秒）。\n数据：\n1. 阻塞线程的完整堆栈信息（Stack Traces）。\n2. 持锁状态与锁竞争信息（Lock Contention）。 处理逻辑 1. 重启前拦截：\n在系统触发看门狗复位（Soft Reboot / Restart）流程前，优先执行监控逻辑，确保有短暂的时间窗口进行数据转存。\n2. 现场固化：\n立即将当前的系统全量线程堆栈（Traces.txt）复制或转存至持久化存储区，防止系统重启过程清理现场文件，导致关键证据丢失。\n3. 异常标记：\n在磁盘特定位置写入“非正常重启”标志位（Flag），以便系统下次启动时进行归因统计，区分正常关机与异常重启。\n4. 事件上报：\n尝试通过 Native 通道（因为 Java 层可能已挂死）发送死锁事件。 输出 1. 结构化事件：生成包含死锁堆栈索引的 GVM_SYS_WATCHDOG 事件对象。\n2. 本地日志：在持久化目录保留死锁现场的 Trace 文件。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-006 Android 系统(SystemServer)异常重启监控 属性 内容 优先级 P0 前置条件 1. 系统完成启动初始化流程（Boot Completed）。\n2. 具备读取系统启动属性（Boot Reason）及持久化存储的权限。 输入 触发源：\n系统启动完成广播 (Boot Completed) 或同等时机的初始化信号。\n数据：\n1. 系统启动原因属性（如 sys.boot.reason 或 ro.boot.bootreason）。\n2. 持久化存储中的历史状态标记（包含上一次启动时间戳、Watchdog/Crash 遗留的异常标志位）。 处理逻辑 1. 原因推断：\n对比本次启动原因与上一次运行状态进行逻辑仲裁：\n- 已知异常：若存在 Watchdog 或 Core Crash 遗留的标记，判定为对应的系统级故障重启。\n- 内核恐慌：若启动属性标识为 Kernel Panic 或 WDT（硬件看门狗），判定为内核级重启。\n- 正常重启：若标识为用户主动关机、OTA 升级或常规电源管理操作，判定为正常重启。\n- 掉电/未知：若无任何异常标记且非正常重启，判定为异常掉电或未知原因重启。\n2. 时长计算：\n基于上一次记录的启动时间戳，计算上一次系统正常运行的时长（Uptime），用于评估系统平均无故障时间（MTBF）。\n3. 状态重置：\n分析完成后，清除历史异常标记，更新本次启动时间戳，为下一次监控周期做准备。 输出 1. 结构化事件：生成包含重启原因分类（Category）及运行时长（Duration）的 GVM_SYS_RESTART 事件对象。\n2. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-007 全路径系统重启归因监控 属性 内容 优先级 P0 目标 建立跨越 Host、GVM、Kernel、Framework 四层架构的重启感知能力，提供统一的重启归因数据源。 前置条件 1. 监控服务具备获取 Hypervisor 状态及 SOC 硬件复位标志的权限。\n2. 已集成 FR-STAB-006 的 Android 内部重启判定结果。 处理逻辑 1. 全路径层次判定：\n系统启动后，根据硬件寄存器、Bootloader 传参及 Android 属性进行综合仲裁，确定重启发生的最高级别：\n- Linux Host层：检测到 SOC 硬件复位或宿主机 Kernel Panic，判定为 Host 重启。\n- GVM层：Host 未重启，但虚拟机管理器（Hypervisor）强制拉起 Android 域，判定为 GVM 重启。\n- Android Kernel层：基于 FR-STAB-006 判定，若为内核崩溃且 GVM 未重启，归类于此层。\n- Android Framework层：仅 SystemServer 进程重启，底层 Linux 内核连续运行，归类于此层。\n2. 异常分类同步：\n- 读取 sys.boot.reason.last 等关键属性，识别是 [异常:死锁/崩溃/掉电] 还是 [正常:用户重启/OTA]。 输出 1. 结构化事件：GVM_SYSTEM_REBOOT_EVENT - boot_reason: 原始启动原因字符串\n- is_unexpected: 布尔值，用于云端统计严重故障率。\n2. 关联日志路径及 ID 注册参考《Polaris 1.0 全局事件ID与注册表规范》。 FR-STAB-008 核心服务崩溃监控 属性 内容 优先级 P0 前置条件 1. 监控进程具备监听系统服务管理器（ServiceManager）或 init 进程状态的能力。\n2. 核心进程白名单配置已加载。 输入 触发源：\n1. Native 守护进程崩溃产生的 Tombstone 文件。\n2. ServiceManager 发出的 DeathRecipient 通知。\n3. init 进程发出的 SIGCHLD 信号。\n数据：\n1. 崩溃进程名称（Process Name）及 PID。\n2. 进程退出状态码或终止信号。 处理逻辑 1. 核心识别：\n匹配崩溃进程名称是否在核心白名单中（如 surfaceflinger, audioserver, netd, lmkd）。若不在白名单，则视为普通 Native Crash 处理（参考 FR-STAB-003）。\n2. 多源仲裁：\n优先使用 Tombstone 信息（包含详细堆栈），若未生成 Tombstone（如被系统强杀或 Watchdog 处决），则使用 ServiceManager 通知作为补充来源。\n3. 等级判定：\n根据服务重要性标记故障等级（例如 SurfaceFlinger 崩溃标记为“致命”，会导致屏幕黑屏或系统软重启）。\n4. 事件生成：\n组装核心服务崩溃事件，记录服务名称、崩溃时间及退出原因。 输出 1. 结构化事件：生成包含服务名及影响等级的 GVM_CORE_CRASH 事件对象。\n2. 本地日志：关联该时间点附近的系统日志（Logcat）与崩溃堆栈。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-009 系统低内存 (LMK) 事件监控 属性 内容 优先级 P0 前置条件 1. 系统启用 Low Memory Killer 机制（如 Userspace LMKD）。\n2. 监控组件具备接收系统内存管理模块通知的权限。 输入 触发源：\n系统内存管理守护进程（lmkd）执行的进程查杀动作。\n数据：\n1. 目标进程信息（PID、UID、Process Name）。\n2. 查杀时的决策依据（OOM Score Adj）。\n3. 触发查杀时的系统内存压力状态（Memory Pressure State / PSI）。 处理逻辑 1. 动作捕获：\n实时感知 LMK 的查杀行为。推荐方案：采用源码插桩（Instrumentation）方式，在 lmkd 执行 kill 操作的原子逻辑处植入通知钩子，以获取零延迟、高精度的上下文信息；（备选方案：监听 EventLog 中的 lmk_kill 标签）。\n2. 水位快照：\n同步记录系统当前的内存水位详情（MemTotal, MemFree, SwapUsed, Cached），用于后续分析是物理内存耗尽还是虚拟内存（Swap）耗尽。\n3. 聚合去噪：\n执行时间窗口聚合策略。由于内存压力常导致短时间内连续查杀多个进程，需将同一压力波峰内（如 1 秒）的一组查杀事件聚合，避免产生告警风暴。\n4. 严重性判定：\n识别被杀进程的类型。若被杀进程为前台可见应用或关键服务，标记为“高影响”事件。 输出 1. 结构化事件：生成包含被杀进程列表及内存水位的 GVM_SYS_LMK 事件对象。\n2. 本地日志：保留查杀时刻的 meminfo 快照。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-010 Binder 通信异常监控 属性 内容 优先级 P1 前置条件 1. 监控探针具备访问内核 Binder 驱动节点或 Hook libbinder 的能力。\n2. 监控配置已定义 Binder 线程池水位的告警阈值。 输入 触发源：\n1. Binder 驱动层的事务失败信号（如 BR_FAILED_REPLY, BR_DEAD_REPLY）。\n2. 进程 Binder 线程池的资源耗尽状态。\n数据：\n1. 通信双方身份（Caller PID/UID, Callee PID/UID）。\n2. 接口描述符（Interface Descriptor）或事务代码（Transaction Code）。\n3. 失败原因（如 TransactionTooLarge, DeadObject, Timeout）。 处理逻辑 1. 异常捕获：\n监测 IPC 通信链路健康度。推荐方案：在 Native libbinder 层进行插桩，拦截 IPCThreadState 中的错误返回码，从而在第一现场捕获异常。\n2. 资源枯竭识别 (Starvation)：\n周期性或事件驱动地检查关键进程的 Binder 线程池状态。若活跃线程数达到上限（默认 16）且仍有请求积压，判定为 Binder 线程耗尽。\n3. 大负载识别：\n识别 TransactionTooLargeException，记录传输过大数据的接口名称，辅助排查跨进程传输大图或大列表导致的性能问题。\n4. 链路还原：\n在异常发生时，自动解析并记录“谁调用谁”（Client -\u003e Server），明确责任方。 输出 1. 结构化事件：生成包含通信双方及错误类型的 GVM_BINDER_ERROR 事件对象。\n2. 本地日志：记录 /sys/kernel/debug/binder/transaction_log (若可用) 或相关 Logcat 片段。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-011 文件句柄 (FD) 泄漏监控 属性 内容 优先级 P1 前置条件 1. 监控探针具备读取 /proc/[pid]/fd 目录或执行 lsof 类指令的权限。\n2. 针对不同类型的进程（System/App）配置了相应的 FD 数量告警阈值。 输入 触发源：\n1. 周期性采样：定时检查系统内进程的资源占用情况。\n2. 被动触发：捕获到系统日志中抛出的 EMFILE (“Too many open files”) 错误信号。\nData：\n1. 目标进程当前打开的文件句柄总数。\n2. 具体的句柄指向路径（Symlinks in /proc/pid/fd/）。 处理逻辑 1. 水位监测：\n对关键进程进行周期性（如每 5 分钟）的 FD 数量扫描。对比系统设定的软限制（Soft Limit）与硬限制（Hard Limit）。\n2. 超限识别：\n当某进程 FD 数量超过预警阈值（例如 \u003e 800 或占比 \u003e 80%）时，判定为存在泄漏风险。\n3. 现场快照：\n在检测到超限瞬间，遍历该进程的 /proc/[pid]/fd/ 目录，生成句柄分布快照。智能分类：统计不同类型句柄的占比（如 Socket, Anon_inode, Regular File），快速定位是网络连接泄漏还是文件未关闭。\n4. 趋势分析：\n结合历史数据，识别 FD 数量是否呈“持续上升且不回落”的阶梯状趋势，以排除正常的业务并发高峰。\n输出 1. 结构化事件：生成包含 FD 总数及分类统计的 GVM_FD_LEAK 事件对象。\n2. 本地日志：保留 top N 的句柄路径列表（Evidence List）。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 异构运行环境监控 FR-STAB-012 Linux Host (PVM) 重启与状态监控 属性 内容 优先级 P0 前置条件 1. PVM (Linux Host) 侧已部署 Host Daemon 并具备读取系统启动日志（如 /sys/fs/pstore 或 systemd journal）的权限。\n2. PVM 与 GVM 之间的跨域通信通道在启动后能够建立连接。 输入 触发源：\n1. PVM 启动完成：Host Daemon 随系统启动初始化。\n2. 连接建立：PVM 与 GVM 建立首次握手成功。\n数据：\n1. PVM 本次启动原因（Boot Reason）。\n2. 历史持久化日志（上一周期的 Kernel Panic Log 或 Watchdog 记录）。\n3. 实时心跳报文。 处理逻辑 1. 启动回溯（Post-Boot Analysis）：\nHost Daemon 在 PVM 启动初期，检查持久化存储中的上一次关机状态。识别是正常关机、掉电还是异常重启（如 Kernel Panic 导致的 WDT Reset）。\n2. 事件缓存：\n若判定为异常重启，Host Daemon 生成重启事件对象并暂存于本地内存或磁盘队列中，等待跨域通道就绪。\n3. 延迟同步（Delayed Sync）：\n当监测到 GVM (Android) 侧的 PolarisNativeDaemon 上线并建立连接后，立即将缓存的“上一次重启事件”发送给 GVM。\n4. 运行时状态监测：\n在连接建立后的运行期间，Host Daemon 周期性向 GVM 发送心跳包与健康度状态（如 Systemd Failed Units），供 GVM 侧记录实时趋势。 输出 1. 结构化事件：生成 PVM_SYS_RESTART（携带重启原因的历史事件）或 PVM_STATUS_REPORT（运行时状态）。\n2. 本地日志：在 PVM 侧保留原始的重启原因分析日志。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-013 MCU 故障码与心跳监控 属性 内容 优先级 P1 前置条件 1. 硬件抽象层（HAL）或驱动层已完成 MCU 通信协议适配。\n2. 监控守护进程（Native Daemon）具备读取 MCU 状态接口的权限。 输入 触发源：\n1. MCU 周期性上报的状态报文（Status Message）。\n2. 硬件中断或底层驱动回调。\n数据：\n1. 存活心跳计数器（Rolling Counter）。\n2. 硬件诊断故障码（DTC - Diagnostic Trouble Code）。\n3. 外设关键状态字（如电源模式、复位原因）。 处理逻辑 1. 存活判定：\n通过监测心跳计数器的连续性和变化率来判断 MCU 状态。若计数器在设定时间窗口（TBD）内停止跳变或非法跳变，判定为 MCU 挂死或通信中断。\n2. 协议映射：\n建立 MCU 原始故障码与平台统一错误定义的映射表。将厂商特定的十六进制 DTC（如 0x1234）转换为可读的平台标准错误码（如 ERR_MCU_PMIC_FAIL）。\n3. 信号去抖：\n对偶发的故障信号进行软件滤波（De-bounce）。只有在连续 N 帧报文中确认同一故障码，或故障持续时长超过阈值时，才确认为有效故障，防止因总线干扰导致的误报。\n4. 复位检测：\n监测 MCU 的复位原因寄存器。若发现异常复位标识（如 WDT Reset），记录异常复位事件。 输出 1. 结构化事件：生成 MCU_HEARTBEAT_LOST（失联）或 MCU_HARDWARE_FAULT（硬件故障）事件对象。\n2. 本地日志：记录原始报文数据（Raw Data）以便后续校验。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-014 异构关键进程 (PVM Critical Process) 稳定性监控 属性 内容 优先级 P0 前置条件 1. 目标关键进程（如 Audio Server, Display Composer, GSL HAL 等）已在 Host 侧启动。\n2. Polaris Host Daemon 具备对目标进程状态的查询或监听权限。 输入 触发源：\n1. 操作系统（Linux Host）发出的进程终止信号（如 SIGCHLD）。\n2. 服务管理框架（如 Systemd）抛出的服务状态变更通知（Service Unit Status Change）。\n3. 目标进程输出到标准错误流（Stderr）的致命错误日志。\n数据：\n1. 进程标识（PID, Process Name, Unit Name）。\n2. 退出状态码（Exit Code）或终止信号（Signal）。 处理逻辑 1. 通用监听：\n采用非侵入式手段实时感知关键进程的生命周期。针对受 Systemd 托管的服务，订阅其 D-Bus 状态变更信号；针对独立进程，采用 PID 存活轮询或父进程信号监听机制。\n2. 状态判定：\n- 异常退出：识别进程非预期的终止（Exit Code != 0）。\n- 僵死/挂起：若具备条件，监测进程是否长时间处于 D 状态（Uninterruptible Sleep）或对心跳接口无响应。\n3. 抖动抑制 (Flapping Detection)：\n针对具有“自动重启”机制的关键服务，在设定时间窗口内（如 10秒）若检测到多次反复重启，应将其聚合为单次“服务抖动”事件上报，防止告警风暴。\n4. 现场记录：\n在进程崩溃瞬间，尝试捕获其最后输出的标准错误日志（Stderr）或 Journalctl 片段，作为归因线索。 输出 1. 结构化事件：生成包含进程名、退出码及故障频次的 PVM_PROCESS_CRASH 事件对象。\n2. 本地日志：Host 侧保留相关的系统日志片段。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-STAB-015 温度监控 属性 内容 优先级 P1 前置条件 1. 系统底层具备热管理子系统（Thermal HAL / Thermal Daemon）。\n2. 监控探针具备读取 /sys/class/thermal 节点或订阅 IThermalService 回调的权限。 输入 触发源：\n1. 被动接收：Thermal HAL 上报的热状态变更回调（如 onStatusChanged）。\n2. 主动采样：周期性读取关键热区（Thermal Zone）的温度传感器数值。\n数据：\n1. 热区名称（Zone Name, 如 cpu, gpu, battery, soc）。\n2. 当前温度值（Temperature in m°C）。\n3. 热状态等级（Thermal Status: NONE, LIGHT, MODERATE, SEVERE, CRITICAL, SHUTDOWN）。 处理逻辑 1. 状态监听：\n实时订阅系统热管理服务的状态变更通知。一旦热状态跨越阈值（例如从 NONE 变为 SEVERE），立即触发记录逻辑。\n2. 降频关联：\n当检测到温度过高触发温控策略（Throttling）时，尝试关联当前的 CPU/GPU 频率限制状态，以解释可能伴随发生的卡顿或掉帧现象（辅助性能分析）。\n3. 危急保护记录：\n当收到 SHUTDOWN 级别的热信号时，视为“过热关机前兆”，必须以最高优先级将当前温度快照写入持久化存储，作为系统异常重启（FR-STAB-006）的直接归因证据。\n4. 趋势采样：\n在非危急状态下，按低频（如每 5 分钟）采样 SoC 核心温度，用于绘制温度变化趋势图。 输出 1. 结构化事件：生成 GVM_SYS_THERMAL_EVENT（包含热区名称、温度值及温控等级）。\n2. 本地日志：保留过热时刻的 thermalservice 状态或相关节点快照。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 性能与资源度量能力 交互体验量化 FR-PERF-001 应用启动耗时 (App Launch Time) 监控 属性 内容 优先级 P1 前置条件 1. 监控探针已 Hook 或插桩至 AMS 启动流程及 Activity 生命周期关键回调。\n2. 目标应用白名单已配置。 输入 触发源：\n应用进程创建及 Activity 界面绘制完成信号。\n数据：\n1. 启动类型：冷启动 (Cold)、热启动 (Hot/Warm)。\n2. 关键时间戳（仅采集内存中的系统时钟 SystemClock.uptimeMillis()）：\n- T0: Intent 发送/用户点击图标时刻。\n- T1: 进程创建成功 (Process Forked)。\n- T2: 应用页面初始化 (Activity.onCreate/onStart)。\n- T3: 首帧绘制完成 (ReportFullyDrawn / Window Focus)。 处理逻辑 1. 零干扰采集：\n在启动的关键路径上，严禁执行任何文件读取、复杂的字符串处理或 IPC 调用。仅在内存中记录长整型时间戳，确保监控逻辑对应用启动速度的影响趋近于零。\n2. 端到端计算：\n启动结束后，异步计算总耗时及分段耗时：\n- 系统调度耗时 (T1 - T0)：反映系统资源紧张程度或 Zygote 响应速度。\n- 应用初始化耗时 (T2 - T1)：反映 Application 级初始化逻辑耗时。\n- 页面渲染耗时 (T3 - T2)：反映 Activity 布局加载与渲染耗时。\n3. 异常判定与分级上报：\n- 正常启动：总耗时未超过基线。上报事件，但留空诊断字段。\n- 慢启动：总耗时超过基线。标记 status=SLOW，并触发关联逻辑。\n4. 关联归因 (Correlation)：\n仅针对“慢启动”事件进行事后时间窗口匹配：\n- 主线程阻塞：检索启动期间是否触发了 FR-PERF-002 (主线程卡顿) 事件。\n- 资源竞争：检索启动期间系统整体 LoadAvg 或 IO 负载是否处于高位 (由 FR-PERF-004 采集)。 输出 1. 结构化事件：生成 GVM_APP_LAUNCH 事件。\n- 必选字段：应用名、启动类型、总耗时、分段耗时、状态(Normal/Slow)。\n- 可选字段（仅 Slow 状态填充）：关联的异常事件 ID (Ref_Event_ID)、Trace 索引。 FR-PERF-002 主线程响应与健康度监控 属性 内容 优先级 P1 前置条件 1. 监控探针已集成至目标应用主线程 Looper。\n2. 监控配置中定义了卡顿阈值（如 BlockThreshold = 200ms）。 输入 触发源：\n主线程 Looper 分发消息（Message Dispatch）的开始与结束时刻。\n数据：\n1. 消息执行耗时：墙钟耗时 (Wall Duration) 与 线程 CPU 耗时 (Thread CPU Duration)。\n2. 消息签名：目标 Handler 类名、Runnable 类名或 Message.what 标识。\n3. 队列状态：MessageQueue 当前积压的消息数量 (Pending Count)。 处理逻辑 1. 双重计时监测：\n在消息分发前后打点。关键逻辑：同时记录“墙钟时间”和“CPU 时间”。\n- 若墙钟时间长且 CPU 时间长 $\\rightarrow$ 计算密集型卡顿 (算法复杂、大循环)。\n- 若墙钟时间长但 CPU 时间短 $\\rightarrow$ IO/锁等待型卡顿 (主线程读写磁盘、Binder 阻塞、等锁)。\n2. 堆栈抓取 (Sample \u0026 Dump)：\n采用“看门狗”机制。当检测到当前消息执行已超过阈值（如 \u003e 200ms）但在结束前，主动抓取主线程瞬时堆栈。避免等消息执行完再抓取导致“现场已过”的问题。\n3. 拥堵识别 (Congestion)：\n即使单条消息未超时，若 MessageQueue 待处理消息数持续超过阈值（如 \u003e 50个），判定为调度拥堵。记录拥堵期间的“头部分发者” (Top Senders)。\n4. 聚合去噪：\n对短时间内连续发生的相同堆栈/相同签名的卡顿事件进行聚合，仅上报一次并附带发生次数 (Count)。 输出 1. 结构化事件：生成 GVM_APP_MAIN_THREAD_BLOCK 事件。\n- 字段：应用名、卡顿类型(CPU/Wait)、耗时、消息签名、堆栈摘要(StackHash)。\n2. 本地日志：记录完整的卡顿堆栈 (Full Stack Trace) 及当时的消息队列快照。 3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-PERF-003 界面流畅度 (UI Jank/FPS) 监控 属性 内容 优先级 P1 前置条件 1. 监控探针已通过 Window.addOnFrameMetricsAvailableListener 注册监听。\n2. 目标应用处于前台可见状态。 输入 触发源：\n系统 FrameMetrics 回调。\n数据：\n1. 核心耗时：TOTAL_DURATION, DEADLINE。\n2. 归因耗时：UI 相关 (LAYOUT, DRAW…), GPU 相关 (GPU_DURATION, SWAP…)。 处理逻辑 1. 生命周期聚合：\n以页面会话（onResume -\u003e onPause）为单位进行统计，不上报单帧数据。\n2. 轻量级计算：\n- 计数：统计总帧数 (total)、掉帧数 (jank, Total \u003e Deadline)、冻结帧数 (frozen, Total \u003e 700ms)。\n- 极值：记录会话期间的最大帧耗时。\n- 均值：累加 UI 耗时与 GPU 耗时，计算平均每帧的 CPU/GPU 开销分布。\n3. 异常判定：\n若掉帧率超过阈值（如 10%）或存在冻结帧，标记 is_laggy=true。 输出 1. 结构化事件：GVM_APP_JANK_STATS。\n- 字段：activity_name, total_frames, jank_count, frozen_count (冻结帧数), max_frame_ms, avg_ui_ms, avg_gpu_ms。\n- 作用：云端可直接计算出“某页面的平均掉帧率”以及“卡顿主要是因为 UI 逻辑还是 GPU 渲染”。 2. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 资源水位画像 FR-PERF-004 进程 CPU 负载监控 属性 内容 优先级 P1 前置条件 1. Android/Host 端监控进程具备 /proc 读取权限。\n2. 跨域通信通道正常。 输入 触发源：\n定时器触发（端侧采样周期：30秒）。\n数据源：\n解析 /proc/stat (System) 和 /proc/[pid]/stat (Process)。严禁使用 top 命令。 处理逻辑 1. 端侧高频采样：\n每 30秒 采集一次快照，计算瞬时系统总负载及各进程负载。不立即上报，暂存内存。\n2. 长周期聚合 (Long-Term Aggregation)：\n每 30分钟 为一个常规上报周期（包含 60 个采样点）。\n- 计算均值：计算该 30 分钟内的系统平均负载。\n- 锁定峰值：找出该周期内负载最高的那个时间点（Peak Snapshot），并提取该时刻的 Top 5 进程。\n3. 异常快速通道 (Immediate Alert)：\n在每次采样后进行实时判断。若检测到系统总负载 \u003e 80% 且持续 2 个采样周期（1分钟），则绕过 30分钟的聚合等待，立即触发一条异常告警事件。\n4. 跨域打包：\nLinux Host 端数据通过跨域通道透传至 Android 端，统一打标 domain (ANDROID/HOST) 后上传。 输出 1. 结构化事件：GVM_PROC_CPU_TOP。\n- 频率：30分钟/次 (常规) 或 立即 (异常)。\n- 字段：timestamp, sys_load_avg (30min均值), sys_load_peak (峰值), top_processes (峰值时刻的 Top 5 列表)。\n2. 流量预估：约 20KB/天/车 (极低)。 3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-PERF-005 进程内存消耗监控 属性 内容 优先级 P1 前置条件 1. Android/Host 端监控进程具备 /proc 读取权限。\n2. /log/perf/meminfo/ 目录已创建且具备写权限。 输入 触发源：\n定时器触发（端侧采样周期：30秒）。\n数据源：\n1. 全局水位：读取 /proc/meminfo (关注 MemAvailable)。\n2. 进程水位：读取 /proc/[pid]/statm (关注 RSS - Resident Set Size)。 处理逻辑 1. 端侧高频采样：\n每 30秒 读取一次全局内存及所有活跃进程的 RSS。\n2. 长周期聚合 (30min Window)：\n每 30分钟 为一个上报周期。\n- 系统趋势：记录周期内 MemAvailable 的最低点。\n- Top 进程：找出周期内 RSS 峰值最高的 Top 5 进程。\n- 异常膨胀识别：计算进程在周期首尾的差值 ($\\Delta RSS = RSS_{End} - RSS_{Start}$)。若净增量 \u003e 200MB (可配置)，标记为异常。\n3. 异常快速通道 (Immediate Alert)：\n- 系统持续高压：若 MemAvailable / MemTotal \u003c 20% 且持续时长 \u003e 3分钟，判定为内存紧张，立即上报。\n- 进程大内存：若某非白名单进程 RSS \u003e 1GB (或配置阈值) 且持续 \u003e 3分钟，判定为异常占用，立即上报。\n- 现场固化：触发上述任一条件时，立即执行 dumpsys meminfo [pid] 并写入日志文件。 输出 1. 本地日志 (Local Dump)：\n- 路径：/log/perf/meminfo/。\n- 文件名格式：{process_name}_{pid}_{timestamp}.txt (注：进程名中的 : 需替换为 _，如 com_android_systemui)。\n- 内容：完整的 dumpsys meminfo 文本输出 (包含 Java Heap, Native Heap, Graphics 等 PSS 详情)。\n2. 结构化事件：GVM_PROC_MEM_TOP。\n- 频率：30分钟/次 (常规) 或 立即 (异常)。\n- 字段：timestamp, sys_mem_available_min (MB), top_processes (List: name, pid, rss_peak_mb, rss_growth_mb)。\n- 关联字段：logf (值为上述生成的日志文件名。仅在触发快速通道时填充，常规上报为空)。 3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-PERF-006 进程磁盘 I/O 吞吐量与系统压力监控 属性 内容 优先级 P1 前置条件 1. Android/Host 端监控进程具备读取 /proc/[pid]/io 及 /proc/stat 的权限。\n2. 内核配置开启 TASK_IO_ACCOUNTING 选项。 输入 触发源：\n定时器触发（端侧采样周期：30秒）。\n数据源：\n1. 进程 I/O：读取 /proc/[pid]/io (read_bytes, write_bytes)。\n2. 系统 I/O 压力：读取 /proc/stat (用于计算全局 iowait)。 处理逻辑 1. 端侧高频采样：\n每 30秒 遍历活跃进程计算 I/O 增量，同时记录系统 iowait 快照。\n2. 长周期聚合 (30min Window)：\n每 30分钟 为一个上报周期。\n- 系统压力：利用周期初和周期末的 /proc/stat 快照，计算该 30分钟内的平均 iowait。\n- 累计总量：计算周期内所有进程的总写入量。\n- Top 进程：找出周期内 I/O 吞吐量 最高的 Top 5 进程。\n3. 异常快速通道 (Immediate Alert)：\n- 高负载写入：若某进程写速 \u003e 10MB/s (可配置) 且持续 \u003e 1分钟，判定为“异常写入 (Abnormal Write)”，立即上报。\n- 高负载读取：若某进程读速 \u003e 50MB/s (可配置) 且持续 \u003e 1分钟，判定为读取密集，可能导致界面卡顿 (ANR)，立即上报。\n- I/O 阻塞：若系统全局 iowait \u003e 40% (可配置) 且持续 \u003e 1分钟，判定为磁盘性能瓶颈，立即上报。\n4. 跨域打包：\nLinux Host 端数据透传至 Android 端合并上报。 输出 1. 结构化事件：GVM_PROC_IO_TOP。\n- 频率：30分钟/次 (常规) 或 立即 (异常)。\n- 字段：timestamp, sys_iow_avg (平均IO等待率 %), total_write_mb (周期总写量), top_processes (List: name, pid, read_rate_mbps, write_rate_mbps)。\n2. 本地日志：触发异常时，记录 /proc/[pid]/io 快照及 /proc/stat 详情。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 系统级性能监控 FR-PERF-007 系统冷启动耗时 (Cold Boot) 监控 属性 内容 优先级 P2 前置条件 1. Host 端: 集成 bootchart，并将解析出的内核/用户态启动耗时传递给 Android。\n2. Android 端: 能够接收 Host 数据，且能访问本地日志目录。 输入 触发源：\n接收到 BOOT_COMPLETED 广播。\n数据源：\n1. Linux: Host 侧 bootchart 统计时长。\n2. Android: SystemClock.uptimeMillis()。 处理逻辑 1. 计算总耗时：\n$Total = T_{Linux} + T_{Android}$。\n2. 本地落盘：\n无论是否超时，将本次启动的耗时数据记录在本地数据库/日志中，并保留 Host 传输过来的 bootchart.tgz 文件。\n3. 异常上报：\n若 $Total \u003e$ 30s，触发云端上报，并携带 bootchart 日志路径。 输出 1. 结构化事件：GVM_BOOT_COLD\n- 字段 (极简)：\n- dur (Int): 总耗时 (ms)\n- lin (Int): Linux Host 耗时 (ms)\n- logf (Str): 关联的日志文件名 (如 bootchart_20251020.tgz)\n2. 本地日志：保留 Linux 生成的 bootchart 压缩包。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-PERF-008 系统热唤醒耗时 (STR Resume) 监控 属性 内容 优先级 P2 前置条件 1. 监控服务有权限读取 Kernel 唤醒中断时间戳。\n2. 监控服务能捕获 Android 界面首帧绘制信号 (First Frame)。 输入 触发源：\nSTR 唤醒流程结束，屏幕点亮。\n数据源：\n1. 起点: Kernel 接收到唤醒中断 (IRQ) 的时间戳。\n2. 终点: Android 界面第一帧绘制完成 (SurfaceFlinger/WindowManager) 的时间戳。 处理逻辑 1. 计算耗时：\n$Duration = T_{FirstFrame} - T_{WakeupIRQ}$。\n2. 异常上报：\n若 $Duration \u003e$ 5s，判定为唤醒过慢，触发云端上报。\n3. 现场固化：\n仅在超时时，抓取当前的 Kernel Log (dmesg) 和 Android Log (logcat -b events) 片段写入文件，用于分析是驱动阻塞还是应用渲染慢。 输出 1. 结构化事件：GVM_BOOT_STR\n- 字段 (极简)：\n- dur (Int): 唤醒总耗时 (ms)\n- logf (Str): 关联的日志文件名 (如 str_trace_20251020.txt)\n2. 本地日志：包含唤醒时间段内的 dmesg 和关键系统日志。 3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 FR-PERF-009 全局存储健康度与多分区空间监控 属性 内容 优先级 P1 前置条件 1. 监控服务持有需监控的分区白名单配置（如 [\"/data\", \"/mnt/camera\", \"/log\"]）及其各自的报警阈值（如 Camera \u003e 95%, Data \u003e 90%）。\n2. 具备读取 UFS/eMMC 硬件寿命寄存器及执行 statfs 的权限。 输入 触发源：\n定时器触发（端侧采样周期：15分钟）。\n数据源：\n1. 硬件寿命：UFS/eMMC Life Time Estimation (Type A/B)。\n2. 分区空间：遍历白名单中的每个挂载点，使用 statfs (系统调用) 获取 Total/Used/Free 块数量。 处理逻辑 1. 空间写满判定：\n- 低频采样：每 15 分钟检查一次各分区使用率。\n- 防抖确认：若发现某分区使用率 \u003e 阈值，立即在 60秒后进行第二次采样。\n- 异常触发：若两次采样均超标，标记为“写满确诊”，立即上报异常事件。\n2. 硬件寿命监测：\n每天检查一次。若 Life A/B $\\ge$ 0x09 (90%)，标记为“寿命耗尽”，立即上报。\n3. 长周期聚合：\n每 30分钟 (即每 2 个采样周期) 上报一次常规状态。\n- 打包所有被监控分区的当前使用率快照。 输出 1. 结构化事件：GVM_DISK_STAT\n- Type字段：typ (0=Routine, 1=Alert_Full, 2=Alert_EOL)\n- 公共字段：la/lb (寿命等级)\n- 常规上报字段 (typ=0)：\nparts: [{\"n\":\"data\",\"u\":45}, {\"n\":\"camera\",\"u\":80}, {\"n\":\"log\",\"u\":10}] (n=name, u=usage%)\n- 异常上报字段 (typ=1)：\ntgt: \"camera\" (报警的具体分区名)\nval: 98 (当前使用率)\n2. 本地日志：仅在触发 Alert 时，执行 df -h 和 du -sh * 写入日志文件。\n3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 现场还原与协同能力 FR-DIAG-001 标准化事件协议体系 系统对于上报事件应该具有统一规范的编码，具体编码规则参考 《Polaris 1.0 全局事件ID与注册表规范》。\nFR-DIAG-002 智能现场快照 上报事件所产生的日志文件应该按照类型存放在指定的目录，参考 《Polaris 1.0 全局事件ID与注册表规范》。\nFR-DIAG-003 远程诊断执行 属性 内容 优先级 P1 目标 在不打扰用户的前提下，通过云端下发安全指令获取更深度的系统运行时信息。 前置条件 1. 端云建立安全加密指令通道。\n2. 密钥预置：端侧 Agent 已预置用于验签的云端公钥。\n3. 安全状态：部分交互类指令仅允许在非行车状态下执行。 输入 触发源：\n云端下发的诊断指令包 CMD_EXEC_DIAG。\n参数：\ncmd_code (指令码), params (参数列表), request_id (云端生成的唯一追踪ID), timestamp, sign (数字签名)。 处理逻辑 1. 安全沙箱校验：\n- 验签策略：Agent 使用内置公钥验证 sign，确保指令未被篡改。\n- 防重放：校验 timestamp 有效性，并检查 request_id 是否已处理过。\n- 白名单过滤：严禁直接执行 eval(raw_shell)。仅允许执行预埋的标准化指令集。\n2. 指令执行：\n在独立的子进程中执行对应逻辑，设置硬性超时时间（如 5s）。\n3. 结果截取与落盘：\n捕获标准输出 (stdout) 和标准错误 (stderr)。\n- 数据：输出内容写入本地文件。\n- 存储目录：/log/perf/diag/\n- 命名规则：diag_{cmd_code}_{request_id}_{timestamp}.txt\n- 返回数据：Payload 中仅返回生成的文件名（如 diag_PING_req123_170982.txt），后续由 VlmAgent 根据文件名及预置路径策略进行拉取。\n4. 审计记录：\n将执行记录写入本地安全审计日志。 输出 1. 结构化事件：GVM_DIAG_RESULT\n- 字段：req_id (回传云端下发的ID), code (0=Success, 1=Fail), output (小数据结果内容), logf (大数据结果的文件名)。\n2. 本地日志：/log/perf/diag/ 下的诊断结果文件。\n3. 详细结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》 数据智能与运营能力 FR-DATA-001 端云数据关联与检索 属性 内容 优先级 P0 (核心能力) 目标 消除人工查找日志的成本。确保研发人员在查看报警详情时，能够直接获取到故障现场的日志文件，实现“所见即所得”。 输入数据 1. 结构化事件：包含 evid (事件ID), vin, timestamp, logf (关联的日志文件名) 等字段。\n2. 日志文件：端侧 VlmAgent 上传至对象存储的 ZIP/TXT 文件。 业务处理规则 1. 自动关联逻辑：\n平台需建立自动化索引机制。当接收到结构化事件时，读取其 logf 字段，以此为 Key 在存储桶中查找对应的文件。无论“事件先到”还是“文件先到”，最终必须在界面上实现绑定。\n2. 生命周期管理：\n普通日志文件默认保留 30天，高危/严重级别的故障日志保留 180天。过期后自动清理以节省存储成本。 功能/展示要求 Event详情页：\n- 日志区域：在事件详情页必须包含显著的“关联日志附件”板块。\n- 状态展示：\n若文件尚未上传完毕，显示状态为 上传中... 若文件已就绪，显示文件大小（如 5MB），并提供 [下载] 按钮。 FR-DATA-002 实时预警与消息推送 属性 内容 优先级 P0 目标 建立从“故障发生”到“责任人获知”的分钟级自动化闭环，确保高危问题被即时响应。 业务处理规则 1. 灵活阈值策略：\n- 支持自定义报警规则（固定阈值或环比突增）。\n2. 智能路由：\n- 自动读取《全局事件注册表》中的 Owner 字段。\n- 维护“Owner - 飞书群/个人ID”映射关系表，实现报警精准派单。 FR-DATA-003 核心指标统计与报表 属性 内容 优先级 P0 目标 提供多维度的质量大盘报表，支持日报、周报、月报的自动统计与趋势分析，覆盖千车严重故障率与应用崩溃率。 统计维度 支持按 车型、系统版本、时间周期 (日/周/月) 进行交叉筛选。 业务处理规则 1. 千车严重故障率\n- 定义：每 1000 辆活跃车中，发生严重故障的车辆数。\n- 严重故障集：EventID 在开发过程中提供。\n- 公式：$$(\\text{发生严重故障的去重VIN数} / \\text{活跃去重VIN数}) \\times 1000$$\n- 精度：保留整数。\n2. 应用崩溃率\n- 定义：衡量应用运行的稳定性。\n- 公式：$$(\\text{Crash事件总数} / \\text{App启动事件总数}) \\times 100%$$\n- 精度：保留两位小数。 功能/展示要求 质量驾驶舱 (Dashboard)：\n- 核心卡片：展示上述指标的当前值。\n- 趋势分析：展示 日环比趋势箭头（红色代表劣化，绿色代表改善）。 FR-DATA-004 稳定性专项分析工作台 属性 内容 优先级 P1 目标 1. 提供 应用稳定性黑榜，快速识别 Crash/ANR 严重的组件。\n2. 提供 系统重启原因分析，定位导致系统非预期重启的根因（如 Watchdog、Kernel Panic）。 业务处理规则 1. 应用稳定性聚合：\n- 以 PackageName (包名) 为维度，统计选定系统版本下的 Crash/ANR 总数与影响车数。\n2. 系统重启分类逻辑：\n- 依据 boot_reason 字段进行自动化分类。 boot_reason的详细分类在开发过程中提供。\n3. 排序策略：\n- 默认按 “影响车数” 降序 排列。\n4. 数据清洗：\n- 自动过滤白名单内的测试车辆数据。 功能/展示要求 一、 应用稳定性排行榜：\n- 列定义：排名 | 应用名称 | 归属部门 | Crash 影响车数 | Crash 次数 | ANR 影响车数 | ANR 次数。\n二、 系统重启分析视图：\n- 分布饼图：直观展示“正常重启”与“异常重启”的比例；点击“异常”扇区可联动下方列表。\n- 重启原因列表：\n* 列定义：重启分类 (如 Watchdog) | 具体原因 (boot_reason) | 影响车数 | 累计发生次数。\n三、 交互交互：\n- 点击应用或重启原因，跳转至对应的 Event 详情页，以便直接下载关联的日志文件。 FR-DATA-005 性能与体验专项分析工作台 属性 内容 优先级 P1 目标 量化用户交互体验，通过 应用掉帧 (Jank) 与 冻结 (Freeze) 指标识别核心场景的性能瓶颈。 输入数据 源事件：GVM_APP_JANK_STATS\n关键字段：\n- activity_name (页面名)\n- total_frames (总帧数)\n- jank_count (掉帧数)\n- frozen_count (冻结帧数, \u003e700ms)\n- avg_ui_ms (平均UI耗时), avg_gpu_ms (平均GPU耗时) 业务处理规则 1. 核心指标计算：\n- 平均掉帧率 (Jank Rate)：$$(\\sum \\text{jank_count} / \\sum \\text{total_frames}) \\times 100%$$\n- 页面冻结率 (Freeze Session Rate)：衡量有多少次页面浏览发生了严重卡死。\n$$(\\text{Count(Events where frozen_count \u003e 0)} / \\text{Count(Total Events)}) \\times 100%$$\n- 卡顿归因倾向 (Bottleneck Check)：\n* 若 avg_ui_ms \u003e avg_gpu_ms，标记为 CPU/MainThread Bound (主线程逻辑重)。\n* 若 avg_gpu_ms \u003e avg_ui_ms，标记为 GPU/Render Bound (渲染负载重)。\n2. 分位值统计：\n- 计算 掉帧率 P90：排除偶发波动，反映绝大多数用户的真实体验底线。 功能/展示要求 流畅度分析视图：\n1. 趋势分析：\n- 展示核心应用（如 Map, Launcher）的掉帧率 P90 趋势曲线。\n2. 红黑榜 (Red/Black List)：\n- 最卡应用 Top 5：按平均掉帧率降序。\n- 最冻结页面 Top 10：按页面冻结率降序。\n3. 归因辅助：\n- 在榜单中显示“主要瓶颈”标签（如 CPU / GPU），基于 UI/GPU 耗时均值自动判定。 FR-DATA-006 单车资源画像与趋势分析 属性 内容 优先级 P2 目标 提供基于 VIN 码的深度诊断能力，通过可视化曲线回溯单车历史资源消耗趋势。 输入数据 1. 数据源：GVM_PROC_CPU_TOP (CPU 负载) 与 GVM_PROC_MEM_TOP (内存消耗) 的历史记录。\n2. 查询条件：VIN 码、起止时间（支持回溯最近 7 天）。 业务处理规则 1. 时序数据重组：\n- 云端需将该 VIN 码下分散的 30 分钟采样点连接成连续的时间序列曲线。\n2. 多维关联展示：\n- 双轴对比：支持在同一时间轴上叠加 CPU 和内存曲线，分析两者是否存在正相关性（如内存泄漏导致系统频繁 LMK 引起的 CPU 波动）。\n3. 异常锚点标注：\n- 若该时段内发生过 GVM_APP_CRASH、GVM_SYS_WATCHDOG等事件，需在曲线上方自动标注“异常图标”，点击图标可跳转至对应事件详情。 功能/展示要求 单车诊断视图：\n1. 资源趋势图：\n- CPU 轴：展示系统总负载曲线，并显示峰值时刻的 Top 5 进程名。\n- Memory 轴：展示 MemAvailable (可用内存) 变化曲线及 Top 5 进程 RSS 占用。\n2. 数据交互：\n- 缩放平移：支持对时间轴进行缩放（Zoom-in/out），查看分钟级或小时级的细微变化。\n- 悬停详情：鼠标悬停在曲线某点时，弹出 Tooltip 显示该时刻具体的进程资源排行快照。 非功能需求 本章节定义了 Polaris 1.0 平台在资源占用、性能、安全性及可靠性方面的约束指标。\n资源消耗约束 目标：确保 Agent 运行时对座舱核心业务（如仪表渲染、语音交互）的影响趋近于零。\n维度 指标要求 说明 CPU 占用 静态采样时平均 \u003c 5%；异常捕获瞬间峰值 \u003c 10% (持续 \u003c 5s) 严禁在主线程执行复杂运算，确保不引起系统卡顿。 内存占用 (PSS) 静态常驻 \u003c 150MB；日志聚合瞬间峰值 \u003c 250MB 需严格控制缓存缓冲区大小，防止触发系统 LMK。 磁盘空间占用 离线日志存储上限：(TBD) 采用循环覆盖策略，达到阈值后自动清理旧日志。 网络流量 结构化数据：\u003c 1MB/车/天(TBD)；大日志上传： (TBD) 关键告警走高优先级通道，大文件需在网络空闲时异步拉取。 系统性能与实时性 目标：确保从故障发生到研发获知的链路尽可能短，实现快速响应。\n维度 指标要求 说明 实时告警延迟 P0 级事件从端侧发出到云端推送（飞书）延迟 \u003c 10 分钟 针对重启等致命问题。 常规数据延迟 性能指标、资源水位数据延迟 \u003c 30 分钟 符合 30min 一个统计窗口的设计。 日志关联成功率 Event 与 Log 的自动关联成功率 \u003e 99% 排除网络极端断连情况。 云端并发支撑 支持同时在线车辆数：100,000 辆 (TBD) 需考虑灰度及全量推广后的并发峰值。 可靠性与稳定性 目标：监控系统自身必须比被监控系统更稳健。\n维度 指标要求 说明 服务自愈 若 PolarisAgent 非预期崩溃，需在 5s 内被守护进程自动拉起 确保监控不长时间掉线。 数据防丢失 断网期间，结构化事件需在端侧持久化缓存，恢复连接后补发 支持本地 FIFO 队列存储，容量 TBD。 容错性 Polaris 内部异常不应触发任何系统级弹窗或 ANR 提示 采用旁路监控设计，对业务流程无侵入。 安全与隐私 目标：符合国家车联网安全标准及个人隐私保护条例。\n维度 指标要求 说明 数据脱敏 日志及事件中严禁包含车主姓名、手机号、实时经纬度等 PII 数据 需对上报 Payload 进行自动脱敏扫描或正则过滤。 传输安全 端云通信全程采用 HTTPS/TLS 1.2+ 加密 防止指令被劫持。 远程诊断权限 指令下发需双重鉴权：操作人权限校验 + 终端数字签名验签 参考远程操作安全校验规则。 审计跟踪 所有远程提数和诊断操作必须 100% 留存审计日志 支持 180 天操作回溯与合规检查。 可维护性与扩展性 目标：适应座舱业务逻辑的快速演进。\n维度 指标要求 说明 配置动态更新 支持在不升级版本的情况下，动态下发采样率和阈值配置 云端下发策略包，端侧 Polaris Agent 实时生效。 协议扩展性 《全局事件注册表》支持热更新及向后兼容 新增字段不应导致旧版云端解析引擎崩溃。 端云交互协议设计 接口定义 参考：VlmWrapper接口说明文档\npublic native int init(String ecu,String appname,String ver) public native int sendWcLog(long evid, byte etype, short elevel, long eTime, String edesc); 参数描述 中文名 英文名 是否必填 参数名 数据类型 数据项说明 备注 任务序号 sn 必填 string 当前维测日志流水号（取 ECU IP 地址最后一字节 + 时间戳） 由 vlmagent 自动填充 产品类别 ProductCat 必填 string 产品类别，即域控名称，比如 s32g、tbox、adas、8295and、8295qnx；部署在各个域控上的应用此字段填对应域控名称即可 初始化传入 应用名称 ProductName 必填 string 应用名称 区分跟 vlmagent 建立的不同连接，初始化传入 版本号 Version 必填 string 应用软件版本号 如 Android / Linux 系统版本号，初始化传入 进程ID PID 必填 int 进程 ID vlmagent pid，用于区分不同进程；由 vlmagent 自动填充 事件ID EventID 必填 evid long EventID 采用 6 开头 10 位长度编码，整车统一分配各域码段，各域内部自行分配应用子码段；同一故障在不同车型需保持相同 EventID。S32G/VCM：6880000000–6889999999TBOX：6860000000–6869999999VIU：6850000000–68599999998295Android：6660000000–66699999998295QNX：6680000000–6689999999ADAS：6690000000–6699999999 [Prefix(3位)] + [Scope(1位)] + [Sequence(6位)]666：GVM（Android 系统）668：PVM（Linux Host 系统）685：MCU 子系统示例：GVM_FRAMEWORK_WATCHDOG_RESET → 6660000001映射到 Logical_Module / Owner 事件类型 EventType 必填 etype byte 1. KEYINFO：关键信息记录，由特定事件累计产生的效应；2. KEYEXCEPTION：进程或系统捕获的异常： 1）影响有限、可自动恢复； 2）严重问题，功能不可用（如 ANR、Crash、tombstone）；3. SYSERROR：操作系统层异常： 1）严重影响系统性能（内存异常、systemserver 死锁等）； 2）系统崩溃（minidump、黑屏卡死、非预期重启等） 映射 SDK_Type 事件等级 EventLevel 必填 elevel short 等级按对用户功能影响程度划分；各事件类型下有 0、1 两个等级：0 为业务级，1 为系统基础级 映射 SDK_Level 事件发生时间 HappenTime 必填 eTime long 事件发生时间，时间戳（毫秒） 事件描述 EventDescription 必填 edesc string JSON 字符串，限制 1000 字节 由 Common 字段 和 Desc_Schema 字段组成（key: value） edesc 字段组成 字段 类型 字节估算 说明 tid string ~32 bytes Trace ID。全链路追踪 ID，用于串联跨端 / 跨进程调用。 pid int ~4 bytes Process ID。结合 logcat / tombstone 时定位具体进程实例。 proc string ~20 bytes Process Name。明确是谁出的事（如 com.map.app）。 ver string ~10 bytes Version。应用 / 模块自身的版本号（如 1.2.0）。 logf string ~20 bytes Log Filename。大文件文件名（不含路径）；默认为空，手动设置。 … … … 注册表 Desc_Schema 中定义的其他业务字段。 安全与隐私 本章节定义了 Polaris 1.0 平台在数据采集、传输、存储及远程控制全生命周期中的安全防护要求，确保符合车联网数据安全国家标准。\n远程诊断安全架构 目标：防止远程诊断接口被恶意利用或误操作导致车辆受损。\n策略名称 详细要求 指令白名单 端侧仅允许执行预埋在 PolarisAgent 中的标准化指令集。严禁执行 eval、rm -rf 等具有破坏性的原始 Shell 命令。 双重身份校验 1. 云端权限校验：操作人必须具备特定安全证书。 2. 端侧验签：指令包必须携带数字签名，Agent 使用内置公钥验证指令来源的真实性。 行车状态限制 部分涉及交互或高资源消耗的指令，必须在车辆处于 非行车状态 (P档/车速为0) 时方可执行，防止分散驾驶员注意力或干扰实时性要求。 强制审计留痕 所有远程执行请求（无论成功与否）必须在端侧审计日志和云端数据库中同步记录，包含：操作人、VIN、指令内容、时间戳及返回码。 用户隐私保护 目标：在故障排查的同时，严格保护车主个人敏感信息（PII）。\n策略名称 详细要求 静态脱敏 结构化事件 Payload 中禁止采集：车主姓名、手机号、家庭住址、车牌号。 动态脱敏过滤 在截取日志文件（如 Logcat/Perflog）时，通过正则表达式自动过滤敏感信息。例如，对 MAC 地址、IP 地址、部分 VIN 码进行掩码处理（如：123****890）。 按需采集原则 严禁采集与性能/稳定性无关的数据。禁止抓取：摄像头实时画面、麦克风录音、短消息内容或通讯录数据。 用户授权 (TBD) 针对特定的深度调试指令（可能涉及部分应用路径数据），需根据合规要求触发 HMI 弹窗询问，获取用户显式授权后方可提数。 数据传输安全 目标：确保端云通信链路不可被监听或篡改。\n策略名称 详细要求 加密传输 全量数据（包括 Event 和 Log 文件）必须通过加密通道传输，强制使用 TLS 1.2 或更高版本协议。 双向认证 车端 VlmAgent 与云端网关之间采用双向数字证书认证，防止伪造云端或非法车辆接入。 防重放攻击 指令包中必须包含一次性 nonce (随机数) 或 毫秒级时间戳，端侧记录最近处理的 ID，超时或重复的指令自动丢弃。 存储与生命周期管理 目标：防止数据因过期或未授权访问而泄露。\n策略名称 详细要求 存储加密 存储在云端对象存储（OSS/S3）中的原始日志文件，必须处于加密存储状态。 访问控制 云端管理平台采用 RBAC（基于角色的权限访问控制），研发人员仅能查看其负责的应用/模块相关的故障数据。 自动销毁机制 严格执行生命周期策略：\n- 普通日志：30 天自动物理删除。\n- 严重故障日志：180 天自动清理。\n- 审计日志：根据法律要求保留 1-3 年。 风险 \u0026 限制 \u0026 依赖 1. 风险点 目标：识别可能导致项目进度延期、功能失效或系统不稳定的不确定因素。\n风险类别 描述 应对策略 性能干扰风险 监控插件（尤其是插桩逻辑）若存在性能缺陷，可能导致被监控的应用出现卡顿或 ANR。 建立端侧 Agent 专项压力测试；引入“熔断机制”，当检测到 Agent 自身资源超限时自动降级或自杀。 日志风暴风险 在极端故障（如系统级死锁并反复重启）下，可能会产生海量日志，导致存储占满或流量激增。 实施本地流量限额及“相同故障聚合”策略；仅在网络空闲下同步大型日志文件。 异构架构兼容性 跨 Hypervisor 的通信协议在不同 SOC 平台上可能存在差异。 采用标准化的 Native Daemon 接口定义，将平台相关性封装在 HAL 层，保持上层逻辑一致。 数据合规风险 国家对车联网数据出境及隐私保护政策的动态调整，可能导致现有的采集方案不合规。 保持与法务合规团队同步；系统架构设计支持“脱敏规则”的云端动态下发和快速调整。 2. 系统限制 目标：明确 Polaris 1.0 平台“不做”什么，以及受限于当前技术环境的边界。\n限制类别 描述 硬件故障捕获限制 本阶段暂不具备对纯硬件物理层故障的直接捕获与诊断能力（如 PCB 短路、传感器物理损坏、屏幕背光模组失效等）。平台目前侧重于软件稳定性、系统逻辑及软硬交互层面的监控。 深度日志追溯限制 由于 EMMC/UFS 擦写寿命及存储空间限制，离线日志仅能保留最近 10G (TBD) 的数据，超过部分将被循环覆盖。 网络依赖性 远程诊断及实时告警功能强依赖于车辆的 4G/5G 连接状态；在地下车库等信号盲区，数据仅能本地缓存，延迟上报。 代码修复限制 Polaris 平台仅提供“故障定位”与“证据收集”能力，不具备对业务 App 或系统内核进行自动代码修复（Hotfix）的功能。 3. 外部依赖 (Dependencies) 目标：明确项目成功落地所需的跨部门协同及软硬件支撑。\n依赖对象 需求描述 关键影响 车云网关 (VlmAgent) 依赖车云现有网关支持断点续传、大文件分片上传以及 TLS 1.2 加密通道。 关系到日志文件能否稳定、安全地回传至云端。 BSP/底层固件 需底层提供解析硬件复位原因（PMIC Reset Reason）的接口。 关系到“Linux Host 重启”原因判定的准确性。 云端基础设施 依赖 IT/车云部门提供高可用的对象存储 (OSS) 以及大数据计算集群支持。 关系到海量结构化数据的实时计算处理与可视化。 实施计划 阶段划分 (Phases) 项目整体周期预计为 6个月 (TBD)，分为四个核心阶段：\n阶段 阶段名称 关键任务 交付物 M1 架构设计与协议定标 1. 完成端云交互协议定义（JSON/ProtoBuf）。\n2. 制定《全局事件 ID 注册表 1.0》。\n3. 完成端侧 Agent 分层架构设计。 《端云接口规范》\n《全局事件注册表》 M2 端侧核心能力开发 1. 开发 PolarisAgent 核心服务（含流控、聚合逻辑）。\n2. 实现 Android/Linux 跨域通信通道。\n3. 集成基础异常捕获（Crash/ANR/Reboot）。 Polaris Agent Alpha版\n测试验证报告 M3 云端平台与数据闭环 1. 建设云端事件解析引擎与大盘（Dashboard）。\n2. 实现“事件-日志”自动关联引擎。\n3. 开展端云一体化全链路冒烟测试。 云端管理台看板\n端云联调报告 M4 灰度发布与全量试点 1. 选取 100~500 台灰度车辆进行试点部署。\n2. 根据灰度数据修正指标阈值与流控策略。\n3. 性能压测与全量版本推送。 灰度运营报告\n正式版 SOP 手册 资源需求计划 (Resource Allocation) 人力资源 预计投入总人力：XX 人/月。\n职能角色 预估人数 核心职责 端侧开发工程师 3-4 名 负责 Android Framework 插桩、Native Daemon 及异构数据采集。 云端开发工程师 TBD 负责大数据流式计算、API 开发及规则引擎构建。 前端开发工程师 TBD 负责可视化驾驶舱、单车画像及诊断工作台页面。 测试/质量工程师 1-2 名 负责端侧资源消耗专项测试、数据准确性验证。 系统架构师 1 名 负责端云协议统筹、跨域通信架构及安全策略设计。 基础设施与硬件 资源类别 规格/描述 用途 对象存储 (OSS) TBD 用于存储非结构化日志文件（Logcat, Trace, Bootchart）。 云端数据库 TBD 用于存储高性能/资源水位等结构化指标数据。 测试样车 3~5 辆 (含高通 8155/8295 平台) 用于端侧采集能力的实车验证及压测。 边缘计算配额 TBD 用于远程诊断指令下发。 外部协同依赖 OTA 团队：负责 Polaris Agent 插件的差分分发与静默安装支持。 IT 网络团队：负责配置端云通信的专用域名及 TLS 证书安全准入。 附录 ","wordCount":"4120","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ethen-cao.github.io/ethenslab/others/%E6%99%BA%E8%83%BD%E5%BA%A7%E8%88%B1%E7%AB%AF%E4%BA%91%E4%B8%80%E4%BD%93%E6%80%A7%E8%83%BD%E4%B8%8E%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%B9%B3%E5%8F%B0%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/"},"publisher":{"@type":"Organization","name":"Ethen 的实验室","logo":{"@type":"ImageObject","url":"https://ethen-cao.github.io/ethenslab/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethen-cao.github.io/ethenslab/ accesskey=h title="Ethen 的实验室 (Alt + H)">Ethen 的实验室</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethen-cao.github.io/ethenslab/android-dev/ title=Android系统开发><span>Android系统开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/android-automotive-os-dev/ title="Android Automotive"><span>Android Automotive</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/qnx/ title=QNX开发><span>QNX开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/gunyah/ title=Gunyah><span>Gunyah</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/ivi-solution/ title=智能座舱方案><span>智能座舱方案</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/explore-ai title="Explore AI"><span>Explore AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethen-cao.github.io/ethenslab/>Home</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/others/>杂记</a></div><h1 class="post-title entry-hint-parent"></h1><div class=post-meta>20 min&nbsp;·&nbsp;4120 words</div></header><div class=post-content><h1 id=智能座舱端云一体性能与稳定性平台-polaris-10-系统设计文档>智能座舱端云一体性能与稳定性平台 (Polaris 1.0) 系统设计文档<a hidden class=anchor aria-hidden=true href=#智能座舱端云一体性能与稳定性平台-polaris-10-系统设计文档>#</a></h1><h2 id=版本信息>版本信息<a hidden class=anchor aria-hidden=true href=#版本信息>#</a></h2><table><thead><tr><th>序号</th><th>版本</th><th>修订内容</th><th>状态</th><th>修订人</th><th>日期</th></tr></thead><tbody><tr><td>1</td><td>0.1</td><td>First draft</td><td></td><td>操权力</td><td>2025/12/31</td></tr></tbody></table><h2 id=文档目的>文档目的<a hidden class=anchor aria-hidden=true href=#文档目的>#</a></h2><p>本文档旨在全面定义 智能座舱端云一体性能与稳定性平台 (代号 Polaris 1.0) 的系统架构、功能需求及实施路径。本文档将服务于以下核心场景：</p><ul><li>管理层决策：清晰阐述项目背景、痛点、目标及资源需求，作为立项审批与资源调度的依据。</li><li>跨部门协同：作为座舱平台部与车云平台部沟通数据协议、接口规范及边界划分的“蓝本”，确保端云技术方案的一致性。</li><li>工程落地指导：作为项目启动后的核心输入，指导研发团队进行端侧 Agent 开发、埋点设计及测试验收。</li></ul><h2 id=背景与问题定义>背景与问题定义<a hidden class=anchor aria-hidden=true href=#背景与问题定义>#</a></h2><h3 id=背景>背景<a hidden class=anchor aria-hidden=true href=#背景>#</a></h3><p>当前智能座舱的数据建设存在数据维度失衡与底层感知缺失的问题，具体表现在以下三个方面：</p><ul><li>应用质量量化手段缺失：目前虽已具备应用层的业务埋点能力（如页面点击流），能支撑产品运营分析；但对于应用技术质量（如 Crash率、ANR率）及 核心性能指标（如启动耗时、页面响应延迟）尚缺乏系统性的监控与度量手段，导致软件交付质量缺乏客观数据支撑。</li><li>平台侧缺乏云端可观测性：作为座舱底座的平台研发部门，目前缺乏专属的云端观测平台。对于线上车辆的系统级健康度（如 SystemServer 重启、关键服务存活、资源水位），研发团队缺乏实时获取线上运行时状态的能力，往往只能在故障发生后进行被动回溯。</li><li>系统稳定性保障体系亟待构建：随着智能座舱软件规模与复杂度的提升，单纯依赖线下测试已难以覆盖所有边缘场景。为了保障用户体验，亟需构建一套严谨的、标准化的端云一体性能与稳定性监控平台，实现对线上真实运行质量的精准监测与闭环管理。</li></ul><h3 id=当前痛点>当前痛点<a hidden class=anchor aria-hidden=true href=#当前痛点>#</a></h3><table><thead><tr><th>痛点</th><th>描述</th><th>业务影响</th></tr></thead><tbody><tr><td>跨端故障排查成本较高</td><td>当前缺乏跨端（Android-Linux-MCU）的自动化关联数据，面对复杂的跨域交互问题，排查过程往往需要人工拼接多端日志。</td><td>研发效率受限：故障定位往往需要多方协同与多次排查，拉长了问题的解决周期。</td></tr><tr><td>性能量化数据覆盖不足</td><td>现有的性能评估主要依赖线下测试或有限样本，缺乏全量用户场景下的启动速度、流畅度等自动化量化数据。</td><td>版本评价受限：难以精确捕捉版本迭代中的细微性能波动，线上实际体验的评估数据不够丰满。</td></tr><tr><td>偶发异常现场回溯困难</td><td>对于线上偶发的非必现问题，目前主要依赖事后尝试复现，缺乏异常发生瞬间的自动“快照”捕获机制。</td><td>闭环周期较长：部分偶发性稳定性问题（如随机黑屏、卡顿）因缺乏现场数据支持，难以快速彻底根除。</td></tr><tr><td>资源效能优化缺乏支撑</td><td>缺乏进程级的 CPU、内存、IO 历史趋势画像，在进行精细化资源管控时缺乏足够的数据颗粒度。</td><td>成本优化受限：硬件资源规划倾向于保守策略以保障稳定性，BOM 成本的进一步精细化挖掘存在困难。</td></tr></tbody></table><h2 id=目标与范围>目标与范围<a hidden class=anchor aria-hidden=true href=#目标与范围>#</a></h2><h3 id=项目目标>项目目标<a hidden class=anchor aria-hidden=true href=#项目目标>#</a></h3><p>本项目旨在基于 “端侧深度探针 + 云端聚合分析 + 全链路追踪” 的技术理念，构建 Polaris 1.0 端云一体化平台，实现以下三个核心目标：</p><ol><li>全链路可观测：打破 Android、Linux Host、MCU 的数据孤岛，建立统一的 全局事件标准 (Global Event ID)，将分散在不同系统的故障与状态数据聚合至同一平台，实现跨端事件的追踪，为后续的可视化链路分析奠定数据基础。</li><li>故障现场自动聚合与关联：突破现有“日志碎片化”及“事后拉取不全”的局限。建立 “事件驱动” 的现场快照机制，在异常发生瞬间，自动聚合与该事件强相关的全维度上下文信息（如 Trace、系统 Log、进程状态等）并生成完整的故障证据包。这不仅实现了 Event 与 Log 的精准索引，更确保了现场信息的完整性，能有效解决因关键日志缺失导致无法定位的难题。</li><li>数据驱动治理：建立系统级的性能与稳定性基线，通过量化数据驱动版本质量改善与硬件资源优化，将质量管理从“定性”转向“定量”。</li></ol><h3 id=核心-kpi-指标>核心 KPI 指标<a hidden class=anchor aria-hidden=true href=#核心-kpi-指标>#</a></h3><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标名称</th><th style=text-align:left>目标值 (示例)</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left>稳定性</td><td style=text-align:left>故障主动捕获率</td><td style=text-align:left>> 90%</td><td style=text-align:left>指系统发生崩溃（如 SurfaceFlinger Crash）、重启后，平台能自动感知识别，无需依赖用户反馈。</td></tr><tr><td style=text-align:left>诊断效率</td><td style=text-align:left>故障取证自动化率</td><td style=text-align:left>100%</td><td style=text-align:left>针对严重异常事件，实现 Event 与 Log 的自动关联，确保研发直接获取现场数据。</td></tr><tr><td style=text-align:left>性能体验</td><td style=text-align:left>核心页面流畅度达标率</td><td style=text-align:left>> 95%</td><td style=text-align:left>监测核心 App 在全量车辆中的掉帧情况，量化用户感知的卡顿程度。</td></tr><tr><td style=text-align:left>资源能效</td><td style=text-align:left>高功耗场景识别产出</td><td style=text-align:left>TOP 5/季度</td><td style=text-align:left>识别并输出高资源占用的异常场景，支撑性能治理。</td></tr></tbody></table><h3 id=项目范围>项目范围<a hidden class=anchor aria-hidden=true href=#项目范围>#</a></h3><h4 id=范围内>范围内<a hidden class=anchor aria-hidden=true href=#范围内>#</a></h4><ol><li><p>端侧全栈感知体系：</p><ul><li>Android 深度探针：构建系统级监控服务 <code>PolarisAgentService</code>，实现对应用生命周期、核心服务状态、底层资源（CPU/Memory/IO/Binder）的<strong>全维度深度监听</strong>。</li><li>Linux/MCU 异构覆盖：建设 Linux Host 侧的<strong>系统健康守护进程</strong>，负责关键服务（Service）存活检测与系统指标采集；适配 MCU 通信协议，实现异构芯片间的故障透传。</li><li>边缘智能处理：在端侧实现数据的<strong>预处理与清洗</strong>，包含事件聚合、流控防爆、日志现场的智能截取与压缩，减轻车云带宽压力。</li><li>标准化基础设施：建立《全局事件注册表》及自动化工具链，统一多端的数据定义与协议标准。</li></ul></li><li><p>云端分析能力需求:</p><ul><li>元数据管理能力：要求云端支持同步《全局事件注册表》，实现对上报事件的自动化解析、分类与标签化管理。</li><li>自动化关联引擎：要求云端具备 “事件-日志” 自动匹配能力，将结构化的 Event 数据与非结构化的 Log 文件在存储层自动关联，形成完整的故障证据包。</li><li>趋势与模式识别：要求云端支持基于时间窗口的聚合计算，能够显示异常爆发趋势及性能指标（CPU/内存）的长期演进趋势。</li></ul></li><li><p>可视化与运营平台</p><ul><li>数字化质量驾驶舱：建设多维度的质量仪表盘（Dashboard），支持按版本、车型、时间段分析千车故障率、性能基线达标率。</li><li>智能排查工作台：提供“一站式”问题分析界面，支持通过 EventID/TraceID 检索故障，直接浏览关联的日志及设备状态，支持远程诊断指令的下发与结果展示。</li></ul></li></ol><h4 id=范围外>范围外<a hidden class=anchor aria-hidden=true href=#范围外>#</a></h4><ol><li>可视化的全链路拓扑分析：1.0 阶段聚焦于跨端链路数据的 标准化采集与逻辑串联，优先夯实数据底座能力；全链路图形化的调用链拓扑展示规划在后续版本迭代中实现。</li><li>业务代码修复：Polaris 平台负责精准“定位”并“指派”问题，<strong>不负责</strong> 具体业务 APP 内部的代码逻辑修复。</li><li>交互体验设计：本项目专注于性能数据的量化，<strong>不包含</strong> HMI 界面（UI/UE）的主观交互设计与优化。</li></ol><h2 id=业务流程与核心场景>业务流程与核心场景<a hidden class=anchor aria-hidden=true href=#业务流程与核心场景>#</a></h2><h3 id=角色定义>角色定义<a hidden class=anchor aria-hidden=true href=#角色定义>#</a></h3><table><thead><tr><th>角色</th><th>职责描述</th><th>关注点</th></tr></thead><tbody><tr><td>研发工程师</td><td>接收告警，分析堆栈与日志，修复 Bug；针对疑难客诉问题，远程下发特定诊断指令</td><td>故障堆栈的完整性，日志关联的准确性，是否需要补充更多运行时信息。</td></tr><tr><td>质量工程师</td><td>配置告警阈值，监控线上大盘水位，识别版本质量风险</td><td>故障率趋势是否劣化，性能指标是否符合预期。</td></tr><tr><td>产品经理</td><td>查看应用活跃度与性能体验趋势</td><td>核心功能的响应速度趋势，用户使用过程中的卡顿频率。</td></tr></tbody></table><h3 id=核心作业流程图>核心作业流程图<a hidden class=anchor aria-hidden=true href=#核心作业流程图>#</a></h3><ol><li>故障主动发现闭环流程</li></ol><blockquote><p><em>描述从异常发生到研发接入的处理路径</em></p></blockquote><ul><li>捕获 (Capture): Polaris Agent 监听到异常（如 ANR），记录运行时状态，抓取 Trace/Logcat，并生成唯一 EventID。</li><li>处理 (Process): 端侧进行流量控制检查，通过 <code>logf</code> 索引将 Event 与 Log 文件进行逻辑组合。</li><li>上报 (Report): Event 数据实时上报，大文件 Log 在网络空闲时段异步上传（支持云端按需拉取）。</li><li>通知 (Notify): 云端检测到异常数据，向 <strong>责任模块负责人</strong> 发送通知。</li><li>分析 (Analyze): 研发工程师查看通知，进入平台查看关联的上下文数据，确认问题根因并修复。</li></ul><ol start=2><li>疑难问题排查流程</li></ol><blockquote><p><em>描述针对复杂客诉处理路径</em></p></blockquote><ul><li>检索: 研发工程师在平台输入车辆 VIN 码或 EventID 检索相关记录。</li><li>查看: 系统展示该事件的发生时间、设备信息、以及<strong>已自动关联</strong>的 Log 文件下载链接。</li><li>诊断: 针对区域技术支持无法处理的复杂客诉，若现有日志不足以定位，<strong>研发工程师</strong> 通过控制台下发诊断指令，端侧执行后回传结果，以获取更深度的运行时信息。</li></ul><h3 id=典型用户故事>典型用户故事<a hidden class=anchor aria-hidden=true href=#典型用户故事>#</a></h3><h4 id=场景一风险预警>场景一：风险预警<a hidden class=anchor aria-hidden=true href=#场景一风险预警>#</a></h4><blockquote><p>背景: 某车型灰度推送 v1.5 OTA 版本。
事件: 上线 24 小时内，Polaris 平台监测到 <code>GVM_SYS_STORAGE_LOW</code>（磁盘空间不足）事件在特定批次车辆上的上报。
行动:</p><ol><li>平台自动触发 <strong>风险预警</strong>，即时通知研发负责人。</li><li>研发工程师通过平台获取存储分布数据，精准定位到某应用私有目录占用空间急剧膨胀。</li><li>分析: 结合自动关联采样的 Log，确认该应用在特定异常分支下陷入<strong>数据库高频重复写入</strong>死循环。
结果: 研发团队在磁盘被完全耗尽导致系统挂死（System Hang）前，紧急输出修复补丁并推送 OTA，成功拦截了批量重大事故。</li></ol></blockquote><h4 id=场景二稳定性治理>场景二：稳定性治理<a hidden class=anchor aria-hidden=true href=#场景二稳定性治理>#</a></h4><blockquote><p>背景: 某应用发布 v2.0 灰度版本。
事件: 灰度发布期间，平台监测到应用出现<strong>偶发性</strong> <code>GVM_APP_ANR</code>（无响应）告警，且线下测试难以复现。
行动:</p><ol><li>研发工程师点击告警详情，查看聚合后的故障样本。</li><li>系统已通过 <code>logf</code> 字段自动关联了故障时刻 <code>perflog</code> (性能日志)。</li><li>分析: 工程师通过日志文件发现应用主线程阻塞在 Binder IPC 调用中；进一步联合分析 <code>perflog</code>，定位到是对端 Service 在高并发场景下因锁竞争导致处理耗时过长，拖累了客户端。
结果: 确认根因为<strong>服务端卡顿</strong>。研发工程师针对服务端逻辑进行异步化优化，彻底解决了这一隐蔽的跨进程阻塞问题。</li></ol></blockquote><h4 id=场景三性能监控>场景三：性能监控<a hidden class=anchor aria-hidden=true href=#场景三性能监控>#</a></h4><blockquote><p>背景: 某版本上线后，产品经理关注核心应用在复杂交互场景下的滑动流畅度。
事件: Polaris 仪表盘显示 <code>GVM_APP_JANK</code> (掉帧/卡顿) 指标在特定列表滑动场景下出现劣化趋势。
行动:</p><ol><li>查询掉帧期间主线程的MessageQueue事件。</li><li>发现: 在卡顿发生的时间段内，主线程 MessageQueue 待处理消息数量显著激增。</li><li>分析: 研发工程师通过分析采集到的 <code>Looper</code> 统计数据，发现是一次性加载过多列表项导致并在主线程频繁 Post UI 刷新消息，引发<strong>主线程消息队列积压</strong>，从而阻塞了渲染信号（Vsync）的处理。
结果: 研发工程师引入消息合并与节流机制，消除了主线程拥堵，恢复了滑动流畅性。</li></ol></blockquote><h4 id=场景四远程指令下发>场景四：远程指令下发<a hidden class=anchor aria-hidden=true href=#场景四远程指令下发>#</a></h4><blockquote><p>背景: 用户反馈方控按键（下一曲）失效，或错误地控制了不显示在屏幕上的后台音乐应用，常规 Logcat 无法体现系统内部的分发逻辑。
行动:</p><ol><li>研发工程师怀疑是 MediaSession 焦点抢占或状态同步异常。</li><li>工程师通过 Polaris 控制台，向目标车辆下发 dumpsys media_session 指令。</li><li>分析: 回传的诊断结果显示，Media button session 仍被后台应用 com.reachauto.clouddesk 占用（尽管其状态为 active=false），导致按键事件未正确分发给前台亮屏的 com.tencent.wecarflow。
结果: 确认根因是后台应用未正确释放焦点，研发工程师将 Bug 准确指派给相关应用团队，无需现场抓包。</li></ol></blockquote><h2 id=需求拆解>需求拆解<a hidden class=anchor aria-hidden=true href=#需求拆解>#</a></h2><p>本章节将 Polaris 1.0 平台的核心需求拆解为四大关键能力域。这些能力定义了系统的边界与核心价值，是后续详细功能设计的基础。
本章节采用能力域拆解方法，以系统应具备的核心能力为中心，而非具体功能或实现方式。每一能力域仅定义目标、适用范围与责任边界，不涉及接口设计、数据结构或技术选型细节。具体功能点将在后续《功能性需求》中展开，质量与约束要求将在《非功能性需求》中统一定义。</p><h3 id=稳定性全栈感知能力>稳定性全栈感知能力<a hidden class=anchor aria-hidden=true href=#稳定性全栈感知能力>#</a></h3><p><strong>目标</strong>：构建覆盖 Android 应用层、系统框架层以及 Linux Host/MCU 异构计算单元的异常捕获体系，实现全栈、全维度的故障感知与现场数据留存。</p><table><thead><tr><th>能力名称</th><th>能力描述与目标</th><th>适用范围</th><th>责任边界</th></tr></thead><tbody><tr><td><strong>应用层稳定性监控</strong></td><td><strong>描述</strong>：具备对 Android <strong>应用层 (APK)</strong> 致命异常的实时监测能力。涵盖 Java Crash、App Native Crash (JNI)、ANR 及 App OOM；在异常触发时同步执行现场冻结与堆栈抓取。<br><strong>目标</strong>：确保应用级崩溃能被捕捉和上报。</td><td>Android Framework<br>Third-party Apps<br>System Apps (Launcher等)</td><td><strong>负责</strong>：捕获应用堆栈、页面栈及进程状态；<br><strong>不负责</strong>：分析应用内部具体的业务逻辑错误。</td></tr><tr><td><strong>系统框架稳定性监控</strong></td><td><strong>描述</strong>：具备对 <strong>Android 核心服务</strong>及<strong>关键守护进程</strong>的存活状态监测能力；识别系统级资源耗尽风险（如 Binder 耗尽、句柄泄漏、LMK）。<br><strong>目标</strong>：准确识别 SystemServer 死锁 (Watchdog)、核心服务崩溃、系统异常重启及严重资源拥堵事件。</td><td>SystemServer<br>Binder Driver<br>Native Daemons (SurfaceFlinger等)</td><td><strong>负责</strong>：识别导致系统不稳定的服务异常和资源瓶颈；<br><strong>不负责</strong>：介入 Linux Kernel 内部调度机制的调试。</td></tr><tr><td><strong>异构运行环境监控</strong><br>(Heterogeneous Env Monitoring)</td><td><strong>描述</strong>：具备对 <strong>Linux Host (PVM)</strong> 及 <strong>MCU</strong> 运行状态的独立监测能力。通过 Native Daemon 标准化采集 Linux 侧服务状态、系统重启事件以及 MCU 侧的心跳与硬件故障码。<br><strong>目标</strong>：实现对底层虚拟化环境与硬件外设健康状况的统一视图监控。</td><td>Linux Host (PVM)<br>MCU<br>Hypervisor</td><td><strong>负责</strong>：异构数据的标准化接入、协议对齐及状态监测；<br><strong>不负责</strong>：异构系统内部具体业务逻辑的监控实现。</td></tr></tbody></table><h3 id=性能与资源度量能力>性能与资源度量能力<a hidden class=anchor aria-hidden=true href=#性能与资源度量能力>#</a></h3><p><strong>目标</strong>：建立可量化的性能基线，从“主观体验”转向“客观数据”，实现对计算资源（CPU/Mem/IO）及系统启动效率的精细化审计。</p><table><thead><tr><th style=text-align:left>能力名称</th><th style=text-align:left>能力描述与目标</th><th style=text-align:left>适用范围</th><th style=text-align:left>责任边界</th></tr></thead><tbody><tr><td style=text-align:left><strong>交互体验量化</strong></td><td style=text-align:left><strong>描述</strong>：具备对用户核心交互体验的监测能力。重点涵盖<strong>应用启动耗时</strong>、<strong>主线程健康度 (MessageQueue)</strong> 以及<strong>界面流畅度 (FPS/Jank)</strong>。<br><strong>目标</strong>：量化 App 启动速度，识别主线程阻塞与掉帧现象。</td><td style=text-align:left>Top 核心应用<br>Launcher<br>SystemUI</td><td style=text-align:left><strong>负责</strong>：采集启动耗时、MessageQueue 调度延迟及绘制帧率；<br><strong>不负责</strong>：应用页面内部的数据加载逻辑。</td></tr><tr><td style=text-align:left><strong>资源水位画像</strong></td><td style=text-align:left><strong>描述</strong>：具备对<strong>进程级</strong>资源消耗（CPU使用率、内存占用、IO吞吐量、句柄数）的周期性采样与 TOP 排行识别能力。<br><strong>目标</strong>：识别<strong>高资源消耗进程</strong>与内存/句柄泄漏，绘制 24h 资源趋势图。</td><td style=text-align:left>Android GVM 进程<br>Native 守护进程</td><td style=text-align:left><strong>负责</strong>：资源数据的统计、归因与异常阈值判定；<br><strong>不负责</strong>：操作系统的资源调度策略 (Scheduler)。</td></tr><tr><td style=text-align:left><strong>系统级性能监控</strong></td><td style=text-align:left><strong>描述</strong>：具备对<strong>操作系统级</strong>关键性能指标的监测能力，重点涵盖系统启动耗时及磁盘整体负载。<br><strong>目标</strong>：确保座舱冷启动（Cold Boot）时间达标，监控存储设备寿命与性能衰减。</td><td style=text-align:left>Android Boot Process<br>Storage Device (UFS/eMMC)</td><td style=text-align:left><strong>负责</strong>：各启动阶段（BootLoader/Kernel/UserSpace）的耗时分解；<br><strong>不负责</strong>：缩短硬件初始化时间。</td></tr></tbody></table><h3 id=现场还原与协同能力>现场还原与协同能力<a hidden class=anchor aria-hidden=true href=#现场还原与协同能力>#</a></h3><p><strong>目标</strong>：解决“有报警无日志”的痛点，构建端云协同的自动化取证与远程诊断通道。</p><table><thead><tr><th>能力名称</th><th>能力描述与目标</th><th>适用范围</th><th>责任边界</th></tr></thead><tbody><tr><td><strong>标准化事件协议体系</strong></td><td><strong>描述</strong>：基于《全局事件注册表》构建统一的事件定义、序列化与解析能力。<br><strong>目标</strong>：确保端侧上报数据与云端解析引擎的语义一致性，支持协议动态扩展。</td><td>端侧 Agent, 车云 SDK, 云端解析服务</td><td>负责协议的定义与维护工具链；不限制业务 Payload 的具体内容。</td></tr><tr><td><strong>智能现场快照</strong></td><td><strong>描述</strong>：具备“事件驱动”的自动化日志聚合能力，在异常发生瞬间关联并打包 Trace、Logcat 及系统状态信息。<br><strong>目标</strong>：实现 Event 与 Log 文件的 1:1 精准索引。</td><td>本地日志系统, 文件系统</td><td>负责日志的定位、截取与压缩；不负责日志内容的语义分析。</td></tr><tr><td><strong>远程诊断执行</strong></td><td><strong>描述</strong>：具备安全可控的云端指令接收与本地执行能力，支持下发 Shell 脚本或调试命令。<br><strong>目标</strong>：在不打扰用户的前提下获取更深度的运行时信息。</td><td>Shell 环境, Debug 接口</td><td>负责指令通道的建立与执行结果回传；严禁执行未授权的高危写操作，不支持批量执行、默认灰度单车、需显式授权、强审计。</td></tr></tbody></table><h3 id=数据智能与运营能力>数据智能与运营能力<a hidden class=anchor aria-hidden=true href=#数据智能与运营能力>#</a></h3><p><strong>目标</strong>：将海量原始数据转化为可行动的洞察（Actionable Insights），支撑研发与质量团队的决策。</p><table><thead><tr><th>能力名称</th><th>能力描述与目标</th><th>适用范围</th><th>责任边界</th></tr></thead><tbody><tr><td><strong>端云数据自动关联</strong></td><td><strong>描述</strong>：具备在海量存储中，根据索引自动将结构化事件与非结构化日志文件进行绑定的能力。<br><strong>目标</strong>：消除人工查找日志的成本。</td><td>云端存储层</td><td>负责数据的逻辑关联与存储生命周期管理。</td></tr><tr><td><strong>实时风险预警</strong></td><td><strong>描述</strong>：具备基于时间窗口的流式计算能力，识别线上故障的爆发趋势并触发告警。<br><strong>目标</strong>：实现故障感知。</td><td>计算引擎</td><td>负责告警策略的计算与推送；不负责告警后的工单流转。</td></tr><tr><td><strong>多维质量可视化</strong></td><td><strong>描述</strong>：具备多维度（版本/车型/时间/地区）的数据聚合与图表展示能力。<br><strong>目标</strong>：提供从“宏观大盘”到“微观个案”的钻取分析视图。</td><td>数据仓库, 可视化前端</td><td>负责数据的可视化呈现。</td></tr></tbody></table><h2 id=系统总体方案>系统总体方案<a hidden class=anchor aria-hidden=true href=#系统总体方案>#</a></h2><h3 id=总体设计概述>总体设计概述<a hidden class=anchor aria-hidden=true href=#总体设计概述>#</a></h3><p>Polaris 1.0 基于 <strong>Hypervisor 虚拟化架构</strong> 设计，旨在构建跨越 <strong>GVM (Guest VM - Android)</strong>、<strong>PVM (Primary VM - Linux)</strong> 及 <strong>MCU</strong> 的端云一体化全栈监控系统。
系统采用 <strong>分层架构</strong> 与 <strong>模块化服务设计</strong>。在控制面上，通过 <strong>注册表驱动（Registry-Driven）</strong> 机制实现业务埋点定义与底层采集逻辑的解耦；在数据面上，通过 <strong>双守护进程（Dual Daemon）</strong> 机制打通异构芯片与系统的通信壁垒。系统将计算能力前置至端侧，通过 <code>PolarisAgentService</code> 实现数据的实时清洗、流控与现场关联，仅将高价值的结构化数据与诊断日志同步至云端。</p><h3 id=系统总体架构图>系统总体架构图<a hidden class=anchor aria-hidden=true href=#系统总体架构图>#</a></h3><p><img src=/ethenslab/images/Polaris1.0.drawio.png alt></p><h3 id=架构分层详解>架构分层详解<a hidden class=anchor aria-hidden=true href=#架构分层详解>#</a></h3><h4 id=业务应用与接口层>业务应用与接口层<a hidden class=anchor aria-hidden=true href=#业务应用与接口层>#</a></h4><p>本层负责定义数据采集的标准接口，通过自动代码生成技术屏蔽底层通信细节：</p><ul><li><strong>Polaris SDK</strong>: 面向上层业务应用（如 Launcher, Maps）。该组件由《全局事件注册表》编译生成，提供强类型的事件对象封装与校验逻辑，负责将业务数据序列化并传递给 Framework 层。</li><li><strong>System Internal SDK</strong>: 面向 SystemServer 内部服务（如 AMS, WMS）。与 Polaris SDK 同源生成，专门用于系统关键服务内部的插桩（Instrumentation），以捕获服务级异常与状态变更。</li><li><strong>Host SDK</strong>: 面向 PVM 侧的 Linux 应用程序（如 Cluster HMI）、系统核心服务，提供 C++ 标准上报接口，负责将 PVM 侧业务数据发送至 Host Daemon。</li></ul><h4 id=框架传输与核心服务层>框架传输与核心服务层<a hidden class=anchor aria-hidden=true href=#框架传输与核心服务层>#</a></h4><p>本层位于 Android GVM，是数据汇聚、策略执行与处理的核心区域：</p><ul><li><p><strong>Polaris SDK (Framework API)</strong>: 部署于 <code>AppFramework API</code> 层。作为系统级的传输接口实现，它承接来自上层业务的调用请求，并维护与*PolarisAgentService的 IPC 通信链路，确保数据的可靠投递。</p></li><li><p><strong>PolarisAgentService</strong>: 常驻系统服务，内部包含五个核心功能模块：</p><ul><li><strong>EventCollector</strong>: <strong>统一接入模块</strong>。作为 Binder 服务端接收 Polaris SDK 请求；同时作为 LocalSocket 客户端，在服务启动时主动连接 Native Daemon 建立长连接通道，并通过后台线程实时拉取 Native 侧上报的事件流。</li><li><strong>ConfigManager</strong>: <strong>配置管理模块</strong>。负责加载本地注册表文件的配置，解析采样率、阈值及采集开关策略。</li><li><strong>FlowController</strong>: <strong>流量控制模块</strong>。对输入事件进行频率限制，防止异常爆发导致系统资源耗尽。</li><li><strong>ContextEngine</strong>: <strong>现场聚合模块</strong>。在事件通过流控后，负责生成唯一 EventID，挂载系统时间戳，并根据事件类型关联 Logcat、Trace 文件及进程快照，生成索引信息。</li><li><strong>DiagnoseHandler</strong>: <strong>诊断执行模块</strong>。负责校验并执行来自云端的诊断指令（Shell Command），并管理执行结果的回传。</li></ul></li></ul><h4 id=原生与异构跨域层>原生与异构跨域层<a hidden class=anchor aria-hidden=true href=#原生与异构跨域层>#</a></h4><p>本层负责 Android Runtime 之外的底层环境监控及跨虚拟机通信：</p><ul><li><p><strong>Polaris Native Daemon (GVM)</strong>:</p></li><li><p><strong>本地采集</strong>: 负责监控 Native 进程崩溃（Tombstone）、系统资源、及 HAL 层状态。</p></li><li><p><strong>跨域网关</strong>: 作为 GVM 侧的通信端点，维护与 PVM/MCU 的连接，接收跨域透传的数据。</p></li><li><p><strong>Polaris Host Daemon (PVM)</strong>:</p></li><li><p><strong>宿主监控</strong>: 负责监控 PVM 侧的 <code>systemd</code> 服务状态、关键驱动状态及虚拟机管理服务（qcrosvm/VMM）。</p></li></ul><h4 id=传输通道与云平台>传输通道与云平台<a hidden class=anchor aria-hidden=true href=#传输通道与云平台>#</a></h4><ul><li><strong>VlmAgent</strong>: <strong>统一传输网关</strong>。作为端侧唯一的数据出口，负责接收来自 <code>PolarisAgentService</code> 的结构化事件,以及日志文件（按需拉取），执行断点续传、数据压缩与网络流量调度。</li><li><strong>Cloud Platform</strong>: 负责数据的计算、存储与可视化。</li></ul><h3 id=核心设计原则>核心设计原则<a hidden class=anchor aria-hidden=true href=#核心设计原则>#</a></h3><ol><li>进程级隔离与服务化：PolarisAgentService 设计为独立系统进程，而非 SystemServer 的内部线程。这种设计带来了两大优势：<ul><li>稳定性：监控服务的异常（如 OOM）不会导致系统核心服务（SystemServer）崩溃，反之亦然。</li><li>高性能：独立的进程空间避免了与 AMS/WMS 争抢主线程资源，确保了监控逻辑的独立调度。</li></ul></li><li>系统核心即客户端：确立 SystemServer 在监控体系中的 Client 身份。AMS、WMS 等核心服务通过 System Internal SDK，以跨进程调用（IPC）的方式向 Polaris 上报数据。这种“旁路监控”模式确保了对系统原有逻辑的最小侵入。</li><li>异构接入抽象化：针对 MCU 等异构单元，系统采用 &ldquo;HAL 驱动适配 + Daemon 统一采集&rdquo; 的接入原则。不强依赖特定的物理连接方式（如直连或透传），而是通过 Native 层的适配层（Adapter/HAL）屏蔽硬件连接差异，确保架构在不同车型硬件拓扑下的通用性与兼容性。</li></ol><h2 id=功能性需求>功能性需求<a hidden class=anchor aria-hidden=true href=#功能性需求>#</a></h2><h3 id=稳定性全栈感知能力-1>稳定性全栈感知能力<a hidden class=anchor aria-hidden=true href=#稳定性全栈感知能力-1>#</a></h3><h4 id=应用层稳定性监控>应用层稳定性监控<a hidden class=anchor aria-hidden=true href=#应用层稳定性监控>#</a></h4><h5 id=fr-stab-001-应用-java-崩溃-java-crash-捕获>FR-STAB-001 应用 Java 崩溃 (Java Crash) 捕获<a hidden class=anchor aria-hidden=true href=#fr-stab-001-应用-java-崩溃-java-crash-捕获>#</a></h5><table><thead><tr><th>属性</th><th>内容</th></tr></thead><tbody><tr><td><strong>优先级</strong></td><td>P0</td></tr><tr><td><strong>前置条件</strong></td><td>1.系统层已部署全局监控探针。<br>2. 监控功能的配置开关处于开启状态。</td></tr><tr><td><strong>输入</strong></td><td><strong>触发源</strong>：<br>应用运行环境（Android Runtime）抛出的<strong>未捕获异常信号</strong>（Uncaught Exception）。<br><strong>数据</strong>：<br>1. 异常堆栈信息（StackTrace）。<br>2. 异常类型与描述消息（Exception Message）。</td></tr><tr><td><strong>处理逻辑</strong></td><td>1. <strong>异常拦截</strong>：<br>在应用进程因异常即将终止前，拦截异常信号，挂起当前线程以确保有足够的 CPU 时间片执行数据采集。<br>2. <strong>上下文捕获</strong>：<br>提取崩溃时刻的运行时环境信息，包括但不限于：<br>- 进程名称、线程名称及 ID。<br>- 应用的前后台状态。<br>- 当前 Activity 页面栈信息（用于还原用户路径）。<br>3. <strong>流控策略</strong>：<br>执行本地频次控制策略。检查该进程在设定时间窗口（如 10 分钟）内的崩溃次数，若超限则降级处理（仅记录统计计数，不抓取详细堆栈），防止日志写入引发 IO 阻塞。<br>4. <strong>透传退出</strong>：<br>数据采集完成后，<strong>必须</strong>将异常信号交还给系统默认处理程序，确保应用能够按照 Android 系统规范正常退出，防止应用界面假死或进程僵滞。</td></tr><tr><td><strong>输出</strong></td><td>1. <strong>结构化事件</strong>：生成包含完整上下文信息的 <code>GVM_APP_CRASH</code> 事件对象。<br>2. <strong>本地日志</strong>：在本地持久化存储区保留一份异常日志备份（作为兜底）。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-002-应用无响应-anr-捕获>FR-STAB-002 应用无响应 (ANR) 捕获<a hidden class=anchor aria-hidden=true href=#fr-stab-002-应用无响应-anr-捕获>#</a></h5><table><thead><tr><th>属性</th><th>内容</th></tr></thead><tbody><tr><td><strong>优先级</strong></td><td>P0</td></tr><tr><td><strong>前置条件</strong></td><td>1. 系统层已部署全局监控探针。<br>2. 监控功能的配置开关处于开启状态。</td></tr><tr><td><strong>输入</strong></td><td><strong>触发源</strong>：系统框架层（Framework）识别到的<strong>应用无响应信号</strong>（AppNotResponding）。<br><strong>数据</strong>：<br>1. 目标应用进程标识（PID/ProcessName）。<br>2. 系统生成的<strong>堆栈跟踪文件</strong>（Trace File，通常位于 <code>/data/anr/</code> 目录）。</td></tr><tr><td><strong>处理逻辑</strong></td><td>1. <strong>信号识别</strong>：<br>实时接收系统 ActivityManagerService 发出的 ANR 通知。<br>2. <strong>目标过滤</strong>：<br>根据配置白名单判断是否采集该进程，过滤非关注应用的 ANR 事件。<br>3. <strong>堆栈截取</strong>：<br>读取系统生成的 Trace 文件，根据目标 PID <strong>精准截取</strong>该进程及其子线程的堆栈片段（需剔除文件中的其他无关进程数据，以减少数据体积）。<br>4. <strong>快照关联</strong>：<br>获取 ANR 发生时刻的系统负载信息（Load Avg / CPU Usage / IO Wait）并与堆栈信息打包。<br>5. <strong>流控策略</strong>：<br>执行本地频次控制策略。检查该进程在设定时间窗口（如 10 分钟）内的 ANR 次数，若超限则仅记录计数，不再执行堆栈截取操作。</td></tr><tr><td><strong>输出</strong></td><td>1. <strong>结构化事件</strong>：生成包含 Trace 附件索引（Reference）的 <code>GVM_APP_ANR</code> 事件对象。<br>2. <strong>本地日志</strong>：在本地持久化存储区生成关联的证据包（包含截取的 Trace 片段与系统负载快照）。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-003-应用-native-库崩溃-app-jni-crash-捕获>FR-STAB-003 应用 Native 库崩溃 (App JNI Crash) 捕获<a hidden class=anchor aria-hidden=true href=#fr-stab-003-应用-native-库崩溃-app-jni-crash-捕获>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 系统层已部署全局监控探针（Native Daemon）。<br>2. 监控功能的配置开关处于开启状态。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>应用进程（APP）加载的 JNI 动态库触发致命信号（SIGSEGV/SIGABRT）。<br><strong>数据</strong>：<br>1. 系统生成的 Tombstone 崩溃文件（通常位于 <code>/data/tombstones/</code>）。<br>2. 进程退出信号（Signal Code）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>监听与解析</strong>：<br>实时监听系统 Tombstone 文件的生成事件，读取文件头部信息。<br>2. <strong>身份识别</strong>：<br>检查崩溃进程的 UID 或进程名称。若属于<strong>非系统核心进程</strong>（即普通 App），则执行应用级采集逻辑；若为系统服务则忽略（交由系统框架监控处理）。<br>3. <strong>指纹去重</strong>：<br>基于“应用名称 + 崩溃堆栈关键帧”生成唯一指纹，在端侧聚合重复的崩溃事件，防止日志风暴。<br>4. <strong>事件生成</strong>：<br>将非结构化的 Tombstone 数据转换为标准化的事件对象。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>GVM_APP_NATIVE_CRASH</code> 事件对象。<br>2. <strong>本地日志</strong>：建立事件 ID 与原始 Tombstone 文件的索引关联。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-004-应用-oom-app-oom-事件监控>FR-STAB-004 应用 OOM (App OOM) 事件监控<a hidden class=anchor aria-hidden=true href=#fr-stab-004-应用-oom-app-oom-事件监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 系统层已部署全局监控探针。<br>2. 监控功能的配置开关处于开启状态。<br>3. 具备获取应用进程退出详细原因的能力（如 ApplicationExitInfo 或类似机制）。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>应用进程<strong>意外终止信号</strong>。<br><strong>数据</strong>：<br>1. 进程退出原因描述（Exit Reason，需区分系统回收/异常崩溃）。<br>2. 进程终止前的内存使用统计数据（如 PSS/RSS/VSS）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>原因甄别</strong>：<br>在进程退出后，识别退出原因。准确区分是<strong>系统低内存查杀 (LMK)</strong>（通常表现为 <code>REASON_LOW_MEMORY</code>）还是<strong>Java 堆内存耗尽</strong>（通常表现为 <code>OutOfMemoryError</code> 导致的 Crash）引发的异常。<br>2. <strong>内存快照回溯</strong>：<br>尝试关联该进程在终止前最近一次采集的内存统计数据（如 PSS/RSS），以辅助判断是否存在内存泄漏。<br>3. <strong>风暴抑制</strong>：<br>针对前台应用因 OOM 导致的反复重启进行检测。若同一应用在短时间内（如 5 分钟）连续触发 OOM，则实施指数退避策略，减少上报频次。<br>4. <strong>事件生成</strong>：<br>组装 OOM 事件负载，标记明确的 OOM 类型（System LMK / Java OOM）。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含内存快照信息的 <code>GVM_APP_OOM</code> 事件对象。<br>2. <strong>本地日志</strong>：记录关联的系统内存水位信息（MemInfo）。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h4 id=系统框架稳定性监控>系统框架稳定性监控<a hidden class=anchor aria-hidden=true href=#系统框架稳定性监控>#</a></h4><h5 id=fr-stab-005-systemserver-watchdog-死锁-监控>FR-STAB-005 SystemServer Watchdog (死锁) 监控<a hidden class=anchor aria-hidden=true href=#fr-stab-005-systemserver-watchdog-死锁-监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针已植入系统看门狗（Watchdog）模块或具备监听能力。<br>2. 监控功能的配置开关处于开启状态。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>系统关键锁或核心线程（如 UI Thread, IoThread）<strong>等待超时信号</strong>（通常阈值为 60秒）。<br><strong>数据</strong>：<br>1. 阻塞线程的完整堆栈信息（Stack Traces）。<br>2. 持锁状态与锁竞争信息（Lock Contention）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>重启前拦截</strong>：<br>在系统触发看门狗复位（Soft Reboot / Restart）流程前，优先执行监控逻辑，确保有短暂的时间窗口进行数据转存。<br>2. <strong>现场固化</strong>：<br>立即将当前的系统全量线程堆栈（Traces.txt）复制或转存至持久化存储区，<strong>防止系统重启过程清理现场文件</strong>，导致关键证据丢失。<br>3. <strong>异常标记</strong>：<br>在磁盘特定位置写入“非正常重启”标志位（Flag），以便系统下次启动时进行归因统计，区分正常关机与异常重启。<br>4. <strong>事件上报</strong>：<br>尝试通过 Native 通道（因为 Java 层可能已挂死）发送死锁事件。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含死锁堆栈索引的 <code>GVM_SYS_WATCHDOG</code> 事件对象。<br>2. <strong>本地日志</strong>：在持久化目录保留死锁现场的 Trace 文件。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-006-android-系统systemserver异常重启监控>FR-STAB-006 Android 系统(SystemServer)异常重启监控<a hidden class=anchor aria-hidden=true href=#fr-stab-006-android-系统systemserver异常重启监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 系统完成启动初始化流程（Boot Completed）。<br>2. 具备读取系统启动属性（Boot Reason）及持久化存储的权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>系统<strong>启动完成广播</strong> (Boot Completed) 或同等时机的初始化信号。<br><strong>数据</strong>：<br>1. 系统启动原因属性（如 <code>sys.boot.reason</code> 或 <code>ro.boot.bootreason</code>）。<br>2. 持久化存储中的<strong>历史状态标记</strong>（包含上一次启动时间戳、Watchdog/Crash 遗留的异常标志位）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>原因推断</strong>：<br>对比本次启动原因与上一次运行状态进行逻辑仲裁：<br>- <strong>已知异常</strong>：若存在 Watchdog 或 Core Crash 遗留的标记，判定为对应的系统级故障重启。<br>- <strong>内核恐慌</strong>：若启动属性标识为 Kernel Panic 或 WDT（硬件看门狗），判定为内核级重启。<br>- <strong>正常重启</strong>：若标识为用户主动关机、OTA 升级或常规电源管理操作，判定为正常重启。<br>- <strong>掉电/未知</strong>：若无任何异常标记且非正常重启，判定为异常掉电或未知原因重启。<br>2. <strong>时长计算</strong>：<br>基于上一次记录的启动时间戳，计算上一次系统正常运行的时长（Uptime），用于评估系统平均无故障时间（MTBF）。<br>3. <strong>状态重置</strong>：<br>分析完成后，清除历史异常标记，更新本次启动时间戳，为下一次监控周期做准备。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含重启原因分类（Category）及运行时长（Duration）的 <code>GVM_SYS_RESTART</code> 事件对象。<br>2. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-007-全路径系统重启归因监控>FR-STAB-007 全路径系统重启归因监控<a hidden class=anchor aria-hidden=true href=#fr-stab-007-全路径系统重启归因监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>建立跨越 Host、GVM、Kernel、Framework 四层架构的重启感知能力，提供统一的重启归因数据源。</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控服务具备获取 Hypervisor 状态及 SOC 硬件复位标志的权限。<br>2. 已集成 <strong>FR-STAB-006</strong> 的 Android 内部重启判定结果。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>全路径层次判定</strong>：<br>系统启动后，根据硬件寄存器、Bootloader 传参及 Android 属性进行综合仲裁，确定重启发生的最高级别：<br>- <strong>Linux Host层</strong>：检测到 SOC 硬件复位或宿主机 Kernel Panic，判定为 Host 重启。<br>- <strong>GVM层</strong>：Host 未重启，但虚拟机管理器（Hypervisor）强制拉起 Android 域，判定为 GVM 重启。<br>- <strong>Android Kernel层</strong>：基于 <strong>FR-STAB-006</strong> 判定，若为内核崩溃且 GVM 未重启，归类于此层。<br>- <strong>Android Framework层</strong>：仅 SystemServer 进程重启，底层 Linux 内核连续运行，归类于此层。<br>2. <strong>异常分类同步</strong>：<br>- 读取 <code>sys.boot.reason.last</code> 等关键属性，识别是 [异常:死锁/崩溃/掉电] 还是 [正常:用户重启/OTA]。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_SYSTEM_REBOOT_EVENT</code><br>- <strong>boot_reason</strong>: 原始启动原因字符串<br>- <strong>is_unexpected</strong>: 布尔值，用于云端统计严重故障率。<br>2. 关联日志路径及 ID 注册参考《Polaris 1.0 全局事件ID与注册表规范》。</td></tr></tbody></table><h5 id=fr-stab-008-核心服务崩溃监控>FR-STAB-008 核心服务崩溃监控<a hidden class=anchor aria-hidden=true href=#fr-stab-008-核心服务崩溃监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控进程具备监听系统服务管理器（ServiceManager）或 init 进程状态的能力。<br>2. 核心进程白名单配置已加载。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. Native 守护进程崩溃产生的 Tombstone 文件。<br>2. ServiceManager 发出的 <code>DeathRecipient</code> 通知。<br>3. init 进程发出的 <code>SIGCHLD</code> 信号。<br><strong>数据</strong>：<br>1. 崩溃进程名称（Process Name）及 PID。<br>2. 进程退出状态码或终止信号。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>核心识别</strong>：<br>匹配崩溃进程名称是否在<strong>核心白名单</strong>中（如 <code>surfaceflinger</code>, <code>audioserver</code>, <code>netd</code>, <code>lmkd</code>）。若不在白名单，则视为普通 Native Crash 处理（参考 FR-STAB-003）。<br>2. <strong>多源仲裁</strong>：<br>优先使用 Tombstone 信息（包含详细堆栈），若未生成 Tombstone（如被系统强杀或 Watchdog 处决），则使用 ServiceManager 通知作为补充来源。<br>3. <strong>等级判定</strong>：<br>根据服务重要性标记故障等级（例如 SurfaceFlinger 崩溃标记为“致命”，会导致屏幕黑屏或系统软重启）。<br>4. <strong>事件生成</strong>：<br>组装核心服务崩溃事件，记录服务名称、崩溃时间及退出原因。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含服务名及影响等级的 <code>GVM_CORE_CRASH</code> 事件对象。<br>2. <strong>本地日志</strong>：关联该时间点附近的系统日志（Logcat）与崩溃堆栈。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-009-系统低内存-lmk-事件监控>FR-STAB-009 系统低内存 (LMK) 事件监控<a hidden class=anchor aria-hidden=true href=#fr-stab-009-系统低内存-lmk-事件监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 系统启用 Low Memory Killer 机制（如 Userspace LMKD）。<br>2. 监控组件具备接收系统内存管理模块通知的权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>系统内存管理守护进程（lmkd）执行的<strong>进程查杀动作</strong>。<br><strong>数据</strong>：<br>1. 目标进程信息（PID、UID、Process Name）。<br>2. 查杀时的决策依据（OOM Score Adj）。<br>3. 触发查杀时的系统内存压力状态（Memory Pressure State / PSI）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>动作捕获</strong>：<br>实时感知 LMK 的查杀行为。<strong>推荐方案</strong>：采用源码插桩（Instrumentation）方式，在 <code>lmkd</code> 执行 kill 操作的原子逻辑处植入通知钩子，以获取零延迟、高精度的上下文信息；（备选方案：监听 EventLog 中的 <code>lmk_kill</code> 标签）。<br>2. <strong>水位快照</strong>：<br>同步记录系统当前的内存水位详情（MemTotal, MemFree, SwapUsed, Cached），用于后续分析是物理内存耗尽还是虚拟内存（Swap）耗尽。<br>3. <strong>聚合去噪</strong>：<br>执行时间窗口聚合策略。由于内存压力常导致短时间内连续查杀多个进程，需将同一压力波峰内（如 1 秒）的一组查杀事件聚合，避免产生告警风暴。<br>4. <strong>严重性判定</strong>：<br>识别被杀进程的类型。若被杀进程为前台可见应用或关键服务，标记为“高影响”事件。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含被杀进程列表及内存水位的 <code>GVM_SYS_LMK</code> 事件对象。<br>2. <strong>本地日志</strong>：保留查杀时刻的 <code>meminfo</code> 快照。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-010-binder-通信异常监控>FR-STAB-010 Binder 通信异常监控<a hidden class=anchor aria-hidden=true href=#fr-stab-010-binder-通信异常监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针具备访问内核 Binder 驱动节点或 Hook <code>libbinder</code> 的能力。<br>2. 监控配置已定义 Binder 线程池水位的告警阈值。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. Binder 驱动层的<strong>事务失败信号</strong>（如 <code>BR_FAILED_REPLY</code>, <code>BR_DEAD_REPLY</code>）。<br>2. 进程 Binder 线程池的<strong>资源耗尽状态</strong>。<br><strong>数据</strong>：<br>1. 通信双方身份（Caller PID/UID, Callee PID/UID）。<br>2. 接口描述符（Interface Descriptor）或事务代码（Transaction Code）。<br>3. 失败原因（如 <code>TransactionTooLarge</code>, <code>DeadObject</code>, <code>Timeout</code>）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>异常捕获</strong>：<br>监测 IPC 通信链路健康度。<strong>推荐方案</strong>：在 Native <code>libbinder</code> 层进行插桩，拦截 <code>IPCThreadState</code> 中的错误返回码，从而在第一现场捕获异常。<br>2. <strong>资源枯竭识别 (Starvation)</strong>：<br>周期性或事件驱动地检查关键进程的 Binder 线程池状态。若活跃线程数达到上限（默认 16）且仍有请求积压，判定为 <strong>Binder 线程耗尽</strong>。<br>3. <strong>大负载识别</strong>：<br>识别 <code>TransactionTooLargeException</code>，记录传输过大数据的接口名称，辅助排查跨进程传输大图或大列表导致的性能问题。<br>4. <strong>链路还原</strong>：<br>在异常发生时，自动解析并记录“谁调用谁”（Client -> Server），明确责任方。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含通信双方及错误类型的 <code>GVM_BINDER_ERROR</code> 事件对象。<br>2. <strong>本地日志</strong>：记录 <code>/sys/kernel/debug/binder/transaction_log</code> (若可用) 或相关 Logcat 片段。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-011-文件句柄-fd-泄漏监控>FR-STAB-011 文件句柄 (FD) 泄漏监控<a hidden class=anchor aria-hidden=true href=#fr-stab-011-文件句柄-fd-泄漏监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针具备读取 <code>/proc/[pid]/fd</code> 目录或执行 <code>lsof</code> 类指令的权限。<br>2. 针对不同类型的进程（System/App）配置了相应的 FD 数量告警阈值。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. <strong>周期性采样</strong>：定时检查系统内进程的资源占用情况。<br>2. <strong>被动触发</strong>：捕获到系统日志中抛出的 <code>EMFILE</code> (&ldquo;Too many open files&rdquo;) 错误信号。<br><strong>Data</strong>：<br>1. 目标进程当前打开的文件句柄总数。<br>2. 具体的句柄指向路径（Symlinks in <code>/proc/pid/fd/</code>）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>水位监测</strong>：<br>对关键进程进行周期性（如每 5 分钟）的 FD 数量扫描。对比系统设定的软限制（Soft Limit）与硬限制（Hard Limit）。<br>2. <strong>超限识别</strong>：<br>当某进程 FD 数量超过预警阈值（例如 > 800 或占比 > 80%）时，判定为存在泄漏风险。<br>3. <strong>现场快照</strong>：<br>在检测到超限瞬间，遍历该进程的 <code>/proc/[pid]/fd/</code> 目录，生成句柄分布快照。<strong>智能分类</strong>：统计不同类型句柄的占比（如 Socket, Anon_inode, Regular File），快速定位是网络连接泄漏还是文件未关闭。<br>4. <strong>趋势分析</strong>：<br>结合历史数据，识别 FD 数量是否呈“持续上升且不回落”的阶梯状趋势，以排除正常的业务并发高峰。<br></td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含 FD 总数及分类统计的 <code>GVM_FD_LEAK</code> 事件对象。<br>2. <strong>本地日志</strong>：保留 top N 的句柄路径列表（Evidence List）。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h4 id=异构运行环境监控>异构运行环境监控<a hidden class=anchor aria-hidden=true href=#异构运行环境监控>#</a></h4><h5 id=fr-stab-012-linux-host-pvm-重启与状态监控>FR-STAB-012 Linux Host (PVM) 重启与状态监控<a hidden class=anchor aria-hidden=true href=#fr-stab-012-linux-host-pvm-重启与状态监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. PVM (Linux Host) 侧已部署 Host Daemon 并具备读取系统启动日志（如 <code>/sys/fs/pstore</code> 或 systemd journal）的权限。<br>2. PVM 与 GVM 之间的跨域通信通道在启动后能够建立连接。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. <strong>PVM 启动完成</strong>：Host Daemon 随系统启动初始化。<br>2. <strong>连接建立</strong>：PVM 与 GVM 建立首次握手成功。<br><strong>数据</strong>：<br>1. PVM 本次启动原因（Boot Reason）。<br>2. 历史持久化日志（上一周期的 Kernel Panic Log 或 Watchdog 记录）。<br>3. 实时心跳报文。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>启动回溯（Post-Boot Analysis）</strong>：<br>Host Daemon 在 PVM 启动初期，检查持久化存储中的上一次关机状态。识别是<strong>正常关机</strong>、<strong>掉电</strong>还是<strong>异常重启</strong>（如 Kernel Panic 导致的 WDT Reset）。<br>2. <strong>事件缓存</strong>：<br>若判定为异常重启，Host Daemon 生成重启事件对象并暂存于本地内存或磁盘队列中，等待跨域通道就绪。<br>3. <strong>延迟同步（Delayed Sync）</strong>：<br>当监测到 GVM (Android) 侧的 <code>PolarisNativeDaemon</code> 上线并建立连接后，立即将缓存的“上一次重启事件”发送给 GVM。<br>4. <strong>运行时状态监测</strong>：<br>在连接建立后的运行期间，Host Daemon 周期性向 GVM 发送心跳包与健康度状态（如 Systemd Failed Units），供 GVM 侧记录实时趋势。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>PVM_SYS_RESTART</code>（携带重启原因的历史事件）或 <code>PVM_STATUS_REPORT</code>（运行时状态）。<br>2. <strong>本地日志</strong>：在 PVM 侧保留原始的重启原因分析日志。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-013-mcu-故障码与心跳监控>FR-STAB-013 MCU 故障码与心跳监控<a hidden class=anchor aria-hidden=true href=#fr-stab-013-mcu-故障码与心跳监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 硬件抽象层（HAL）或驱动层已完成 MCU 通信协议适配。<br>2. 监控守护进程（Native Daemon）具备读取 MCU 状态接口的权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. MCU 周期性上报的<strong>状态报文</strong>（Status Message）。<br>2. 硬件中断或底层驱动回调。<br><strong>数据</strong>：<br>1. 存活心跳计数器（Rolling Counter）。<br>2. 硬件诊断故障码（DTC - Diagnostic Trouble Code）。<br>3. 外设关键状态字（如电源模式、复位原因）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>存活判定</strong>：<br>通过监测心跳计数器的连续性和变化率来判断 MCU 状态。若计数器在设定时间窗口（<strong>TBD</strong>）内停止跳变或非法跳变，判定为 <strong>MCU 挂死或通信中断</strong>。<br>2. <strong>协议映射</strong>：<br>建立 MCU 原始故障码与平台统一错误定义的映射表。将厂商特定的十六进制 DTC（如 <code>0x1234</code>）转换为可读的平台标准错误码（如 <code>ERR_MCU_PMIC_FAIL</code>）。<br>3. <strong>信号去抖</strong>：<br>对偶发的故障信号进行软件滤波（De-bounce）。只有在连续 N 帧报文中确认同一故障码，或故障持续时长超过阈值时，才确认为有效故障，防止因总线干扰导致的误报。<br>4. <strong>复位检测</strong>：<br>监测 MCU 的复位原因寄存器。若发现异常复位标识（如 WDT Reset），记录异常复位事件。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>MCU_HEARTBEAT_LOST</code>（失联）或 <code>MCU_HARDWARE_FAULT</code>（硬件故障）事件对象。<br>2. <strong>本地日志</strong>：记录原始报文数据（Raw Data）以便后续校验。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-014-异构关键进程-pvm-critical-process-稳定性监控>FR-STAB-014 异构关键进程 (PVM Critical Process) 稳定性监控<a hidden class=anchor aria-hidden=true href=#fr-stab-014-异构关键进程-pvm-critical-process-稳定性监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 目标关键进程（如 Audio Server, Display Composer, GSL HAL 等）已在 Host 侧启动。<br>2. Polaris Host Daemon 具备对目标进程状态的查询或监听权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. 操作系统（Linux Host）发出的<strong>进程终止信号</strong>（如 SIGCHLD）。<br>2. 服务管理框架（如 Systemd）抛出的<strong>服务状态变更通知</strong>（Service Unit Status Change）。<br>3. 目标进程输出到标准错误流（Stderr）的<strong>致命错误日志</strong>。<br><strong>数据</strong>：<br>1. 进程标识（PID, Process Name, Unit Name）。<br>2. 退出状态码（Exit Code）或终止信号（Signal）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>通用监听</strong>：<br>采用非侵入式手段实时感知关键进程的生命周期。针对受 Systemd 托管的服务，订阅其 D-Bus 状态变更信号；针对独立进程，采用 PID 存活轮询或父进程信号监听机制。<br>2. <strong>状态判定</strong>：<br>- <strong>异常退出</strong>：识别进程非预期的终止（Exit Code != 0）。<br>- <strong>僵死/挂起</strong>：若具备条件，监测进程是否长时间处于 D 状态（Uninterruptible Sleep）或对心跳接口无响应。<br>3. <strong>抖动抑制 (Flapping Detection)</strong>：<br>针对具有“自动重启”机制的关键服务，在设定时间窗口内（如 10秒）若检测到多次反复重启，应将其聚合为单次“服务抖动”事件上报，防止告警风暴。<br>4. <strong>现场记录</strong>：<br>在进程崩溃瞬间，尝试捕获其最后输出的标准错误日志（Stderr）或 Journalctl 片段，作为归因线索。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成包含进程名、退出码及故障频次的 <code>PVM_PROCESS_CRASH</code> 事件对象。<br>2. <strong>本地日志</strong>：Host 侧保留相关的系统日志片段。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-stab-015-温度监控>FR-STAB-015 温度监控<a hidden class=anchor aria-hidden=true href=#fr-stab-015-温度监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 系统底层具备热管理子系统（Thermal HAL / Thermal Daemon）。<br>2. 监控探针具备读取 <code>/sys/class/thermal</code> 节点或订阅 <code>IThermalService</code> 回调的权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>1. <strong>被动接收</strong>：Thermal HAL 上报的热状态变更回调（如 <code>onStatusChanged</code>）。<br>2. <strong>主动采样</strong>：周期性读取关键热区（Thermal Zone）的温度传感器数值。<br><strong>数据</strong>：<br>1. 热区名称（Zone Name, 如 <code>cpu</code>, <code>gpu</code>, <code>battery</code>, <code>soc</code>）。<br>2. 当前温度值（Temperature in m°C）。<br>3. 热状态等级（Thermal Status: NONE, LIGHT, MODERATE, SEVERE, CRITICAL, SHUTDOWN）。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>状态监听</strong>：<br>实时订阅系统热管理服务的状态变更通知。一旦热状态跨越阈值（例如从 <code>NONE</code> 变为 <code>SEVERE</code>），立即触发记录逻辑。<br>2. <strong>降频关联</strong>：<br>当检测到温度过高触发温控策略（Throttling）时，尝试关联当前的 CPU/GPU 频率限制状态，以解释可能伴随发生的卡顿或掉帧现象（辅助性能分析）。<br>3. <strong>危急保护记录</strong>：<br>当收到 <code>SHUTDOWN</code> 级别的热信号时，视为“过热关机前兆”，必须以最高优先级将当前温度快照写入持久化存储，作为系统异常重启（FR-STAB-006）的直接归因证据。<br>4. <strong>趋势采样</strong>：<br>在非危急状态下，按低频（如每 5 分钟）采样 SoC 核心温度，用于绘制温度变化趋势图。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>GVM_SYS_THERMAL_EVENT</code>（包含热区名称、温度值及温控等级）。<br>2. <strong>本地日志</strong>：保留过热时刻的 <code>thermalservice</code> 状态或相关节点快照。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h3 id=性能与资源度量能力-1>性能与资源度量能力<a hidden class=anchor aria-hidden=true href=#性能与资源度量能力-1>#</a></h3><h4 id=交互体验量化>交互体验量化<a hidden class=anchor aria-hidden=true href=#交互体验量化>#</a></h4><h5 id=fr-perf-001-应用启动耗时-app-launch-time-监控>FR-PERF-001 应用启动耗时 (App Launch Time) 监控<a hidden class=anchor aria-hidden=true href=#fr-perf-001-应用启动耗时-app-launch-time-监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针已 Hook 或插桩至 AMS 启动流程及 Activity 生命周期关键回调。<br>2. 目标应用白名单已配置。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>应用进程创建及 Activity 界面绘制完成信号。<br><strong>数据</strong>：<br>1. <strong>启动类型</strong>：冷启动 (Cold)、热启动 (Hot/Warm)。<br>2. <strong>关键时间戳</strong>（仅采集内存中的系统时钟 <code>SystemClock.uptimeMillis()</code>）：<br>- <code>T0</code>: Intent 发送/用户点击图标时刻。<br>- <code>T1</code>: 进程创建成功 (Process Forked)。<br>- <code>T2</code>: 应用页面初始化 (<code>Activity.onCreate/onStart</code>)。<br>- <code>T3</code>: 首帧绘制完成 (ReportFullyDrawn / Window Focus)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>零干扰采集</strong>：<br>在启动的关键路径上，<strong>严禁</strong>执行任何文件读取、复杂的字符串处理或 IPC 调用。仅在内存中记录长整型时间戳，确保监控逻辑对应用启动速度的影响趋近于零。<br>2. <strong>端到端计算</strong>：<br>启动结束后，异步计算总耗时及分段耗时：<br>- <strong>系统调度耗时 (T1 - T0)</strong>：反映系统资源紧张程度或 Zygote 响应速度。<br>- <strong>应用初始化耗时 (T2 - T1)</strong>：反映 Application 级初始化逻辑耗时。<br>- <strong>页面渲染耗时 (T3 - T2)</strong>：反映 Activity 布局加载与渲染耗时。<br>3. <strong>异常判定与分级上报</strong>：<br>- <strong>正常启动</strong>：总耗时未超过基线。上报事件，但留空诊断字段。<br>- <strong>慢启动</strong>：总耗时超过基线。标记 <code>status=SLOW</code>，并触发关联逻辑。<br>4. <strong>关联归因 (Correlation)</strong>：<br>仅针对“慢启动”事件进行事后时间窗口匹配：<br>- <strong>主线程阻塞</strong>：检索启动期间是否触发了 <strong>FR-PERF-002</strong> (主线程卡顿) 事件。<br>- <strong>资源竞争</strong>：检索启动期间系统整体 LoadAvg 或 IO 负载是否处于高位 (由 <strong>FR-PERF-004</strong> 采集)。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>GVM_APP_LAUNCH</code> 事件。<br>- <strong>必选字段</strong>：应用名、启动类型、总耗时、分段耗时、状态(Normal/Slow)。<br>- <strong>可选字段</strong>（仅 Slow 状态填充）：关联的异常事件 ID (Ref_Event_ID)、Trace 索引。</td></tr></tbody></table><h5 id=fr-perf-002-主线程响应与健康度监控>FR-PERF-002 主线程响应与健康度监控<a hidden class=anchor aria-hidden=true href=#fr-perf-002-主线程响应与健康度监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针已集成至目标应用主线程 Looper。<br>2. 监控配置中定义了卡顿阈值（如 <code>BlockThreshold = 200ms</code>）。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>主线程 Looper 分发消息（Message Dispatch）的开始与结束时刻。<br><strong>数据</strong>：<br>1. <strong>消息执行耗时</strong>：墙钟耗时 (Wall Duration) 与 线程 CPU 耗时 (Thread CPU Duration)。<br>2. <strong>消息签名</strong>：目标 Handler 类名、Runnable 类名或 Message.what 标识。<br>3. <strong>队列状态</strong>：MessageQueue 当前积压的消息数量 (Pending Count)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>双重计时监测</strong>：<br>在消息分发前后打点。<strong>关键逻辑</strong>：同时记录“墙钟时间”和“CPU 时间”。<br>- 若墙钟时间长且 CPU 时间长 $\rightarrow$ <strong>计算密集型卡顿</strong> (算法复杂、大循环)。<br>- 若墙钟时间长但 CPU 时间短 $\rightarrow$ <strong>IO/锁等待型卡顿</strong> (主线程读写磁盘、Binder 阻塞、等锁)。<br>2. <strong>堆栈抓取 (Sample & Dump)</strong>：<br>采用“看门狗”机制。当检测到当前消息执行已超过阈值（如 > 200ms）但在结束前，<strong>主动抓取</strong>主线程瞬时堆栈。避免等消息执行完再抓取导致“现场已过”的问题。<br>3. <strong>拥堵识别 (Congestion)</strong>：<br>即使单条消息未超时，若 MessageQueue 待处理消息数持续超过阈值（如 > 50个），判定为<strong>调度拥堵</strong>。记录拥堵期间的“头部分发者” (Top Senders)。<br>4. <strong>聚合去噪</strong>：<br>对短时间内连续发生的相同堆栈/相同签名的卡顿事件进行聚合，仅上报一次并附带发生次数 (Count)。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：生成 <code>GVM_APP_MAIN_THREAD_BLOCK</code> 事件。<br>- <strong>字段</strong>：应用名、卡顿类型(CPU/Wait)、耗时、消息签名、堆栈摘要(StackHash)。<br>2. <strong>本地日志</strong>：记录完整的卡顿堆栈 (Full Stack Trace) 及当时的消息队列快照。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-perf-003-界面流畅度-ui-jankfps-监控>FR-PERF-003 界面流畅度 (UI Jank/FPS) 监控<a hidden class=anchor aria-hidden=true href=#fr-perf-003-界面流畅度-ui-jankfps-监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控探针已通过 <code>Window.addOnFrameMetricsAvailableListener</code> 注册监听。<br>2. 目标应用处于前台可见状态。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>系统 <code>FrameMetrics</code> 回调。<br><strong>数据</strong>：<br>1. <strong>核心耗时</strong>：<code>TOTAL_DURATION</code>, <code>DEADLINE</code>。<br>2. <strong>归因耗时</strong>：UI 相关 (<code>LAYOUT</code>, <code>DRAW</code>&mldr;), GPU 相关 (<code>GPU_DURATION</code>, <code>SWAP</code>&mldr;)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>生命周期聚合</strong>：<br>以页面会话（onResume -> onPause）为单位进行统计，<strong>不上报单帧数据</strong>。<br>2. <strong>轻量级计算</strong>：<br>- <strong>计数</strong>：统计总帧数 (<code>total</code>)、掉帧数 (<code>jank</code>, Total > Deadline)、冻结帧数 (<code>frozen</code>, Total > 700ms)。<br>- <strong>极值</strong>：记录会话期间的最大帧耗时。<br>- <strong>均值</strong>：累加 UI 耗时与 GPU 耗时，计算平均每帧的 CPU/GPU 开销分布。<br>3. <strong>异常判定</strong>：<br>若掉帧率超过阈值（如 10%）或存在冻结帧，标记 <code>is_laggy=true</code>。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_APP_JANK_STATS</code>。<br>- <strong>字段</strong>：<code>activity_name</code>, <code>total_frames</code>, <code>jank_count</code>, <code>frozen_count</code> (冻结帧数), <code>max_frame_ms</code>, <code>avg_ui_ms</code>, <code>avg_gpu_ms</code>。<br>- <strong>作用</strong>：云端可直接计算出“某页面的平均掉帧率”以及“卡顿主要是因为 UI 逻辑还是 GPU 渲染”。<br>2. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h4 id=资源水位画像>资源水位画像<a hidden class=anchor aria-hidden=true href=#资源水位画像>#</a></h4><h5 id=fr-perf-004-进程-cpu-负载监控>FR-PERF-004 进程 CPU 负载监控<a hidden class=anchor aria-hidden=true href=#fr-perf-004-进程-cpu-负载监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. Android/Host 端监控进程具备 <code>/proc</code> 读取权限。<br>2. 跨域通信通道正常。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>定时器触发（端侧采样周期：30秒）。<br><strong>数据源</strong>：<br>解析 <code>/proc/stat</code> (System) 和 <code>/proc/[pid]/stat</code> (Process)。<strong>严禁</strong>使用 top 命令。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>端侧高频采样</strong>：<br>每 <strong>30秒</strong> 采集一次快照，计算瞬时系统总负载及各进程负载。不立即上报，暂存内存。<br>2. <strong>长周期聚合 (Long-Term Aggregation)</strong>：<br>每 <strong>30分钟</strong> 为一个常规上报周期（包含 60 个采样点）。<br>- <strong>计算均值</strong>：计算该 30 分钟内的系统平均负载。<br>- <strong>锁定峰值</strong>：找出该周期内负载最高的那个时间点（Peak Snapshot），并提取该时刻的 Top 5 进程。<br>3. <strong>异常快速通道 (Immediate Alert)</strong>：<br>在每次采样后进行实时判断。若检测到<strong>系统总负载 > 80% 且持续 2 个采样周期（1分钟）</strong>，则<strong>绕过</strong> 30分钟的聚合等待，<strong>立即触发</strong>一条异常告警事件。<br>4. <strong>跨域打包</strong>：<br>Linux Host 端数据通过跨域通道透传至 Android 端，统一打标 <code>domain</code> (ANDROID/HOST) 后上传。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_PROC_CPU_TOP</code>。<br>- <strong>频率</strong>：30分钟/次 (常规) 或 立即 (异常)。<br>- <strong>字段</strong>：<code>timestamp</code>, <code>sys_load_avg</code> (30min均值), <code>sys_load_peak</code> (峰值), <code>top_processes</code> (峰值时刻的 Top 5 列表)。<br>2. <strong>流量预估</strong>：约 20KB/天/车 (极低)。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-perf-005-进程内存消耗监控>FR-PERF-005 进程内存消耗监控<a hidden class=anchor aria-hidden=true href=#fr-perf-005-进程内存消耗监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. Android/Host 端监控进程具备 <code>/proc</code> 读取权限。<br>2. <code>/log/perf/meminfo/</code> 目录已创建且具备写权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>定时器触发（端侧采样周期：30秒）。<br><strong>数据源</strong>：<br>1. <strong>全局水位</strong>：读取 <code>/proc/meminfo</code> (关注 <code>MemAvailable</code>)。<br>2. <strong>进程水位</strong>：读取 <code>/proc/[pid]/statm</code> (关注 <code>RSS</code> - Resident Set Size)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>端侧高频采样</strong>：<br>每 <strong>30秒</strong> 读取一次全局内存及所有活跃进程的 RSS。<br>2. <strong>长周期聚合 (30min Window)</strong>：<br>每 <strong>30分钟</strong> 为一个上报周期。<br>- <strong>系统趋势</strong>：记录周期内 <code>MemAvailable</code> 的最低点。<br>- <strong>Top 进程</strong>：找出周期内 <code>RSS</code> 峰值最高的 Top 5 进程。<br>- <strong>异常膨胀识别</strong>：计算进程在周期首尾的差值 ($\Delta RSS = RSS_{End} - RSS_{Start}$)。若净增量 > <strong>200MB</strong> (可配置)，标记为异常。<br>3. <strong>异常快速通道 (Immediate Alert)</strong>：<br>- <strong>系统持续高压</strong>：若 <code>MemAvailable / MemTotal &lt;</code> <strong>20%</strong> 且持续时长 > <strong>3分钟</strong>，判定为内存紧张，<strong>立即上报</strong>。<br>- <strong>进程大内存</strong>：若某非白名单进程 <code>RSS > 1GB</code> (或配置阈值) 且持续 > <strong>3分钟</strong>，判定为异常占用，<strong>立即上报</strong>。<br>- <strong>现场固化</strong>：触发上述任一条件时，立即执行 <code>dumpsys meminfo [pid]</code> 并写入日志文件。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>本地日志 (Local Dump)</strong>：<br>- <strong>路径</strong>：<code>/log/perf/meminfo/</code>。<br>- <strong>文件名格式</strong>：<code>{process_name}_{pid}_{timestamp}.txt</code> (注：进程名中的 <code>:</code> 需替换为 <code>_</code>，如 <code>com_android_systemui</code>)。<br>- <strong>内容</strong>：完整的 <code>dumpsys meminfo</code> 文本输出 (包含 Java Heap, Native Heap, Graphics 等 PSS 详情)。<br>2. <strong>结构化事件</strong>：<code>GVM_PROC_MEM_TOP</code>。<br>- <strong>频率</strong>：30分钟/次 (常规) 或 立即 (异常)。<br>- <strong>字段</strong>：<code>timestamp</code>, <code>sys_mem_available_min</code> (MB), <code>top_processes</code> (List: name, pid, rss_peak_mb, rss_growth_mb)。<br>- <strong>关联字段</strong>：<code>logf</code> (值为上述生成的日志文件名。仅在触发快速通道时填充，常规上报为空)。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-perf-006-进程磁盘-io-吞吐量与系统压力监控>FR-PERF-006 进程磁盘 I/O 吞吐量与系统压力监控<a hidden class=anchor aria-hidden=true href=#fr-perf-006-进程磁盘-io-吞吐量与系统压力监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. Android/Host 端监控进程具备读取 <code>/proc/[pid]/io</code> 及 <code>/proc/stat</code> 的权限。<br>2. 内核配置开启 <code>TASK_IO_ACCOUNTING</code> 选项。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>定时器触发（端侧采样周期：30秒）。<br><strong>数据源</strong>：<br>1. <strong>进程 I/O</strong>：读取 <code>/proc/[pid]/io</code> (<code>read_bytes</code>, <code>write_bytes</code>)。<br>2. <strong>系统 I/O 压力</strong>：读取 <code>/proc/stat</code> (用于计算全局 <code>iowait</code>)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>端侧高频采样</strong>：<br>每 <strong>30秒</strong> 遍历活跃进程计算 I/O 增量，同时记录系统 <code>iowait</code> 快照。<br>2. <strong>长周期聚合 (30min Window)</strong>：<br>每 <strong>30分钟</strong> 为一个上报周期。<br>- <strong>系统压力</strong>：利用周期初和周期末的 <code>/proc/stat</code> 快照，计算该 30分钟内的<strong>平均 <code>iowait</code></strong>。<br>- <strong>累计总量</strong>：计算周期内所有进程的总写入量。<br>- <strong>Top 进程</strong>：找出周期内 <strong>I/O 吞吐量</strong> 最高的 Top 5 进程。<br>3. <strong>异常快速通道 (Immediate Alert)</strong>：<br>- <strong>高负载写入</strong>：若某进程写速 > <strong>10MB/s</strong> (可配置) 且持续 > <strong>1分钟</strong>，判定为“异常写入 (Abnormal Write)”，<strong>立即上报</strong>。<br>- <strong>高负载读取</strong>：若某进程读速 > <strong>50MB/s</strong> (可配置) 且持续 > <strong>1分钟</strong>，判定为读取密集，可能导致界面卡顿 (ANR)，<strong>立即上报</strong>。<br>- <strong>I/O 阻塞</strong>：若系统全局 <code>iowait</code> > <strong>40%</strong> (可配置) 且持续 > <strong>1分钟</strong>，判定为磁盘性能瓶颈，<strong>立即上报</strong>。<br>4. <strong>跨域打包</strong>：<br>Linux Host 端数据透传至 Android 端合并上报。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_PROC_IO_TOP</code>。<br>- <strong>频率</strong>：30分钟/次 (常规) 或 立即 (异常)。<br>- <strong>字段</strong>：<code>timestamp</code>, <code>sys_iow_avg</code> (平均IO等待率 %), <code>total_write_mb</code> (周期总写量), <code>top_processes</code> (List: name, pid, read_rate_mbps, write_rate_mbps)。<br>2. <strong>本地日志</strong>：触发异常时，记录 <code>/proc/[pid]/io</code> 快照及 <code>/proc/stat</code> 详情。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h4 id=系统级性能监控>系统级性能监控<a hidden class=anchor aria-hidden=true href=#系统级性能监控>#</a></h4><h5 id=fr-perf-007-系统冷启动耗时-cold-boot-监控>FR-PERF-007 系统冷启动耗时 (Cold Boot) 监控<a hidden class=anchor aria-hidden=true href=#fr-perf-007-系统冷启动耗时-cold-boot-监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P2</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. <strong>Host 端</strong>: 集成 <code>bootchart</code>，并将解析出的内核/用户态启动耗时传递给 Android。<br>2. <strong>Android 端</strong>: 能够接收 Host 数据，且能访问本地日志目录。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>接收到 <code>BOOT_COMPLETED</code> 广播。<br><strong>数据源</strong>：<br>1. <strong>Linux</strong>: Host 侧 <code>bootchart</code> 统计时长。<br>2. <strong>Android</strong>: <code>SystemClock.uptimeMillis()</code>。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>计算总耗时</strong>：<br>$Total = T_{Linux} + T_{Android}$。<br>2. <strong>本地落盘</strong>：<br>无论是否超时，将本次启动的耗时数据记录在本地数据库/日志中，并保留 Host 传输过来的 <code>bootchart.tgz</code> 文件。<br>3. <strong>异常上报</strong>：<br>若 $Total >$ <strong>30s</strong>，触发云端上报，并携带 bootchart 日志路径。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_BOOT_COLD</code><br>- <strong>字段 (极简)</strong>：<br>- <code>dur</code> (Int): 总耗时 (ms)<br>- <code>lin</code> (Int): Linux Host 耗时 (ms)<br>- <code>logf</code> (Str): 关联的日志文件名 (如 <code>bootchart_20251020.tgz</code>)<br>2. <strong>本地日志</strong>：保留 Linux 生成的 bootchart 压缩包。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-perf-008-系统热唤醒耗时-str-resume-监控>FR-PERF-008 系统热唤醒耗时 (STR Resume) 监控<a hidden class=anchor aria-hidden=true href=#fr-perf-008-系统热唤醒耗时-str-resume-监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P2</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控服务有权限读取 Kernel 唤醒中断时间戳。<br>2. 监控服务能捕获 Android 界面首帧绘制信号 (First Frame)。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>STR 唤醒流程结束，屏幕点亮。<br><strong>数据源</strong>：<br>1. <strong>起点</strong>: Kernel 接收到唤醒中断 (IRQ) 的时间戳。<br>2. <strong>终点</strong>: Android 界面第一帧绘制完成 (SurfaceFlinger/WindowManager) 的时间戳。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>计算耗时</strong>：<br>$Duration = T_{FirstFrame} - T_{WakeupIRQ}$。<br>2. <strong>异常上报</strong>：<br>若 $Duration >$ <strong>5s</strong>，判定为唤醒过慢，触发云端上报。<br>3. <strong>现场固化</strong>：<br>仅在超时时，抓取当前的 Kernel Log (<code>dmesg</code>) 和 Android Log (<code>logcat -b events</code>) 片段写入文件，用于分析是驱动阻塞还是应用渲染慢。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_BOOT_STR</code><br>- <strong>字段 (极简)</strong>：<br>- <code>dur</code> (Int): 唤醒总耗时 (ms)<br>- <code>logf</code> (Str): 关联的日志文件名 (如 <code>str_trace_20251020.txt</code>)<br>2. <strong>本地日志</strong>：包含唤醒时间段内的 <code>dmesg</code> 和关键系统日志。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h5 id=fr-perf-009-全局存储健康度与多分区空间监控>FR-PERF-009 全局存储健康度与多分区空间监控<a hidden class=anchor aria-hidden=true href=#fr-perf-009-全局存储健康度与多分区空间监控>#</a></h5><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 监控服务持有需监控的分区白名单配置（如 <code>["/data", "/mnt/camera", "/log"]</code>）及其各自的报警阈值（如 Camera > 95%, Data > 90%）。<br>2. 具备读取 UFS/eMMC 硬件寿命寄存器及执行 <code>statfs</code> 的权限。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>定时器触发（端侧采样周期：<strong>15分钟</strong>）。<br><strong>数据源</strong>：<br>1. <strong>硬件寿命</strong>：UFS/eMMC <code>Life Time Estimation</code> (Type A/B)。<br>2. <strong>分区空间</strong>：遍历白名单中的每个挂载点，使用 <code>statfs</code> (系统调用) 获取 Total/Used/Free 块数量。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>空间写满判定</strong>：<br>- <strong>低频采样</strong>：每 15 分钟检查一次各分区使用率。<br>- <strong>防抖确认</strong>：若发现某分区使用率 > 阈值，<strong>立即在 60秒后进行第二次采样</strong>。<br>- <strong>异常触发</strong>：若两次采样均超标，标记为“写满确诊”，<strong>立即上报</strong>异常事件。<br>2. <strong>硬件寿命监测</strong>：<br>每天检查一次。若 Life A/B $\ge$ <strong>0x09</strong> (90%)，标记为“寿命耗尽”，<strong>立即上报</strong>。<br>3. <strong>长周期聚合</strong>：<br>每 <strong>30分钟</strong> (即每 2 个采样周期) 上报一次常规状态。<br>- 打包所有被监控分区的当前使用率快照。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_DISK_STAT</code><br>- <strong>Type字段</strong>：<code>typ</code> (0=Routine, 1=Alert_Full, 2=Alert_EOL)<br>- <strong>公共字段</strong>：<code>la</code>/<code>lb</code> (寿命等级)<br>- <strong>常规上报字段 (typ=0)</strong>：<br><code>parts</code>: <code>[{"n":"data","u":45}, {"n":"camera","u":80}, {"n":"log","u":10}]</code> (n=name, u=usage%)<br>- <strong>异常上报字段 (typ=1)</strong>：<br><code>tgt</code>: <code>"camera"</code> (报警的具体分区名)<br><code>val</code>: <code>98</code> (当前使用率)<br>2. <strong>本地日志</strong>：仅在触发 Alert 时，执行 <code>df -h</code> 和 <code>du -sh *</code> 写入日志文件。<br>3. 结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h3 id=现场还原与协同能力-1>现场还原与协同能力<a hidden class=anchor aria-hidden=true href=#现场还原与协同能力-1>#</a></h3><h4 id=fr-diag-001-标准化事件协议体系>FR-DIAG-001 标准化事件协议体系<a hidden class=anchor aria-hidden=true href=#fr-diag-001-标准化事件协议体系>#</a></h4><p>系统对于上报事件应该具有统一规范的编码，具体编码规则参考 《Polaris 1.0 全局事件ID与注册表规范》。</p><h4 id=fr-diag-002-智能现场快照>FR-DIAG-002 智能现场快照<a hidden class=anchor aria-hidden=true href=#fr-diag-002-智能现场快照>#</a></h4><p>上报事件所产生的日志文件应该按照类型存放在指定的目录，参考 《Polaris 1.0 全局事件ID与注册表规范》。</p><h4 id=fr-diag-003-远程诊断执行>FR-DIAG-003 远程诊断执行<a hidden class=anchor aria-hidden=true href=#fr-diag-003-远程诊断执行>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>在不打扰用户的前提下，通过云端下发安全指令获取更深度的系统运行时信息。</td></tr><tr><td style=text-align:left><strong>前置条件</strong></td><td style=text-align:left>1. 端云建立安全加密指令通道。<br>2. <strong>密钥预置</strong>：端侧 Agent 已预置用于验签的云端公钥。<br>3. <strong>安全状态</strong>：部分交互类指令仅允许在非行车状态下执行。</td></tr><tr><td style=text-align:left><strong>输入</strong></td><td style=text-align:left><strong>触发源</strong>：<br>云端下发的诊断指令包 <code>CMD_EXEC_DIAG</code>。<br><strong>参数</strong>：<br><code>cmd_code</code> (指令码), <code>params</code> (参数列表), <code>request_id</code> (云端生成的唯一追踪ID), <code>timestamp</code>, <code>sign</code> (数字签名)。</td></tr><tr><td style=text-align:left><strong>处理逻辑</strong></td><td style=text-align:left>1. <strong>安全沙箱校验</strong>：<br>- <strong>验签策略</strong>：Agent 使用内置公钥验证 <code>sign</code>，确保指令未被篡改。<br>- <strong>防重放</strong>：校验 <code>timestamp</code> 有效性，并检查 <code>request_id</code> 是否已处理过。<br>- <strong>白名单过滤</strong>：<strong>严禁</strong>直接执行 <code>eval(raw_shell)</code>。仅允许执行预埋的<strong>标准化指令集</strong>。<br>2. <strong>指令执行</strong>：<br>在独立的子进程中执行对应逻辑，设置硬性超时时间（如 5s）。<br>3. <strong>结果截取与落盘</strong>：<br>捕获标准输出 (<code>stdout</code>) 和标准错误 (<code>stderr</code>)。<br>- <strong>数据</strong>：输出内容写入本地文件。<br>- <strong>存储目录</strong>：<code>/log/perf/diag/</code><br>- <strong>命名规则</strong>：<code>diag_{cmd_code}_{request_id}_{timestamp}.txt</code><br>- <strong>返回数据</strong>：Payload 中<strong>仅返回生成的文件名</strong>（如 <code>diag_PING_req123_170982.txt</code>），后续由 VlmAgent 根据文件名及预置路径策略进行拉取。<br>4. <strong>审计记录</strong>：<br>将执行记录写入本地安全审计日志。</td></tr><tr><td style=text-align:left><strong>输出</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：<code>GVM_DIAG_RESULT</code><br>- <strong>字段</strong>：<code>req_id</code> (回传云端下发的ID), <code>code</code> (0=Success, 1=Fail), <code>output</code> (小数据结果内容), <code>logf</code> (大数据结果的文件名)。<br>2. <strong>本地日志</strong>：<code>/log/perf/diag/</code> 下的诊断结果文件。<br>3. 详细结构化事件以及本地日志存储目录参考《Polaris 1.0 全局事件ID与注册表规范》</td></tr></tbody></table><h3 id=数据智能与运营能力-1>数据智能与运营能力<a hidden class=anchor aria-hidden=true href=#数据智能与运营能力-1>#</a></h3><h4 id=fr-data-001-端云数据关联与检索>FR-DATA-001 端云数据关联与检索<a hidden class=anchor aria-hidden=true href=#fr-data-001-端云数据关联与检索>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0 (核心能力)</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>消除人工查找日志的成本。确保研发人员在查看报警详情时，能够直接获取到故障现场的日志文件，实现“所见即所得”。</td></tr><tr><td style=text-align:left><strong>输入数据</strong></td><td style=text-align:left>1. <strong>结构化事件</strong>：包含 <code>evid</code> (事件ID), <code>vin</code>, <code>timestamp</code>, <code>logf</code> (关联的日志文件名) 等字段。<br>2. <strong>日志文件</strong>：端侧 VlmAgent 上传至对象存储的 ZIP/TXT 文件。</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left>1. <strong>自动关联逻辑</strong>：<br>平台需建立自动化索引机制。当接收到结构化事件时，读取其 <code>logf</code> 字段，以此为 Key 在存储桶中查找对应的文件。无论“事件先到”还是“文件先到”，最终必须在界面上实现绑定。<br>2. <strong>生命周期管理</strong>：<br>普通日志文件默认保留 <strong>30天</strong>，高危/严重级别的故障日志保留 <strong>180天</strong>。过期后自动清理以节省存储成本。</td></tr><tr><td style=text-align:left><strong>功能/展示要求</strong></td><td style=text-align:left><strong>Event详情页</strong>：<br>- <strong>日志区域</strong>：在事件详情页必须包含显著的“关联日志附件”板块。<br>- <strong>状态展示</strong>：<br>若文件尚未上传完毕，显示状态为 <code>上传中...</code><br>若文件已就绪，显示文件大小（如 5MB），并提供 <strong>[下载]</strong> 按钮。</td></tr></tbody></table><h4 id=fr-data-002-实时预警与消息推送>FR-DATA-002 实时预警与消息推送<a hidden class=anchor aria-hidden=true href=#fr-data-002-实时预警与消息推送>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>建立从“故障发生”到“责任人获知”的分钟级自动化闭环，确保高危问题被即时响应。</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left>1. <strong>灵活阈值策略</strong>：<br>- 支持自定义报警规则（固定阈值或环比突增）。<br>2. <strong>智能路由</strong>：<br>- 自动读取《全局事件注册表》中的 <code>Owner</code> 字段。<br>- 维护“Owner - 飞书群/个人ID”映射关系表，实现报警精准派单。</td></tr></tbody></table><h4 id=fr-data-003-核心指标统计与报表>FR-DATA-003 核心指标统计与报表<a hidden class=anchor aria-hidden=true href=#fr-data-003-核心指标统计与报表>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P0</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>提供多维度的质量大盘报表，支持日报、周报、月报的自动统计与趋势分析，覆盖<strong>千车严重故障率</strong>与<strong>应用崩溃率</strong>。</td></tr><tr><td style=text-align:left><strong>统计维度</strong></td><td style=text-align:left>支持按 <strong>车型</strong>、<strong>系统版本</strong>、<strong>时间周期</strong> (日/周/月) 进行交叉筛选。</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left><strong>1. 千车严重故障率</strong><br>- <strong>定义</strong>：每 1000 辆活跃车中，发生严重故障的车辆数。<br>- <strong>严重故障集</strong>：EventID 在开发过程中提供。<br>- <strong>公式</strong>：$$(\text{发生严重故障的去重VIN数} / \text{活跃去重VIN数}) \times 1000$$<br>- <strong>精度</strong>：保留整数。<br><br><strong>2. 应用崩溃率</strong><br>- <strong>定义</strong>：衡量应用运行的稳定性。<br>- <strong>公式</strong>：$$(\text{Crash事件总数} / \text{App启动事件总数}) \times 100%$$<br>- <strong>精度</strong>：保留两位小数。</td></tr><tr><td style=text-align:left><strong>功能/展示要求</strong></td><td style=text-align:left><strong>质量驾驶舱 (Dashboard)</strong>：<br>- <strong>核心卡片</strong>：展示上述指标的当前值。<br>- <strong>趋势分析</strong>：展示 日环比趋势箭头（红色代表劣化，绿色代表改善）。</td></tr></tbody></table><h4 id=fr-data-004-稳定性专项分析工作台>FR-DATA-004 稳定性专项分析工作台<a hidden class=anchor aria-hidden=true href=#fr-data-004-稳定性专项分析工作台>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>1. 提供 <strong>应用稳定性黑榜</strong>，快速识别 Crash/ANR 严重的组件。<br>2. 提供 <strong>系统重启原因分析</strong>，定位导致系统非预期重启的根因（如 Watchdog、Kernel Panic）。</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left><strong>1. 应用稳定性聚合</strong>：<br>- 以 <code>PackageName</code> (包名) 为维度，统计选定系统版本下的 Crash/ANR 总数与影响车数。<br><strong>2. 系统重启分类逻辑</strong>：<br>- 依据 <code>boot_reason</code> 字段进行自动化分类。 boot_reason的详细分类在开发过程中提供。<br><strong>3. 排序策略</strong>：<br>- 默认按 <strong>“影响车数” 降序</strong> 排列。<br><strong>4. 数据清洗</strong>：<br>- 自动过滤白名单内的测试车辆数据。</td></tr><tr><td style=text-align:left><strong>功能/展示要求</strong></td><td style=text-align:left><strong>一、 应用稳定性排行榜</strong>：<br>- <strong>列定义</strong>：排名 | 应用名称 | 归属部门 | Crash 影响车数 | Crash 次数 | ANR 影响车数 | ANR 次数。<br><br><strong>二、 系统重启分析视图</strong>：<br>- <strong>分布饼图</strong>：直观展示“正常重启”与“异常重启”的比例；点击“异常”扇区可联动下方列表。<br>- <strong>重启原因列表</strong>：<br>* <strong>列定义</strong>：重启分类 (如 Watchdog) | 具体原因 (boot_reason) | <strong>影响车数</strong> | 累计发生次数。<br><br><strong>三、 交互交互</strong>：<br>- 点击应用或重启原因，跳转至对应的 <strong>Event 详情页</strong>，以便直接下载关联的日志文件。</td></tr></tbody></table><h4 id=fr-data-005-性能与体验专项分析工作台>FR-DATA-005 性能与体验专项分析工作台<a hidden class=anchor aria-hidden=true href=#fr-data-005-性能与体验专项分析工作台>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P1</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>量化用户交互体验，通过 <strong>应用掉帧 (Jank)</strong> 与 <strong>冻结 (Freeze)</strong> 指标识别核心场景的性能瓶颈。</td></tr><tr><td style=text-align:left><strong>输入数据</strong></td><td style=text-align:left><strong>源事件</strong>：<code>GVM_APP_JANK_STATS</code><br><strong>关键字段</strong>：<br>- <code>activity_name</code> (页面名)<br>- <code>total_frames</code> (总帧数)<br>- <code>jank_count</code> (掉帧数)<br>- <code>frozen_count</code> (冻结帧数, >700ms)<br>- <code>avg_ui_ms</code> (平均UI耗时), <code>avg_gpu_ms</code> (平均GPU耗时)</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left>1. <strong>核心指标计算</strong>：<br>- <strong>平均掉帧率 (Jank Rate)</strong>：$$(\sum \text{jank_count} / \sum \text{total_frames}) \times 100%$$<br>- <strong>页面冻结率 (Freeze Session Rate)</strong>：衡量有多少次页面浏览发生了严重卡死。<br>$$(\text{Count(Events where frozen_count > 0)} / \text{Count(Total Events)}) \times 100%$$<br>- <strong>卡顿归因倾向 (Bottleneck Check)</strong>：<br>* 若 <code>avg_ui_ms</code> > <code>avg_gpu_ms</code>，标记为 <strong>CPU/MainThread Bound</strong> (主线程逻辑重)。<br>* 若 <code>avg_gpu_ms</code> > <code>avg_ui_ms</code>，标记为 <strong>GPU/Render Bound</strong> (渲染负载重)。<br>2. <strong>分位值统计</strong>：<br>- 计算 <strong>掉帧率 P90</strong>：排除偶发波动，反映绝大多数用户的真实体验底线。</td></tr><tr><td style=text-align:left><strong>功能/展示要求</strong></td><td style=text-align:left><strong>流畅度分析视图</strong>：<br>1. <strong>趋势分析</strong>：<br>- 展示核心应用（如 Map, Launcher）的掉帧率 P90 趋势曲线。<br>2. <strong>红黑榜 (Red/Black List)</strong>：<br>- <strong>最卡应用 Top 5</strong>：按平均掉帧率降序。<br>- <strong>最冻结页面 Top 10</strong>：按页面冻结率降序。<br>3. <strong>归因辅助</strong>：<br>- 在榜单中显示“主要瓶颈”标签（如 <span style=color:red>CPU</span> / <span style=color:blue>GPU</span>），基于 UI/GPU 耗时均值自动判定。</td></tr></tbody></table><h4 id=fr-data-006-单车资源画像与趋势分析>FR-DATA-006 单车资源画像与趋势分析<a hidden class=anchor aria-hidden=true href=#fr-data-006-单车资源画像与趋势分析>#</a></h4><table><thead><tr><th style=text-align:left>属性</th><th style=text-align:left>内容</th></tr></thead><tbody><tr><td style=text-align:left><strong>优先级</strong></td><td style=text-align:left>P2</td></tr><tr><td style=text-align:left><strong>目标</strong></td><td style=text-align:left>提供基于 VIN 码的深度诊断能力，通过可视化曲线回溯单车历史资源消耗趋势。</td></tr><tr><td style=text-align:left><strong>输入数据</strong></td><td style=text-align:left>1. <strong>数据源</strong>：<code>GVM_PROC_CPU_TOP</code> (CPU 负载) 与 <code>GVM_PROC_MEM_TOP</code> (内存消耗) 的历史记录。<br>2. <strong>查询条件</strong>：VIN 码、起止时间（支持回溯最近 7 天）。</td></tr><tr><td style=text-align:left><strong>业务处理规则</strong></td><td style=text-align:left>1. <strong>时序数据重组</strong>：<br>- 云端需将该 VIN 码下分散的 30 分钟采样点连接成连续的时间序列曲线。<br>2. <strong>多维关联展示</strong>：<br>- <strong>双轴对比</strong>：支持在同一时间轴上叠加 CPU 和内存曲线，分析两者是否存在正相关性（如内存泄漏导致系统频繁 LMK 引起的 CPU 波动）。<br>3. <strong>异常锚点标注</strong>：<br>- 若该时段内发生过 <code>GVM_APP_CRASH</code>、<code>GVM_SYS_WATCHDOG</code>等事件，需在曲线上方自动标注“异常图标”，点击图标可跳转至对应事件详情。</td></tr><tr><td style=text-align:left><strong>功能/展示要求</strong></td><td style=text-align:left><strong>单车诊断视图</strong>：<br>1. <strong>资源趋势图</strong>：<br>- <strong>CPU 轴</strong>：展示系统总负载曲线，并显示峰值时刻的 Top 5 进程名。<br>- <strong>Memory 轴</strong>：展示 <code>MemAvailable</code> (可用内存) 变化曲线及 Top 5 进程 RSS 占用。<br>2. <strong>数据交互</strong>：<br>- <strong>缩放平移</strong>：支持对时间轴进行缩放（Zoom-in/out），查看分钟级或小时级的细微变化。<br>- <strong>悬停详情</strong>：鼠标悬停在曲线某点时，弹出 Tooltip 显示该时刻具体的进程资源排行快照。</td></tr></tbody></table><h2 id=非功能需求>非功能需求<a hidden class=anchor aria-hidden=true href=#非功能需求>#</a></h2><p>本章节定义了 Polaris 1.0 平台在资源占用、性能、安全性及可靠性方面的约束指标。</p><h3 id=资源消耗约束>资源消耗约束<a hidden class=anchor aria-hidden=true href=#资源消耗约束>#</a></h3><blockquote><p><strong>目标</strong>：确保 Agent 运行时对座舱核心业务（如仪表渲染、语音交互）的影响趋近于零。</p></blockquote><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标要求</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>CPU 占用</strong></td><td style=text-align:left>静态采样时平均 &lt; 5%；异常捕获瞬间峰值 &lt; 10% (持续 &lt; 5s)</td><td style=text-align:left>严禁在主线程执行复杂运算，确保不引起系统卡顿。</td></tr><tr><td style=text-align:left><strong>内存占用 (PSS)</strong></td><td style=text-align:left>静态常驻 &lt; 150MB；日志聚合瞬间峰值 &lt; 250MB</td><td style=text-align:left>需严格控制缓存缓冲区大小，防止触发系统 LMK。</td></tr><tr><td style=text-align:left><strong>磁盘空间占用</strong></td><td style=text-align:left>离线日志存储上限：(TBD)</td><td style=text-align:left>采用循环覆盖策略，达到阈值后自动清理旧日志。</td></tr><tr><td style=text-align:left><strong>网络流量</strong></td><td style=text-align:left>结构化数据：&lt; 1MB/车/天(TBD)；大日志上传： (TBD)</td><td style=text-align:left>关键告警走高优先级通道，大文件需在网络空闲时异步拉取。</td></tr></tbody></table><h3 id=系统性能与实时性>系统性能与实时性<a hidden class=anchor aria-hidden=true href=#系统性能与实时性>#</a></h3><blockquote><p><strong>目标</strong>：确保从故障发生到研发获知的链路尽可能短，实现快速响应。</p></blockquote><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标要求</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>实时告警延迟</strong></td><td style=text-align:left>P0 级事件从端侧发出到云端推送（飞书）延迟 &lt; <strong>10 分钟</strong></td><td style=text-align:left>针对重启等致命问题。</td></tr><tr><td style=text-align:left><strong>常规数据延迟</strong></td><td style=text-align:left>性能指标、资源水位数据延迟 &lt; <strong>30 分钟</strong></td><td style=text-align:left>符合 30min 一个统计窗口的设计。</td></tr><tr><td style=text-align:left><strong>日志关联成功率</strong></td><td style=text-align:left>Event 与 Log 的自动关联成功率 > 99%</td><td style=text-align:left>排除网络极端断连情况。</td></tr><tr><td style=text-align:left><strong>云端并发支撑</strong></td><td style=text-align:left>支持同时在线车辆数：<strong>100,000 辆</strong> (TBD)</td><td style=text-align:left>需考虑灰度及全量推广后的并发峰值。</td></tr></tbody></table><h3 id=可靠性与稳定性>可靠性与稳定性<a hidden class=anchor aria-hidden=true href=#可靠性与稳定性>#</a></h3><blockquote><p><strong>目标</strong>：监控系统自身必须比被监控系统更稳健。</p></blockquote><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标要求</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>服务自愈</strong></td><td style=text-align:left>若 PolarisAgent 非预期崩溃，需在 5s 内被守护进程自动拉起</td><td style=text-align:left>确保监控不长时间掉线。</td></tr><tr><td style=text-align:left><strong>数据防丢失</strong></td><td style=text-align:left>断网期间，结构化事件需在端侧持久化缓存，恢复连接后补发</td><td style=text-align:left>支持本地 FIFO 队列存储，容量 TBD。</td></tr><tr><td style=text-align:left><strong>容错性</strong></td><td style=text-align:left>Polaris 内部异常不应触发任何系统级弹窗或 ANR 提示</td><td style=text-align:left>采用旁路监控设计，对业务流程无侵入。</td></tr></tbody></table><h3 id=安全与隐私>安全与隐私<a hidden class=anchor aria-hidden=true href=#安全与隐私>#</a></h3><blockquote><p><strong>目标</strong>：符合国家车联网安全标准及个人隐私保护条例。</p></blockquote><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标要求</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>数据脱敏</strong></td><td style=text-align:left>日志及事件中严禁包含车主姓名、手机号、实时经纬度等 PII 数据</td><td style=text-align:left>需对上报 Payload 进行自动脱敏扫描或正则过滤。</td></tr><tr><td style=text-align:left><strong>传输安全</strong></td><td style=text-align:left>端云通信全程采用 HTTPS/TLS 1.2+ 加密</td><td style=text-align:left>防止指令被劫持。</td></tr><tr><td style=text-align:left><strong>远程诊断权限</strong></td><td style=text-align:left>指令下发需双重鉴权：操作人权限校验 + 终端数字签名验签</td><td style=text-align:left>参考远程操作安全校验规则。</td></tr><tr><td style=text-align:left><strong>审计跟踪</strong></td><td style=text-align:left>所有远程提数和诊断操作必须 100% 留存审计日志</td><td style=text-align:left>支持 180 天操作回溯与合规检查。</td></tr></tbody></table><h3 id=可维护性与扩展性>可维护性与扩展性<a hidden class=anchor aria-hidden=true href=#可维护性与扩展性>#</a></h3><blockquote><p><strong>目标</strong>：适应座舱业务逻辑的快速演进。</p></blockquote><table><thead><tr><th style=text-align:left>维度</th><th style=text-align:left>指标要求</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left><strong>配置动态更新</strong></td><td style=text-align:left>支持在不升级版本的情况下，动态下发采样率和阈值配置</td><td style=text-align:left>云端下发策略包，端侧 Polaris Agent 实时生效。</td></tr><tr><td style=text-align:left><strong>协议扩展性</strong></td><td style=text-align:left>《全局事件注册表》支持热更新及向后兼容</td><td style=text-align:left>新增字段不应导致旧版云端解析引擎崩溃。</td></tr></tbody></table><h2 id=端云交互协议设计>端云交互协议设计<a hidden class=anchor aria-hidden=true href=#端云交互协议设计>#</a></h2><h3 id=接口定义>接口定义<a hidden class=anchor aria-hidden=true href=#接口定义>#</a></h3><p>参考：VlmWrapper接口说明文档</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-java data-lang=java><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>native</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>init</span>(String ecu,String appname,String ver)
</span></span><span style=display:flex><span><span style=color:#66d9ef>public</span> <span style=color:#66d9ef>native</span> <span style=color:#66d9ef>int</span> <span style=color:#a6e22e>sendWcLog</span>(<span style=color:#66d9ef>long</span> evid, <span style=color:#66d9ef>byte</span> etype, <span style=color:#66d9ef>short</span> elevel, <span style=color:#66d9ef>long</span> eTime, String edesc);
</span></span></code></pre></div><h3 id=参数描述>参数描述<a hidden class=anchor aria-hidden=true href=#参数描述>#</a></h3><table><thead><tr><th>中文名</th><th>英文名</th><th>是否必填</th><th>参数名</th><th>数据类型</th><th>数据项说明</th><th>备注</th></tr></thead><tbody><tr><td>任务序号</td><td>sn</td><td>必填</td><td></td><td>string</td><td>当前维测日志流水号（取 ECU IP 地址最后一字节 + 时间戳）</td><td>由 vlmagent 自动填充</td></tr><tr><td>产品类别</td><td>ProductCat</td><td>必填</td><td></td><td>string</td><td>产品类别，即域控名称，比如 s32g、tbox、adas、8295and、8295qnx；部署在各个域控上的应用此字段填对应域控名称即可</td><td>初始化传入</td></tr><tr><td>应用名称</td><td>ProductName</td><td>必填</td><td></td><td>string</td><td>应用名称</td><td>区分跟 vlmagent 建立的不同连接，初始化传入</td></tr><tr><td>版本号</td><td>Version</td><td>必填</td><td></td><td>string</td><td>应用软件版本号</td><td>如 Android / Linux 系统版本号，初始化传入</td></tr><tr><td>进程ID</td><td>PID</td><td>必填</td><td></td><td>int</td><td>进程 ID</td><td>vlmagent pid，用于区分不同进程；由 vlmagent 自动填充</td></tr><tr><td>事件ID</td><td>EventID</td><td>必填</td><td>evid</td><td>long</td><td>EventID 采用 6 开头 10 位长度编码，整车统一分配各域码段，各域内部自行分配应用子码段；同一故障在不同车型需保持相同 EventID。<br>S32G/VCM：6880000000–6889999999<br>TBOX：6860000000–6869999999<br>VIU：6850000000–6859999999<br>8295Android：6660000000–6669999999<br>8295QNX：6680000000–6689999999<br>ADAS：6690000000–6699999999</td><td><code>[Prefix(3位)] + [Scope(1位)] + [Sequence(6位)]</code><br>666：GVM（Android 系统）<br>668：PVM（Linux Host 系统）<br>685：MCU 子系统<br>示例：<code>GVM_FRAMEWORK_WATCHDOG_RESET</code> → <code>6660000001</code><br>映射到 Logical_Module / Owner</td></tr><tr><td>事件类型</td><td>EventType</td><td>必填</td><td>etype</td><td>byte</td><td>1. <strong>KEYINFO</strong>：关键信息记录，由特定事件累计产生的效应；<br>2. <strong>KEYEXCEPTION</strong>：进程或系统捕获的异常：<br> 1）影响有限、可自动恢复；<br> 2）严重问题，功能不可用（如 ANR、Crash、tombstone）；<br>3. <strong>SYSERROR</strong>：操作系统层异常：<br> 1）严重影响系统性能（内存异常、systemserver 死锁等）；<br> 2）系统崩溃（minidump、黑屏卡死、非预期重启等）</td><td>映射 SDK_Type</td></tr><tr><td>事件等级</td><td>EventLevel</td><td>必填</td><td>elevel</td><td>short</td><td>等级按对用户功能影响程度划分；各事件类型下有 0、1 两个等级：0 为业务级，1 为系统基础级</td><td>映射 SDK_Level</td></tr><tr><td>事件发生时间</td><td>HappenTime</td><td>必填</td><td>eTime</td><td>long</td><td>事件发生时间，时间戳（毫秒）</td><td></td></tr><tr><td>事件描述</td><td>EventDescription</td><td>必填</td><td>edesc</td><td>string</td><td>JSON 字符串，限制 1000 字节</td><td>由 Common 字段 和 Desc_Schema 字段组成（key: value）</td></tr></tbody></table><h3 id=edesc-字段组成>edesc 字段组成<a hidden class=anchor aria-hidden=true href=#edesc-字段组成>#</a></h3><table><thead><tr><th>字段</th><th>类型</th><th>字节估算</th><th>说明</th></tr></thead><tbody><tr><td>tid</td><td>string</td><td>~32 bytes</td><td>Trace ID。全链路追踪 ID，用于串联跨端 / 跨进程调用。</td></tr><tr><td>pid</td><td>int</td><td>~4 bytes</td><td>Process ID。结合 logcat / tombstone 时定位具体进程实例。</td></tr><tr><td>proc</td><td>string</td><td>~20 bytes</td><td>Process Name。明确是谁出的事（如 <code>com.map.app</code>）。</td></tr><tr><td>ver</td><td>string</td><td>~10 bytes</td><td>Version。应用 / 模块自身的版本号（如 <code>1.2.0</code>）。</td></tr><tr><td>logf</td><td>string</td><td>~20 bytes</td><td>Log Filename。大文件文件名（不含路径）；默认为空，手动设置。</td></tr><tr><td>&mldr;</td><td>&mldr;</td><td>&mldr;</td><td>注册表 <code>Desc_Schema</code> 中定义的其他业务字段。</td></tr></tbody></table><h2 id=安全与隐私-1>安全与隐私<a hidden class=anchor aria-hidden=true href=#安全与隐私-1>#</a></h2><p>本章节定义了 Polaris 1.0 平台在数据采集、传输、存储及远程控制全生命周期中的安全防护要求，确保符合车联网数据安全国家标准。</p><h3 id=远程诊断安全架构>远程诊断安全架构<a hidden class=anchor aria-hidden=true href=#远程诊断安全架构>#</a></h3><blockquote><p><strong>目标</strong>：防止远程诊断接口被恶意利用或误操作导致车辆受损。</p></blockquote><table><thead><tr><th style=text-align:left>策略名称</th><th style=text-align:left>详细要求</th></tr></thead><tbody><tr><td style=text-align:left><strong>指令白名单</strong></td><td style=text-align:left>端侧仅允许执行预埋在 <code>PolarisAgent</code> 中的标准化指令集。严禁执行 <code>eval</code>、<code>rm -rf</code> 等具有破坏性的原始 Shell 命令。</td></tr><tr><td style=text-align:left><strong>双重身份校验</strong></td><td style=text-align:left>1. <strong>云端权限校验</strong>：操作人必须具备特定安全证书。<br>2. <strong>端侧验签</strong>：指令包必须携带数字签名，Agent 使用内置公钥验证指令来源的真实性。</td></tr><tr><td style=text-align:left><strong>行车状态限制</strong></td><td style=text-align:left>部分涉及交互或高资源消耗的指令，必须在车辆处于 <strong>非行车状态 (P档/车速为0)</strong> 时方可执行，防止分散驾驶员注意力或干扰实时性要求。</td></tr><tr><td style=text-align:left><strong>强制审计留痕</strong></td><td style=text-align:left>所有远程执行请求（无论成功与否）必须在端侧审计日志和云端数据库中同步记录，包含：操作人、VIN、指令内容、时间戳及返回码。</td></tr></tbody></table><h3 id=用户隐私保护>用户隐私保护<a hidden class=anchor aria-hidden=true href=#用户隐私保护>#</a></h3><blockquote><p><strong>目标</strong>：在故障排查的同时，严格保护车主个人敏感信息（PII）。</p></blockquote><table><thead><tr><th style=text-align:left>策略名称</th><th style=text-align:left>详细要求</th></tr></thead><tbody><tr><td style=text-align:left><strong>静态脱敏</strong></td><td style=text-align:left>结构化事件 Payload 中禁止采集：车主姓名、手机号、家庭住址、车牌号。</td></tr><tr><td style=text-align:left><strong>动态脱敏过滤</strong></td><td style=text-align:left>在截取日志文件（如 Logcat/Perflog）时，通过正则表达式自动过滤敏感信息。例如，对 MAC 地址、IP 地址、部分 VIN 码进行掩码处理（如：<code>123****890</code>）。</td></tr><tr><td style=text-align:left><strong>按需采集原则</strong></td><td style=text-align:left>严禁采集与性能/稳定性无关的数据。禁止抓取：摄像头实时画面、麦克风录音、短消息内容或通讯录数据。</td></tr><tr><td style=text-align:left><strong>用户授权 (TBD)</strong></td><td style=text-align:left>针对特定的深度调试指令（可能涉及部分应用路径数据），需根据合规要求触发 HMI 弹窗询问，获取用户显式授权后方可提数。</td></tr></tbody></table><h3 id=数据传输安全>数据传输安全<a hidden class=anchor aria-hidden=true href=#数据传输安全>#</a></h3><blockquote><p><strong>目标</strong>：确保端云通信链路不可被监听或篡改。</p></blockquote><table><thead><tr><th style=text-align:left>策略名称</th><th style=text-align:left>详细要求</th></tr></thead><tbody><tr><td style=text-align:left><strong>加密传输</strong></td><td style=text-align:left>全量数据（包括 Event 和 Log 文件）必须通过加密通道传输，强制使用 TLS 1.2 或更高版本协议。</td></tr><tr><td style=text-align:left><strong>双向认证</strong></td><td style=text-align:left>车端 VlmAgent 与云端网关之间采用双向数字证书认证，防止伪造云端或非法车辆接入。</td></tr><tr><td style=text-align:left><strong>防重放攻击</strong></td><td style=text-align:left>指令包中必须包含一次性 <code>nonce</code> (随机数) 或 毫秒级时间戳，端侧记录最近处理的 ID，超时或重复的指令自动丢弃。</td></tr></tbody></table><h3 id=存储与生命周期管理>存储与生命周期管理<a hidden class=anchor aria-hidden=true href=#存储与生命周期管理>#</a></h3><blockquote><p><strong>目标</strong>：防止数据因过期或未授权访问而泄露。</p></blockquote><table><thead><tr><th style=text-align:left>策略名称</th><th style=text-align:left>详细要求</th></tr></thead><tbody><tr><td style=text-align:left><strong>存储加密</strong></td><td style=text-align:left>存储在云端对象存储（OSS/S3）中的原始日志文件，必须处于加密存储状态。</td></tr><tr><td style=text-align:left><strong>访问控制</strong></td><td style=text-align:left>云端管理平台采用 RBAC（基于角色的权限访问控制），研发人员仅能查看其负责的应用/模块相关的故障数据。</td></tr><tr><td style=text-align:left><strong>自动销毁机制</strong></td><td style=text-align:left>严格执行生命周期策略：<br>- 普通日志：30 天自动物理删除。<br>- 严重故障日志：180 天自动清理。<br>- 审计日志：根据法律要求保留 1-3 年。</td></tr></tbody></table><h2 id=风险--限制--依赖>风险 & 限制 & 依赖<a hidden class=anchor aria-hidden=true href=#风险--限制--依赖>#</a></h2><h3 id=1-风险点>1. 风险点<a hidden class=anchor aria-hidden=true href=#1-风险点>#</a></h3><blockquote><p><strong>目标</strong>：识别可能导致项目进度延期、功能失效或系统不稳定的不确定因素。</p></blockquote><table><thead><tr><th style=text-align:left>风险类别</th><th style=text-align:left>描述</th><th style=text-align:left>应对策略</th></tr></thead><tbody><tr><td style=text-align:left><strong>性能干扰风险</strong></td><td style=text-align:left>监控插件（尤其是插桩逻辑）若存在性能缺陷，可能导致被监控的应用出现卡顿或 ANR。</td><td style=text-align:left>建立端侧 Agent 专项压力测试；引入“熔断机制”，当检测到 Agent 自身资源超限时自动降级或自杀。</td></tr><tr><td style=text-align:left><strong>日志风暴风险</strong></td><td style=text-align:left>在极端故障（如系统级死锁并反复重启）下，可能会产生海量日志，导致存储占满或流量激增。</td><td style=text-align:left>实施本地流量限额及“相同故障聚合”策略；仅在网络空闲下同步大型日志文件。</td></tr><tr><td style=text-align:left><strong>异构架构兼容性</strong></td><td style=text-align:left>跨 Hypervisor 的通信协议在不同 SOC 平台上可能存在差异。</td><td style=text-align:left>采用标准化的 Native Daemon 接口定义，将平台相关性封装在 HAL 层，保持上层逻辑一致。</td></tr><tr><td style=text-align:left><strong>数据合规风险</strong></td><td style=text-align:left>国家对车联网数据出境及隐私保护政策的动态调整，可能导致现有的采集方案不合规。</td><td style=text-align:left>保持与法务合规团队同步；系统架构设计支持“脱敏规则”的云端动态下发和快速调整。</td></tr></tbody></table><h3 id=2-系统限制>2. 系统限制<a hidden class=anchor aria-hidden=true href=#2-系统限制>#</a></h3><blockquote><p><strong>目标</strong>：明确 Polaris 1.0 平台“不做”什么，以及受限于当前技术环境的边界。</p></blockquote><table><thead><tr><th style=text-align:left>限制类别</th><th style=text-align:left>描述</th></tr></thead><tbody><tr><td style=text-align:left><strong>硬件故障捕获限制</strong></td><td style=text-align:left>本阶段暂不具备对纯硬件物理层故障的直接捕获与诊断能力（如 PCB 短路、传感器物理损坏、屏幕背光模组失效等）。平台目前侧重于软件稳定性、系统逻辑及软硬交互层面的监控。</td></tr><tr><td style=text-align:left><strong>深度日志追溯限制</strong></td><td style=text-align:left>由于 EMMC/UFS 擦写寿命及存储空间限制，离线日志仅能保留最近 <strong>10G</strong> (TBD) 的数据，超过部分将被循环覆盖。</td></tr><tr><td style=text-align:left><strong>网络依赖性</strong></td><td style=text-align:left>远程诊断及实时告警功能强依赖于车辆的 4G/5G 连接状态；在地下车库等信号盲区，数据仅能本地缓存，延迟上报。</td></tr><tr><td style=text-align:left><strong>代码修复限制</strong></td><td style=text-align:left>Polaris 平台仅提供“故障定位”与“证据收集”能力，不具备对业务 App 或系统内核进行自动代码修复（Hotfix）的功能。</td></tr></tbody></table><h3 id=3-外部依赖-dependencies>3. 外部依赖 (Dependencies)<a hidden class=anchor aria-hidden=true href=#3-外部依赖-dependencies>#</a></h3><blockquote><p><strong>目标</strong>：明确项目成功落地所需的跨部门协同及软硬件支撑。</p></blockquote><table><thead><tr><th style=text-align:left>依赖对象</th><th style=text-align:left>需求描述</th><th style=text-align:left>关键影响</th></tr></thead><tbody><tr><td style=text-align:left><strong>车云网关 (VlmAgent)</strong></td><td style=text-align:left>依赖车云现有网关支持断点续传、大文件分片上传以及 TLS 1.2 加密通道。</td><td style=text-align:left>关系到日志文件能否稳定、安全地回传至云端。</td></tr><tr><td style=text-align:left><strong>BSP/底层固件</strong></td><td style=text-align:left>需底层提供解析硬件复位原因（PMIC Reset Reason）的接口。</td><td style=text-align:left>关系到“Linux Host 重启”原因判定的准确性。</td></tr><tr><td style=text-align:left><strong>云端基础设施</strong></td><td style=text-align:left>依赖 IT/车云部门提供高可用的对象存储 (OSS) 以及大数据计算集群支持。</td><td style=text-align:left>关系到海量结构化数据的实时计算处理与可视化。</td></tr></tbody></table><h2 id=实施计划>实施计划<a hidden class=anchor aria-hidden=true href=#实施计划>#</a></h2><h3 id=阶段划分-phases>阶段划分 (Phases)<a hidden class=anchor aria-hidden=true href=#阶段划分-phases>#</a></h3><p>项目整体周期预计为 <strong>6个月</strong> (TBD)，分为四个核心阶段：</p><table><thead><tr><th style=text-align:left>阶段</th><th style=text-align:left>阶段名称</th><th style=text-align:left>关键任务</th><th style=text-align:left>交付物</th></tr></thead><tbody><tr><td style=text-align:left><strong>M1</strong></td><td style=text-align:left><strong>架构设计与协议定标</strong></td><td style=text-align:left>1. 完成端云交互协议定义（JSON/ProtoBuf）。<br>2. 制定《全局事件 ID 注册表 1.0》。<br>3. 完成端侧 Agent 分层架构设计。</td><td style=text-align:left>《端云接口规范》<br>《全局事件注册表》</td></tr><tr><td style=text-align:left><strong>M2</strong></td><td style=text-align:left><strong>端侧核心能力开发</strong></td><td style=text-align:left>1. 开发 <code>PolarisAgent</code> 核心服务（含流控、聚合逻辑）。<br>2. 实现 Android/Linux 跨域通信通道。<br>3. 集成基础异常捕获（Crash/ANR/Reboot）。</td><td style=text-align:left>Polaris Agent Alpha版<br>测试验证报告</td></tr><tr><td style=text-align:left><strong>M3</strong></td><td style=text-align:left><strong>云端平台与数据闭环</strong></td><td style=text-align:left>1. 建设云端事件解析引擎与大盘（Dashboard）。<br>2. 实现“事件-日志”自动关联引擎。<br>3. 开展端云一体化全链路冒烟测试。</td><td style=text-align:left>云端管理台看板<br>端云联调报告</td></tr><tr><td style=text-align:left><strong>M4</strong></td><td style=text-align:left><strong>灰度发布与全量试点</strong></td><td style=text-align:left>1. 选取 100~500 台灰度车辆进行试点部署。<br>2. 根据灰度数据修正指标阈值与流控策略。<br>3. 性能压测与全量版本推送。</td><td style=text-align:left>灰度运营报告<br>正式版 SOP 手册</td></tr></tbody></table><h3 id=资源需求计划-resource-allocation>资源需求计划 (Resource Allocation)<a hidden class=anchor aria-hidden=true href=#资源需求计划-resource-allocation>#</a></h3><h4 id=人力资源>人力资源<a hidden class=anchor aria-hidden=true href=#人力资源>#</a></h4><blockquote><p>预计投入总人力：<strong>XX 人/月</strong>。</p></blockquote><table><thead><tr><th style=text-align:left>职能角色</th><th style=text-align:left>预估人数</th><th style=text-align:left>核心职责</th></tr></thead><tbody><tr><td style=text-align:left><strong>端侧开发工程师</strong></td><td style=text-align:left>3-4 名</td><td style=text-align:left>负责 Android Framework 插桩、Native Daemon 及异构数据采集。</td></tr><tr><td style=text-align:left><strong>云端开发工程师</strong></td><td style=text-align:left>TBD</td><td style=text-align:left>负责大数据流式计算、API 开发及规则引擎构建。</td></tr><tr><td style=text-align:left><strong>前端开发工程师</strong></td><td style=text-align:left>TBD</td><td style=text-align:left>负责可视化驾驶舱、单车画像及诊断工作台页面。</td></tr><tr><td style=text-align:left><strong>测试/质量工程师</strong></td><td style=text-align:left>1-2 名</td><td style=text-align:left>负责端侧资源消耗专项测试、数据准确性验证。</td></tr><tr><td style=text-align:left><strong>系统架构师</strong></td><td style=text-align:left>1 名</td><td style=text-align:left>负责端云协议统筹、跨域通信架构及安全策略设计。</td></tr></tbody></table><h4 id=基础设施与硬件>基础设施与硬件<a hidden class=anchor aria-hidden=true href=#基础设施与硬件>#</a></h4><table><thead><tr><th style=text-align:left>资源类别</th><th style=text-align:left>规格/描述</th><th style=text-align:left>用途</th></tr></thead><tbody><tr><td style=text-align:left><strong>对象存储 (OSS)</strong></td><td style=text-align:left>TBD</td><td style=text-align:left>用于存储非结构化日志文件（Logcat, Trace, Bootchart）。</td></tr><tr><td style=text-align:left><strong>云端数据库</strong></td><td style=text-align:left>TBD</td><td style=text-align:left>用于存储高性能/资源水位等结构化指标数据。</td></tr><tr><td style=text-align:left><strong>测试样车</strong></td><td style=text-align:left>3~5 辆 (含高通 8155/8295 平台)</td><td style=text-align:left>用于端侧采集能力的实车验证及压测。</td></tr><tr><td style=text-align:left><strong>边缘计算配额</strong></td><td style=text-align:left>TBD</td><td style=text-align:left>用于远程诊断指令下发。</td></tr></tbody></table><h4 id=外部协同依赖>外部协同依赖<a hidden class=anchor aria-hidden=true href=#外部协同依赖>#</a></h4><ul><li><strong>OTA 团队</strong>：负责 Polaris Agent 插件的差分分发与静默安装支持。</li><li><strong>IT 网络团队</strong>：负责配置端云通信的专用域名及 TLS 证书安全准入。</li></ul><h2 id=附录>附录<a hidden class=anchor aria-hidden=true href=#附录>#</a></h2></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ethen-cao.github.io/ethenslab/>Ethen 的实验室</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0,theme:"default"})</script><script src=https://cdn.jsdelivr.net/npm/plantuml-encoder@1.4.0/dist/plantuml-encoder.min.js></script><script>(function(){const e=document.querySelectorAll("pre > code.language-plantuml, pre > code.language-planuml");e.forEach(e=>{const s=e.innerText,o=plantumlEncoder.encode(s),i="https://www.plantuml.com/plantuml/svg/"+o,t=document.createElement("img");t.src=i,t.alt="PlantUML Diagram",t.style.maxWidth="100%";const n=e.parentNode;n.parentNode.replaceChild(t,n)})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>