<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Hexagon DSP简介 | Ethen 的实验室</title><meta name=keywords content><meta name=description content="Hexagon DSP 的生态
在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为两条主要赛道。它们针对的场景不同，开发难度不同，工具链也完全不同。
1. 第一条赛道：AI 推理 (AI Inference)
关键词：TFLite, ONNX, SNPE, QNN, NNAPI
核心逻辑：“模型即代码” (Model is the code)
这是目前最主流的用法，主要用于深度学习。

主要工作：

训练模型（在 PC 上用 PyTorch/TensorFlow）。
量化模型（把 float32 转成 int8，为了 DSP 效率）。
配置：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。


IDL 哪里去了？

高通（或 Google）已经提前写好了通用的 .idl 和 skel.so。
比如 libQnnDsp.so 或 libhexagon_nn_skel.so。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。


优点：开发快，不用懂 DSP 汇编，只要模型能跑通就行。
缺点：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。

2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing)
关键词：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX
核心逻辑：“手写算子” (Custom Implementation)"><meta name=author content><link rel=canonical href=https://ethen-cao.github.io/ethenslab/explore-ai/hexagon-dsp%E7%AE%80%E4%BB%8B/><link crossorigin=anonymous href=/ethenslab/assets/css/stylesheet.a1917769c3c78460b110da6d7905321bb53af4a56f22ba4cc0de824cf4d097ab.css integrity="sha256-oZF3acPHhGCxENpteQUyG7U69KVvIrpMwN6CTPTQl6s=" rel="preload stylesheet" as=style><link rel=icon href=https://ethen-cao.github.io/ethenslab/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ethen-cao.github.io/ethenslab/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ethen-cao.github.io/ethenslab/favicon-32x32.png><link rel=apple-touch-icon href=https://ethen-cao.github.io/ethenslab/apple-touch-icon.png><link rel=mask-icon href=https://ethen-cao.github.io/ethenslab/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ethen-cao.github.io/ethenslab/explore-ai/hexagon-dsp%E7%AE%80%E4%BB%8B/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://ethen-cao.github.io/ethenslab/explore-ai/hexagon-dsp%E7%AE%80%E4%BB%8B/"><meta property="og:site_name" content="Ethen 的实验室"><meta property="og:title" content="Hexagon DSP简介"><meta property="og:description" content="Hexagon DSP 的生态 在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为两条主要赛道。它们针对的场景不同，开发难度不同，工具链也完全不同。
1. 第一条赛道：AI 推理 (AI Inference) 关键词：TFLite, ONNX, SNPE, QNN, NNAPI 核心逻辑：“模型即代码” (Model is the code)
这是目前最主流的用法，主要用于深度学习。
主要工作： 训练模型（在 PC 上用 PyTorch/TensorFlow）。 量化模型（把 float32 转成 int8，为了 DSP 效率）。 配置：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。 IDL 哪里去了？ 高通（或 Google）已经提前写好了通用的 .idl 和 skel.so。 比如 libQnnDsp.so 或 libhexagon_nn_skel.so。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。 优点：开发快，不用懂 DSP 汇编，只要模型能跑通就行。 缺点：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。 2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing) 关键词：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX 核心逻辑：“手写算子” (Custom Implementation)"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="explore-ai"><meta property="article:published_time" content="2025-08-27T17:17:50+08:00"><meta property="article:modified_time" content="2025-08-27T17:17:50+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hexagon DSP简介"><meta name=twitter:description content="Hexagon DSP 的生态
在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为两条主要赛道。它们针对的场景不同，开发难度不同，工具链也完全不同。
1. 第一条赛道：AI 推理 (AI Inference)
关键词：TFLite, ONNX, SNPE, QNN, NNAPI
核心逻辑：“模型即代码” (Model is the code)
这是目前最主流的用法，主要用于深度学习。

主要工作：

训练模型（在 PC 上用 PyTorch/TensorFlow）。
量化模型（把 float32 转成 int8，为了 DSP 效率）。
配置：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。


IDL 哪里去了？

高通（或 Google）已经提前写好了通用的 .idl 和 skel.so。
比如 libQnnDsp.so 或 libhexagon_nn_skel.so。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。


优点：开发快，不用懂 DSP 汇编，只要模型能跑通就行。
缺点：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。

2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing)
关键词：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX
核心逻辑：“手写算子” (Custom Implementation)"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Explore AI","item":"https://ethen-cao.github.io/ethenslab/explore-ai/"},{"@type":"ListItem","position":2,"name":"Hexagon DSP简介","item":"https://ethen-cao.github.io/ethenslab/explore-ai/hexagon-dsp%E7%AE%80%E4%BB%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Hexagon DSP简介","name":"Hexagon DSP简介","description":"Hexagon DSP 的生态 在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为两条主要赛道。它们针对的场景不同，开发难度不同，工具链也完全不同。\n1. 第一条赛道：AI 推理 (AI Inference) 关键词：TFLite, ONNX, SNPE, QNN, NNAPI 核心逻辑：“模型即代码” (Model is the code)\n这是目前最主流的用法，主要用于深度学习。\n主要工作： 训练模型（在 PC 上用 PyTorch/TensorFlow）。 量化模型（把 float32 转成 int8，为了 DSP 效率）。 配置：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。 IDL 哪里去了？ 高通（或 Google）已经提前写好了通用的 .idl 和 skel.so。 比如 libQnnDsp.so 或 libhexagon_nn_skel.so。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。 优点：开发快，不用懂 DSP 汇编，只要模型能跑通就行。 缺点：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。 2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing) 关键词：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX 核心逻辑：“手写算子” (Custom Implementation)\n","keywords":[],"articleBody":"Hexagon DSP 的生态 在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为两条主要赛道。它们针对的场景不同，开发难度不同，工具链也完全不同。\n1. 第一条赛道：AI 推理 (AI Inference) 关键词：TFLite, ONNX, SNPE, QNN, NNAPI 核心逻辑：“模型即代码” (Model is the code)\n这是目前最主流的用法，主要用于深度学习。\n主要工作： 训练模型（在 PC 上用 PyTorch/TensorFlow）。 量化模型（把 float32 转成 int8，为了 DSP 效率）。 配置：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。 IDL 哪里去了？ 高通（或 Google）已经提前写好了通用的 .idl 和 skel.so。 比如 libQnnDsp.so 或 libhexagon_nn_skel.so。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。 优点：开发快，不用懂 DSP 汇编，只要模型能跑通就行。 缺点：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。 2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing) 关键词：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX 核心逻辑：“手写算子” (Custom Implementation)\n这是传统的嵌入式开发用法。\n主要工作： 定义接口：必须写 .idl 文件，告诉 CPU 和 DSP 怎么传参数。 生成胶水代码：使用 QAIC 编译 IDL，生成 Stub 和 Skel。 实现算法：在 DSP 侧写 C/C++ 代码（Skel 实现）。如果是为了高性能，你甚至需要用 Hexagon Intrinsics 手写向量化指令（利用 HVX 硬件加速）。 IDL 的作用： 因为你的函数是自定义的（例如 my_special_image_filter()），高通没法预知，所以必须由你通过 IDL 定义，并由 QAIC 生成专用的桥梁。 优点： 极高的自由度：可以做任何计算任务（图像处理、CV 算法、音频编解码、加密解密、传感器数据融合）。 极致性能：你可以手写汇编级优化，榨干 DSP 的每一个时钟周期。 缺点：门槛极高，需要懂内存管理、多线程同步、向量化编程，还要处理复杂的签名和打包流程。 对比总结表 特性 途径 1: AI 框架 (TFLite/ONNX) 途径 2: 普通 DSP 开发 (FastRPC) 输入物 神经网络模型文件 (.tflite, .onnx, .dlc) C/C++ 源代码 + .idl 接口定义 中间工具 模型转换器 (Converter / Quantizer) QAIC 编译器 运行库 通用推理引擎 (libQnnDsp.so, libSnpeDsp.so) 你编译生成的专用库 (lib_skel.so) 是否需要 IDL 不需要 (框架内部封装好了) 必须需要 主要难点 模型量化精度损失、算子支持度 内存管理、并行编程、HVX 向量化优化 典型场景 物体检测、人脸识别、语音识别 图像预处理(缩放/旋转)、ISP 算法、音频降噪 它们有交集吗？ 有，而且很重要。\n这就是所谓的 “Custom Operator” (自定义算子)。 假设你在跑一个 TFLite 模型，里面有一个很新的数学运算层（比如某种特殊的 Attention 机制），TFLite 的 DSP 代理不支持它。 这时候，你需要：\n走 途径 2：写 .idl，用 C++ 实现这个特殊的算子，编译成一个 DSP 库。 走 途径 1：告诉 TFLite，“遇到这个特殊的层，请调用我刚才写的那个库”。 所以，途径 2 其实是途径 1 的底层基石。\n神经网络相关的任务简介 当我们说“基于 TFLite/ONNX 等框架只能做神经网络相关的任务”时，意思是这套工具链的本质是**“解释器”（Interpreter）**。\n它不懂你的 C++ 业务逻辑，它只懂**“张量运算”（Matrix Math）**。它的唯一工作就是加载一个你训练好的模型文件，把输入数据喂进去，算出输出结果。\n具体来说，这些任务通常指的是可以用深度学习模型（Deep Learning Models）解决的问题。\n以下是这类任务的具体分类和典型例子：\n1. 计算机视觉 (Computer Vision - CV) 这是 DSP 上最常见的神经网络任务。输入是图片/视频帧，输出是识别结果。\n物体检测 (Object Detection): 识别画面里哪里有人、车、红绿灯。（典型模型：YOLO, SSD, EfficientDet）。 图像分类 (Image Classification): 判断这张图是“猫”还是“狗”。（典型模型：MobileNet, ResNet）。 语义分割 (Semantic Segmentation): 把背景虚化（比如视频会议换背景），或者自动驾驶中识别哪里是路面。（典型模型：DeepLab, UNet）。 人脸关键点 (Face Landmark): 识别眼睛、鼻子嘴巴的位置，用于美颜、贴纸特效。 超分辨率 (Super Resolution): 把 720p 的模糊视频通过 AI 猜想变成 1080p 清晰视频。 2. 音频与语音处理 (Audio \u0026 Speech) 输入是麦克风的音频流，输出是文字或增强后的音频。\n关键词唤醒 (Keyword Spotting): 手机待机时监听“Hey Siri”或“小爱同学”。这是一个极小的神经网络，常驻 DSP。 语音降噪 (AI Noise Suppression): 区分人声和背景噪音（如风扇声），只保留人声。（传统降噪是用滤波算法，现在流行用 RNN/LSTM 神经网络做）。 声纹识别 (Speaker Verification): 确认说话的人是不是机主。 3. 自然语言处理 (Natural Language - NLP) 虽然大模型（LLM）通常跑在 NPU 上，但 DSP 也可以处理轻量级的 NLP 任务。\n意图识别: 比如输入“定明天的闹钟”，模型判断这是一个“设置闹钟”的指令。 智能键盘预测: 预测你下一个要打的字。 关键区别：它“不能”做什么？ 为了更清楚边界在哪里，我们来看看哪些任务不属于“神经网络相关任务”，因此不能用 TFLite/ONNX 途径，而必须用 FastRPC + C++ (IDL) 的方式去开发：\n传统的 ISP 图像处理:\n比如：Bayer 格式转 RGB、白平衡算法 (AWB)、镜头畸变校正。 这些主要靠几何计算和查表，不是靠神经网络的“权重”算出来的。 虽然现在也有 AI-ISP，但传统流程依然是 C++ 算法的主场。 特征点匹配 (传统 CV):\n比如：ORB, SIFT, RANSAC 算法。 这些算法包含大量的 if-else 逻辑判断、循环和排序。神经网络框架（TFLite）处理大量的逻辑分支效率极低，它擅长的是乘法和加法。 编解码 (Codec):\n比如：解析 H.264 视频流，或者解码 MP3/AAC 音频。 这是严格标准的数学流程，不能用神经网络去“猜”。 传感器融合 (Sensor Fusion):\n比如：把加速度计、陀螺仪和 GPS 的数据结合起来计算车辆位置（卡尔曼滤波）。 这是一套纯数学公式，完全不需要神经网络。 总结 如果你有一个 .tflite 或 .onnx 文件，里面装满了卷积层（Conv）、激活层（Relu），那就是途径 1。 如果你有一堆 C++ 代码，里面充满了 for 循环、if 判断、数学公式（FFT、矩阵求逆），那就是途径 2。 TFLite/SNPE 就像是一个“播放器”，它只能播放“神经网络”这一种格式的“影片”。如果你想在这个播放器里运行一个 Excel 表格（复杂的逻辑控制），它是做不到的。\nHexagon SDK 和 Qualcomm AI Engine Direct SDK (QNN)的对比 1. 核心定位区别 特性 Hexagon SDK Qualcomm AI Engine Direct SDK (QNN) 全称 Qualcomm Hexagon SDK Qualcomm AI Engine Direct (QNN) 主要目标 通用 DSP 编程。开发运行在 Hexagon DSP 上的任意 C/C++ 算法。 AI 模型推理。在 SoC 各个核心上加速神经网络模型的运行。 抽象层级 底层 (Low-level)。直接操作 DSP 寄存器、HVX 向量指令、线程调度 (QuRT)。 高层 (High-level)。抽象了硬件细节，提供统一的构图 API (AddNode, Execute)。 支持硬件 仅限 Hexagon DSP (cDSP, aDSP, sDSP)。 全平台 (AI Engine)：CPU, GPU (Adreno), DSP/HTP (Hexagon)。 输入 C/C++ 源代码、汇编代码。 训练好的 AI 模型 (ONNX, TFLite, PyTorch 等)。 核心机制 FastRPC (CPU 与 DSP 通信)。 Backend (后端) 机制，自动调度到底层硬件。 典型产物 libskel.so (DSP 库), libstub.so (CPU 库)。 model.cpp/.bin, model.so, context.bin。 2. 详细功能解析 Hexagon SDK (底层开发者工具) 它是给嵌入式开发者用的，让你能完全控制 DSP。\n用途：\n非 AI 算法加速：图像处理（ISP 后处理）、音频编解码、传感器融合算法。\nFastRPC 开发：你需要自己定义 .idl，自己写 stub 和 skel，自己管理内存映射。\n自定义 AI 算子：当 QNN 不支持某个算子时，你需要用 Hexagon SDK 手写这个算子的底层实现（Op Package）。\n你需要懂：Hexagon 汇编、HVX/HMX 向量化指令、QuRT 操作系统、缓存管理。\nQNN SDK (AI 应用开发者工具) 它是给 AI 算法工程师和 App 开发者用的，让你能快速部署模型。\n用途：\n模型转换：把 PyTorch/ONNX 模型转成 QNN 格式。\n统一推理：用一套 API，你可以选择是在 CPU 上跑，还是 GPU 上跑，还是 HTP (DSP) 上跑，只需改个参数。\n图优化：自动进行算子融合、量化、内存复用。\n你需要懂：神经网络结构、量化（Int8/FP16）、模型转换工具链。\n3. 它们的关系：上下游依赖 这是最关键的理解点：QNN SDK 的 HTP Backend 本质上是基于 Hexagon SDK 开发出来的一个超级复杂的“应用”。\n高通内部团队使用 Hexagon SDK 开发了 QNN 的 HTP 后端驱动（即 libQnnHtpVxxSkel.so）。 作为外部开发者，你直接使用 QNN SDK 提供的接口，实际上是间接调用了底层的 Hexagon 能力。 4. 架构图解 为了更直观地展示区别，我们来看它们在软件栈中的位置：\n@startuml !theme plain skinparam backgroundColor white skinparam linetype ortho title Hexagon SDK vs. QNN SDK package \"Android Application Layer\" { component \"Camera App\" as CamApp component \"AI Assistant App\" as AIApp } package \"Qualcomm AI Engine Direct (QNN SDK)\" #e1f5fe { component \"QNN API\" as QNN_API component \"QNN HTP Backend\\n(libQnnHtp.so)\" as HTP_Backend note right of QNN_API **QNN SDK 领域:** 关注模型、层、张量 自动调度 end note } package \"Hexagon SDK\" #fff9c4 { component \"FastRPC Framework\\n(libadsprpc.so)\" as FastRPC interface \"IDL Interfaces\" as IDL note right of FastRPC **Hexagon SDK 领域:** 关注 RPC 通信、 内存映射、线程 end note } package \"DSP Hardware (Signed PD)\" { component \"QuRT OS\" component \"libQnnHtpV81Skel.so\" as QnnSkel #e1f5fe note bottom of QnnSkel QNN 的 DSP 实现 (高通写好的) end note component \"libmy_algo_skel.so\" as MyAlgo #fff9c4 note bottom of MyAlgo **你自己用 Hexagon SDK 写的** 自定义算法库 end note } ' Flow 1: AI Flow AIApp --\u003e QNN_API : Run Model QNN_API --\u003e HTP_Backend HTP_Backend --\u003e FastRPC : Invoke FastRPC --\u003e QnnSkel : Execute Graph ' Flow 2: Custom Algo Flow CamApp --\u003e IDL : Call my_func() IDL --\u003e FastRPC : Invoke FastRPC --\u003e MyAlgo : Execute C Code @enduml 5. 什么时候用哪个？ 场景 A：我要在手机上跑 YOLOv8 或 ResNet 模型。\n👉 用 QNN SDK。这是标准用法，无需写底层 C 代码。\n场景 B：我有一个传统的 CV 算法（如高斯模糊、边缘检测），想搬到 DSP 上省电。\n👉 用 Hexagon SDK。你需要定义 IDL，写 C/C++ 代码，编译成 Skel。\n场景 C：我在用 QNN 跑模型，但模型里有一个特殊的层（例如 MySpecialLayer），QNN 转换器报错说不支持。\n👉 两个都用。\n你需要用 Hexagon SDK 编写这个算子的 DSP 实现（Op Package）。\n然后将其注册给 QNN SDK，让 QNN 在推理时调用你的代码。\n总结 Hexagon SDK 是给 DSP 程序员 用的，它是通用的、底层的。 QNN SDK 是给 AI 工程师 用的，它是专用的、高层的。 这是一篇为您起草的 Wiki 技术文档，详细对比了 AI 模型在端侧部署时的两种核心模式：解释执行模式与模型库模式。\nAI 模型部署模式：解释执行 vs. 模型库 在移动端 AI 推理（如 Qualcomm QNN、TensorFlow Lite、ONNX Runtime）中，根据应用程序加载和执行模型的方式不同，主要分为两种部署形态：解释执行模式 (Interpreted Mode) 和 模型库模式 (Model Library Mode)。\n本文档将详细解释这两者的工作原理、架构区别及优缺点，以辅助架构决策。\n1. 解释执行模式 (Interpreted Mode) 这是最通用、最常见的开发模式。在这种模式下，模型被视为数据文件。\n1.1 核心概念 别名：Standard Mode, File Mode, Dynamic Graph Construction。 输入：标准模型文件（如 .onnx, .tflite, .qnn）。 原理：推理引擎（Runtime）充当“解释器”的角色。App 启动时，Runtime 读取模型文件，解析其中的节点结构（拓扑关系、算子参数），然后在内存中动态构建计算图。 1.2 工作流 加载 (Load)：读取磁盘上的模型文件。 解析 (Parse)：遍历文件节点（如 Conv2d, Relu），理解其含义。 构图 (Compose)：调用后端 API（如 QnnGraph_addNode）在内存中重建图结构。 执行 (Execute)：输入数据，运行推理。 1.3 类比 就像照着菜谱做菜。厨师（Runtime）每次做菜前，都要先打开菜谱（模型文件），从头阅读每一个步骤，理解含义后，再开始动手。\n2. 模型库模式 (Model Library Mode) 这是追求极致性能和安全性的原生集成模式。在这种模式下，模型被转化为二进制代码。\n2.1 核心概念 别名：Native Mode, Compiled Mode, Shared Library Mode (.so Mode)。 输入：编译后的动态链接库（如 libMyModel.so）。 原理：使用工具链（如 qnn-model-lib-generator）将模型的网络结构“硬编码”为 C++ 代码，并与权重一起编译成机器码。App 运行时直接加载该库，跳过解析过程。 2.2 工作流 离线编译 (Offline Build)： model.onnx -\u003e Converter -\u003e model.cpp + model.bin model.cpp -\u003e Compiler (NDK) -\u003e libmodel.so 加载 (Load)：App 通过 dlopen 或 System API 加载 .so。 注册 (Register)：库中的函数直接向后端注册计算图（图结构已固化在指令中）。 执行 (Execute)：输入数据，运行推理。 2.3 类比 就像背下菜谱直接做菜。厨师已经将菜谱烂熟于心（编译进大脑/代码段），不需要翻书，拿起锅铲直接开始，省去了阅读和理解的时间。\n3. 架构视图对比 以下是两种模式在系统层面的架构差异（以 Qualcomm QNN 为例）：\n@startuml !theme plain skinparam backgroundColor white skinparam linetype ortho skinparam nodesep 60 skinparam ranksep 50 title 架构对比：解释执行 vs. 模型库 package \"Mode A: 解释执行 (Interpreted)\" { file \"model.onnx\" as File component \"Runtime / Parser\" as Parser #e1f5fe component \"Backend (QNN HTP)\" as Backend1 note right of Parser 运行时开销: 1. 解析文件格式 2. 循环调用 addNode 3. 动态内存分配 end note File --\u003e Parser : 1. Read Parser --\u003e Backend1 : 2. Construct Graph (API Calls) } package \"Mode B: 模型库 (Model Library)\" { component \"libMyModel.so\" as Lib #fff9c4 component \"Backend (QNN HTP)\" as Backend2 note right of Lib 零解析开销: 图结构已编译为 二进制指令序列 end note Lib .right.\u003e Backend2 : 1. Direct Link / Register } @enduml 4. 核心维度对比表 维度 解释执行模式 (Standard) 模型库模式 (Library .so) 模型形态 文件 (.onnx, .tflite) 动态库 (.so) 集成难度 低。只需替换文件即可更新模型。 高。模型更新需重新编译库，并重新打包 APK。 启动速度 较慢。需要解析文件、动态构图。 极快。无解析过程，直接加载符号。 包体积 较小（仅模型权重+结构）。但需打包庞大的解析引擎（如 libonnxruntime.so）。 较大（模型代码化）。但可省去解析引擎，只需核心后端库。 灵活性 极高。支持热更新（下发新模型文件）。 低。模型与 APK 版本强绑定。 安全性 低。模型文件容易被解包和逆向查看结构。 高。模型结构变成了汇编指令，难以还原。 适用场景 快速迭代、需要热更新、非极致性能敏感的应用。 系统级应用、旗舰游戏、核心算法保密、冷启动要求极高的场景。 5. 总结与建议 选择“解释执行模式”如果： 您的项目处于快速开发和迭代阶段。 您需要支持模型热更新（不发版更新 APK 即可下发新模型）。 您不想处理繁琐的 NDK/C++ 编译流程。 使用的是通用推理框架（TFLite, ONNX Runtime）。 选择“模型库模式”如果： 性能是首要指标：您需要毫秒级的极致冷启动速度（例如相机打开即识别）。 模型保密：您不希望竞争对手轻易拿到您的模型结构。 包体积优化：您确定只需要运行这一个模型，且不想引入庞大的 ONNX/TF 解析库。 您的应用是系统内置应用，且模型更新频率较低。 ","wordCount":"1075","inLanguage":"en","datePublished":"2025-08-27T17:17:50+08:00","dateModified":"2025-08-27T17:17:50+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://ethen-cao.github.io/ethenslab/explore-ai/hexagon-dsp%E7%AE%80%E4%BB%8B/"},"publisher":{"@type":"Organization","name":"Ethen 的实验室","logo":{"@type":"ImageObject","url":"https://ethen-cao.github.io/ethenslab/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ethen-cao.github.io/ethenslab/ accesskey=h title="Ethen 的实验室 (Alt + H)">Ethen 的实验室</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://ethen-cao.github.io/ethenslab/android-dev/ title=Android系统开发><span>Android系统开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/android-automotive-os-dev/ title="Android Automotive"><span>Android Automotive</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/qnx/ title=QNX开发><span>QNX开发</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/gunyah/ title=Gunyah><span>Gunyah</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/ivi-solution/ title=智能座舱方案><span>智能座舱方案</span></a></li><li><a href=https://ethen-cao.github.io/ethenslab/explore-ai title="Explore AI"><span>Explore AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ethen-cao.github.io/ethenslab/>Home</a>&nbsp;»&nbsp;<a href=https://ethen-cao.github.io/ethenslab/explore-ai/>Explore AI</a></div><h1 class="post-title entry-hint-parent">Hexagon DSP简介</h1><div class=post-meta><span title='2025-08-27 17:17:50 +0800 CST'>August 27, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1075 words</div></header><div class=post-content><h2 id=hexagon-dsp-的生态>Hexagon DSP 的生态<a hidden class=anchor aria-hidden=true href=#hexagon-dsp-的生态>#</a></h2><p>在 Qualcomm Hexagon DSP 的生态中，可以把资源的使用方式明确划分为<strong>两条主要赛道</strong>。它们针对的场景不同，开发难度不同，工具链也完全不同。</p><h3 id=1-第一条赛道ai-推理-ai-inference>1. 第一条赛道：AI 推理 (AI Inference)<a hidden class=anchor aria-hidden=true href=#1-第一条赛道ai-推理-ai-inference>#</a></h3><p><strong>关键词</strong>：TFLite, ONNX, SNPE, QNN, NNAPI
<strong>核心逻辑</strong>：<strong>“模型即代码” (Model is the code)</strong></p><p>这是目前最主流的用法，主要用于深度学习。</p><ul><li><strong>主要工作</strong>：<ul><li>训练模型（在 PC 上用 PyTorch/TensorFlow）。</li><li>量化模型（把 float32 转成 int8，为了 DSP 效率）。</li><li><strong>配置</strong>：你不需要写 DSP 代码，只需要在 App 里配置“代理”（Delegate）或“后端”（Backend）。</li></ul></li><li><strong>IDL 哪里去了？</strong><ul><li>高通（或 Google）已经提前写好了通用的 <code>.idl</code> 和 <code>skel.so</code>。</li><li>比如 <code>libQnnDsp.so</code> 或 <code>libhexagon_nn_skel.so</code>。这些库就像一个“万能翻译官”，它能读懂你的神经网络层（Conv2d, Softmax 等），并指挥 DSP 去执行。</li></ul></li><li><strong>优点</strong>：开发快，不用懂 DSP 汇编，只要模型能跑通就行。</li><li><strong>缺点</strong>：只能做神经网络相关的任务。如果你想做个特殊的“图像去雾算法”或者“音频变声”，这套框架帮不了你。</li></ul><h3 id=2-第二条赛道通用计算-general-compute--heterogeneous-computing>2. 第二条赛道：通用计算 (General Compute / Heterogeneous Computing)<a hidden class=anchor aria-hidden=true href=#2-第二条赛道通用计算-general-compute--heterogeneous-computing>#</a></h3><p><strong>关键词</strong>：Hexagon SDK, FastRPC, IDL, QAIC, C/C++, HVX/HMX
<strong>核心逻辑</strong>：<strong>“手写算子” (Custom Implementation)</strong></p><p>这是传统的嵌入式开发用法。</p><ul><li><strong>主要工作</strong>：<ul><li><strong>定义接口</strong>：必须写 <code>.idl</code> 文件，告诉 CPU 和 DSP 怎么传参数。</li><li><strong>生成胶水代码</strong>：使用 <code>QAIC</code> 编译 IDL，生成 Stub 和 Skel。</li><li><strong>实现算法</strong>：在 DSP 侧写 C/C++ 代码（Skel 实现）。如果是为了高性能，你甚至需要用 <strong>Hexagon Intrinsics</strong> 手写向量化指令（利用 HVX 硬件加速）。</li></ul></li><li><strong>IDL 的作用</strong>：<ul><li>因为你的函数是自定义的（例如 <code>my_special_image_filter()</code>），高通没法预知，所以必须由你通过 IDL 定义，并由 QAIC 生成专用的桥梁。</li></ul></li><li><strong>优点</strong>：<ul><li><strong>极高的自由度</strong>：可以做任何计算任务（图像处理、CV 算法、音频编解码、加密解密、传感器数据融合）。</li><li><strong>极致性能</strong>：你可以手写汇编级优化，榨干 DSP 的每一个时钟周期。</li></ul></li><li><strong>缺点</strong>：门槛极高，需要懂内存管理、多线程同步、向量化编程，还要处理复杂的签名和打包流程。</li></ul><hr><h3 id=对比总结表>对比总结表<a hidden class=anchor aria-hidden=true href=#对比总结表>#</a></h3><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left><strong>途径 1: AI 框架 (TFLite/ONNX)</strong></th><th style=text-align:left><strong>途径 2: 普通 DSP 开发 (FastRPC)</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>输入物</strong></td><td style=text-align:left>神经网络模型文件 (<code>.tflite</code>, <code>.onnx</code>, <code>.dlc</code>)</td><td style=text-align:left>C/C++ 源代码 + <code>.idl</code> 接口定义</td></tr><tr><td style=text-align:left><strong>中间工具</strong></td><td style=text-align:left>模型转换器 (Converter / Quantizer)</td><td style=text-align:left><strong>QAIC 编译器</strong></td></tr><tr><td style=text-align:left><strong>运行库</strong></td><td style=text-align:left>通用推理引擎 (<code>libQnnDsp.so</code>, <code>libSnpeDsp.so</code>)</td><td style=text-align:left>你编译生成的专用库 (<code>lib_skel.so</code>)</td></tr><tr><td style=text-align:left><strong>是否需要 IDL</strong></td><td style=text-align:left><strong>不需要</strong> (框架内部封装好了)</td><td style=text-align:left><strong>必须需要</strong></td></tr><tr><td style=text-align:left><strong>主要难点</strong></td><td style=text-align:left>模型量化精度损失、算子支持度</td><td style=text-align:left>内存管理、并行编程、HVX 向量化优化</td></tr><tr><td style=text-align:left><strong>典型场景</strong></td><td style=text-align:left>物体检测、人脸识别、语音识别</td><td style=text-align:left>图像预处理(缩放/旋转)、ISP 算法、音频降噪</td></tr></tbody></table><h3 id=它们有交集吗>它们有交集吗？<a hidden class=anchor aria-hidden=true href=#它们有交集吗>#</a></h3><p><strong>有，而且很重要。</strong></p><p>这就是所谓的 <strong>&ldquo;Custom Operator&rdquo; (自定义算子)</strong>。
假设你在跑一个 TFLite 模型，里面有一个很新的数学运算层（比如某种特殊的 Attention 机制），TFLite 的 DSP 代理不支持它。
这时候，你需要：</p><ol><li>走 <strong>途径 2</strong>：写 <code>.idl</code>，用 C++ 实现这个特殊的算子，编译成一个 DSP 库。</li><li>走 <strong>途径 1</strong>：告诉 TFLite，“遇到这个特殊的层，请调用我刚才写的那个库”。</li></ol><p>所以，<strong>途径 2 其实是途径 1 的底层基石</strong>。</p><h4 id=神经网络相关的任务简介>神经网络相关的任务简介<a hidden class=anchor aria-hidden=true href=#神经网络相关的任务简介>#</a></h4><p>当我们说“基于 TFLite/ONNX 等框架<strong>只能做神经网络相关的任务</strong>”时，意思是这套工具链的本质是**“解释器”（Interpreter）**。</p><p>它不懂你的 C++ 业务逻辑，它只懂**“张量运算”（Matrix Math）**。它的唯一工作就是加载一个你训练好的模型文件，把输入数据喂进去，算出输出结果。</p><p>具体来说，这些任务通常指的是<strong>可以用深度学习模型（Deep Learning Models）解决的问题</strong>。</p><p>以下是这类任务的具体分类和典型例子：</p><h5 id=1-计算机视觉-computer-vision---cv>1. 计算机视觉 (Computer Vision - CV)<a hidden class=anchor aria-hidden=true href=#1-计算机视觉-computer-vision---cv>#</a></h5><p>这是 DSP 上最常见的神经网络任务。输入是图片/视频帧，输出是识别结果。</p><ul><li><strong>物体检测 (Object Detection)</strong>: 识别画面里哪里有人、车、红绿灯。（典型模型：YOLO, SSD, EfficientDet）。</li><li><strong>图像分类 (Image Classification)</strong>: 判断这张图是“猫”还是“狗”。（典型模型：MobileNet, ResNet）。</li><li><strong>语义分割 (Semantic Segmentation)</strong>: 把背景虚化（比如视频会议换背景），或者自动驾驶中识别哪里是路面。（典型模型：DeepLab, UNet）。</li><li><strong>人脸关键点 (Face Landmark)</strong>: 识别眼睛、鼻子嘴巴的位置，用于美颜、贴纸特效。</li><li><strong>超分辨率 (Super Resolution)</strong>: 把 720p 的模糊视频通过 AI 猜想变成 1080p 清晰视频。</li></ul><h5 id=2-音频与语音处理-audio--speech>2. 音频与语音处理 (Audio & Speech)<a hidden class=anchor aria-hidden=true href=#2-音频与语音处理-audio--speech>#</a></h5><p>输入是麦克风的音频流，输出是文字或增强后的音频。</p><ul><li><strong>关键词唤醒 (Keyword Spotting)</strong>: 手机待机时监听“Hey Siri”或“小爱同学”。这是一个极小的神经网络，常驻 DSP。</li><li><strong>语音降噪 (AI Noise Suppression)</strong>: 区分人声和背景噪音（如风扇声），只保留人声。（传统降噪是用滤波算法，现在流行用 RNN/LSTM 神经网络做）。</li><li><strong>声纹识别 (Speaker Verification)</strong>: 确认说话的人是不是机主。</li></ul><h5 id=3-自然语言处理-natural-language---nlp>3. 自然语言处理 (Natural Language - NLP)<a hidden class=anchor aria-hidden=true href=#3-自然语言处理-natural-language---nlp>#</a></h5><p>虽然大模型（LLM）通常跑在 NPU 上，但 DSP 也可以处理轻量级的 NLP 任务。</p><ul><li><strong>意图识别</strong>: 比如输入“定明天的闹钟”，模型判断这是一个“设置闹钟”的指令。</li><li><strong>智能键盘预测</strong>: 预测你下一个要打的字。</li></ul><hr><h5 id=关键区别它不能做什么>关键区别：它“不能”做什么？<a hidden class=anchor aria-hidden=true href=#关键区别它不能做什么>#</a></h5><p>为了更清楚边界在哪里，我们来看看哪些任务<strong>不属于</strong>“神经网络相关任务”，因此<strong>不能</strong>用 TFLite/ONNX 途径，而必须用 <strong>FastRPC + C++ (IDL)</strong> 的方式去开发：</p><ol><li><p><strong>传统的 ISP 图像处理</strong>:</p><ul><li>比如：<strong>Bayer 格式转 RGB</strong>、<strong>白平衡算法 (AWB)</strong>、<strong>镜头畸变校正</strong>。</li><li>这些主要靠几何计算和查表，不是靠神经网络的“权重”算出来的。</li><li><em>虽然现在也有 AI-ISP，但传统流程依然是 C++ 算法的主场。</em></li></ul></li><li><p><strong>特征点匹配 (传统 CV)</strong>:</p><ul><li>比如：<strong>ORB, SIFT, RANSAC</strong> 算法。</li><li>这些算法包含大量的 <code>if-else</code> 逻辑判断、循环和排序。神经网络框架（TFLite）处理大量的逻辑分支效率极低，它擅长的是乘法和加法。</li></ul></li><li><p><strong>编解码 (Codec)</strong>:</p><ul><li>比如：解析 H.264 视频流，或者解码 MP3/AAC 音频。</li><li>这是严格标准的数学流程，不能用神经网络去“猜”。</li></ul></li><li><p><strong>传感器融合 (Sensor Fusion)</strong>:</p><ul><li>比如：把加速度计、陀螺仪和 GPS 的数据结合起来计算车辆位置（<strong>卡尔曼滤波</strong>）。</li><li>这是一套纯数学公式，完全不需要神经网络。</li></ul></li></ol><h5 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h5><ul><li>如果你有一个 <strong><code>.tflite</code></strong> 或 <strong><code>.onnx</code></strong> 文件，里面装满了卷积层（Conv）、激活层（Relu），那就是<strong>途径 1</strong>。</li><li>如果你有一堆 <strong>C++ 代码</strong>，里面充满了 <code>for</code> 循环、<code>if</code> 判断、数学公式（FFT、矩阵求逆），那就是<strong>途径 2</strong>。</li></ul><p><img src=/ethenslab/images/licensed-image.jpeg alt="Image of neural network architecture layers"></p><p><strong>TFLite/SNPE 就像是一个“播放器”</strong>，它只能播放“神经网络”这一种格式的“影片”。如果你想在这个播放器里运行一个 Excel 表格（复杂的逻辑控制），它是做不到的。</p><h2 id=hexagon-sdk-和-qualcomm-ai-engine-direct-sdk-qnn的对比>Hexagon SDK 和 Qualcomm AI Engine Direct SDK (QNN)的对比<a hidden class=anchor aria-hidden=true href=#hexagon-sdk-和-qualcomm-ai-engine-direct-sdk-qnn的对比>#</a></h2><h3 id=1-核心定位区别>1. 核心定位区别<a hidden class=anchor aria-hidden=true href=#1-核心定位区别>#</a></h3><table><thead><tr><th>特性</th><th><strong>Hexagon SDK</strong></th><th><strong>Qualcomm AI Engine Direct SDK (QNN)</strong></th></tr></thead><tbody><tr><td><strong>全称</strong></td><td>Qualcomm Hexagon SDK</td><td>Qualcomm AI Engine Direct (QNN)</td></tr><tr><td><strong>主要目标</strong></td><td><strong>通用 DSP 编程</strong>。开发运行在 Hexagon DSP 上的任意 C/C++ 算法。</td><td><strong>AI 模型推理</strong>。在 SoC 各个核心上加速神经网络模型的运行。</td></tr><tr><td><strong>抽象层级</strong></td><td><strong>底层 (Low-level)</strong>。直接操作 DSP 寄存器、HVX 向量指令、线程调度 (QuRT)。</td><td><strong>高层 (High-level)</strong>。抽象了硬件细节，提供统一的构图 API (AddNode, Execute)。</td></tr><tr><td><strong>支持硬件</strong></td><td>仅限 <strong>Hexagon DSP</strong> (cDSP, aDSP, sDSP)。</td><td><strong>全平台 (AI Engine)</strong>：CPU, GPU (Adreno), DSP/HTP (Hexagon)。</td></tr><tr><td><strong>输入</strong></td><td>C/C++ 源代码、汇编代码。</td><td>训练好的 AI 模型 (ONNX, TFLite, PyTorch 等)。</td></tr><tr><td><strong>核心机制</strong></td><td>FastRPC (CPU 与 DSP 通信)。</td><td>Backend (后端) 机制，自动调度到底层硬件。</td></tr><tr><td><strong>典型产物</strong></td><td><code>libskel.so</code> (DSP 库), <code>libstub.so</code> (CPU 库)。</td><td><code>model.cpp/.bin</code>, <code>model.so</code>, <code>context.bin</code>。</td></tr></tbody></table><hr><h3 id=2-详细功能解析>2. 详细功能解析<a hidden class=anchor aria-hidden=true href=#2-详细功能解析>#</a></h3><h4 id=hexagon-sdk-底层开发者工具><strong>Hexagon SDK (底层开发者工具)</strong><a hidden class=anchor aria-hidden=true href=#hexagon-sdk-底层开发者工具>#</a></h4><p>它是给嵌入式开发者用的，让你能完全控制 DSP。</p><ul><li><p><strong>用途</strong>：</p></li><li><p><strong>非 AI 算法加速</strong>：图像处理（ISP 后处理）、音频编解码、传感器融合算法。</p></li><li><p><strong>FastRPC 开发</strong>：你需要自己定义 <code>.idl</code>，自己写 <code>stub</code> 和 <code>skel</code>，自己管理内存映射。</p></li><li><p><strong>自定义 AI 算子</strong>：当 QNN 不支持某个算子时，你需要用 Hexagon SDK 手写这个算子的底层实现（Op Package）。</p></li><li><p><strong>你需要懂</strong>：Hexagon 汇编、HVX/HMX 向量化指令、QuRT 操作系统、缓存管理。</p></li></ul><h4 id=qnn-sdk-ai-应用开发者工具><strong>QNN SDK (AI 应用开发者工具)</strong><a hidden class=anchor aria-hidden=true href=#qnn-sdk-ai-应用开发者工具>#</a></h4><p>它是给 AI 算法工程师和 App 开发者用的，让你能快速部署模型。</p><ul><li><p><strong>用途</strong>：</p></li><li><p><strong>模型转换</strong>：把 PyTorch/ONNX 模型转成 QNN 格式。</p></li><li><p><strong>统一推理</strong>：用一套 API，你可以选择是在 CPU 上跑，还是 GPU 上跑，还是 HTP (DSP) 上跑，只需改个参数。</p></li><li><p><strong>图优化</strong>：自动进行算子融合、量化、内存复用。</p></li><li><p><strong>你需要懂</strong>：神经网络结构、量化（Int8/FP16）、模型转换工具链。</p></li></ul><hr><h3 id=3-它们的关系上下游依赖>3. 它们的关系：上下游依赖<a hidden class=anchor aria-hidden=true href=#3-它们的关系上下游依赖>#</a></h3><p>这是最关键的理解点：<strong>QNN SDK 的 HTP Backend 本质上是基于 Hexagon SDK 开发出来的一个超级复杂的“应用”。</strong></p><ul><li>高通内部团队使用 <strong>Hexagon SDK</strong> 开发了 QNN 的 HTP 后端驱动（即 <code>libQnnHtpVxxSkel.so</code>）。</li><li>作为外部开发者，你直接使用 QNN SDK 提供的接口，实际上是间接调用了底层的 Hexagon 能力。</li></ul><h3 id=4-架构图解>4. 架构图解<a hidden class=anchor aria-hidden=true href=#4-架构图解>#</a></h3><p>为了更直观地展示区别，我们来看它们在软件栈中的位置：</p><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
skinparam backgroundColor white
skinparam linetype ortho

title Hexagon SDK vs. QNN SDK

package &#34;Android Application Layer&#34; {
    component &#34;Camera App&#34; as CamApp
    component &#34;AI Assistant App&#34; as AIApp
}

package &#34;Qualcomm AI Engine Direct (QNN SDK)&#34; #e1f5fe {
    component &#34;QNN API&#34; as QNN_API
    component &#34;QNN HTP Backend\n(libQnnHtp.so)&#34; as HTP_Backend
    note right of QNN_API
        **QNN SDK 领域:**
        关注模型、层、张量
        自动调度
    end note
}

package &#34;Hexagon SDK&#34; #fff9c4 {
    component &#34;FastRPC Framework\n(libadsprpc.so)&#34; as FastRPC
    interface &#34;IDL Interfaces&#34; as IDL
    note right of FastRPC
        **Hexagon SDK 领域:**
        关注 RPC 通信、
        内存映射、线程
    end note
}

package &#34;DSP Hardware (Signed PD)&#34; {
    component &#34;QuRT OS&#34;
    
    component &#34;libQnnHtpV81Skel.so&#34; as QnnSkel #e1f5fe
    note bottom of QnnSkel
        QNN 的 DSP 实现
        (高通写好的)
    end note
    
    component &#34;libmy_algo_skel.so&#34; as MyAlgo #fff9c4
    note bottom of MyAlgo
        **你自己用 Hexagon SDK 写的**
        自定义算法库
    end note
}

&#39; Flow 1: AI Flow
AIApp --&gt; QNN_API : Run Model
QNN_API --&gt; HTP_Backend
HTP_Backend --&gt; FastRPC : Invoke
FastRPC --&gt; QnnSkel : Execute Graph

&#39; Flow 2: Custom Algo Flow
CamApp --&gt; IDL : Call my_func()
IDL --&gt; FastRPC : Invoke
FastRPC --&gt; MyAlgo : Execute C Code

@enduml
</code></pre><h3 id=5-什么时候用哪个>5. 什么时候用哪个？<a hidden class=anchor aria-hidden=true href=#5-什么时候用哪个>#</a></h3><ul><li><p><strong>场景 A：我要在手机上跑 YOLOv8 或 ResNet 模型。</strong></p></li><li><p>👉 <strong>用 QNN SDK</strong>。这是标准用法，无需写底层 C 代码。</p></li><li><p><strong>场景 B：我有一个传统的 CV 算法（如高斯模糊、边缘检测），想搬到 DSP 上省电。</strong></p></li><li><p>👉 <strong>用 Hexagon SDK</strong>。你需要定义 IDL，写 C/C++ 代码，编译成 Skel。</p></li><li><p><strong>场景 C：我在用 QNN 跑模型，但模型里有一个特殊的层（例如 <code>MySpecialLayer</code>），QNN 转换器报错说不支持。</strong></p></li><li><p>👉 <strong>两个都用</strong>。</p></li><li><p>你需要用 <strong>Hexagon SDK</strong> 编写这个算子的 DSP 实现（Op Package）。</p></li><li><p>然后将其注册给 <strong>QNN SDK</strong>，让 QNN 在推理时调用你的代码。</p></li></ul><h3 id=总结-1>总结<a hidden class=anchor aria-hidden=true href=#总结-1>#</a></h3><ul><li><strong>Hexagon SDK</strong> 是给 <strong>DSP 程序员</strong> 用的，它是通用的、底层的。</li><li><strong>QNN SDK</strong> 是给 <strong>AI 工程师</strong> 用的，它是专用的、高层的。</li></ul><p>这是一篇为您起草的 Wiki 技术文档，详细对比了 AI 模型在端侧部署时的两种核心模式：<strong>解释执行模式</strong>与<strong>模型库模式</strong>。</p><hr><h1 id=ai-模型部署模式解释执行-vs-模型库>AI 模型部署模式：解释执行 vs. 模型库<a hidden class=anchor aria-hidden=true href=#ai-模型部署模式解释执行-vs-模型库>#</a></h1><p>在移动端 AI 推理（如 Qualcomm QNN、TensorFlow Lite、ONNX Runtime）中，根据应用程序加载和执行模型的方式不同，主要分为两种部署形态：<strong>解释执行模式 (Interpreted Mode)</strong> 和 <strong>模型库模式 (Model Library Mode)</strong>。</p><p>本文档将详细解释这两者的工作原理、架构区别及优缺点，以辅助架构决策。</p><h2 id=1-解释执行模式-interpreted-mode>1. 解释执行模式 (Interpreted Mode)<a hidden class=anchor aria-hidden=true href=#1-解释执行模式-interpreted-mode>#</a></h2><p>这是最通用、最常见的开发模式。在这种模式下，模型被视为<strong>数据文件</strong>。</p><h3 id=11-核心概念>1.1 核心概念<a hidden class=anchor aria-hidden=true href=#11-核心概念>#</a></h3><ul><li><strong>别名</strong>：Standard Mode, File Mode, Dynamic Graph Construction。</li><li><strong>输入</strong>：标准模型文件（如 <code>.onnx</code>, <code>.tflite</code>, <code>.qnn</code>）。</li><li><strong>原理</strong>：推理引擎（Runtime）充当“解释器”的角色。App 启动时，Runtime 读取模型文件，解析其中的节点结构（拓扑关系、算子参数），然后在内存中动态构建计算图。</li></ul><h3 id=12-工作流>1.2 工作流<a hidden class=anchor aria-hidden=true href=#12-工作流>#</a></h3><ol><li><strong>加载 (Load)</strong>：读取磁盘上的模型文件。</li><li><strong>解析 (Parse)</strong>：遍历文件节点（如 Conv2d, Relu），理解其含义。</li><li><strong>构图 (Compose)</strong>：调用后端 API（如 <code>QnnGraph_addNode</code>）在内存中重建图结构。</li><li><strong>执行 (Execute)</strong>：输入数据，运行推理。</li></ol><h3 id=13-类比>1.3 类比<a hidden class=anchor aria-hidden=true href=#13-类比>#</a></h3><p>就像<strong>照着菜谱做菜</strong>。厨师（Runtime）每次做菜前，都要先打开菜谱（模型文件），从头阅读每一个步骤，理解含义后，再开始动手。</p><hr><h2 id=2-模型库模式-model-library-mode>2. 模型库模式 (Model Library Mode)<a hidden class=anchor aria-hidden=true href=#2-模型库模式-model-library-mode>#</a></h2><p>这是追求极致性能和安全性的原生集成模式。在这种模式下，模型被转化为<strong>二进制代码</strong>。</p><h3 id=21-核心概念>2.1 核心概念<a hidden class=anchor aria-hidden=true href=#21-核心概念>#</a></h3><ul><li><strong>别名</strong>：Native Mode, Compiled Mode, Shared Library Mode (<code>.so</code> Mode)。</li><li><strong>输入</strong>：编译后的动态链接库（如 <code>libMyModel.so</code>）。</li><li><strong>原理</strong>：使用工具链（如 <code>qnn-model-lib-generator</code>）将模型的网络结构“硬编码”为 C++ 代码，并与权重一起编译成机器码。App 运行时直接加载该库，跳过解析过程。</li></ul><h3 id=22-工作流>2.2 工作流<a hidden class=anchor aria-hidden=true href=#22-工作流>#</a></h3><ol><li><strong>离线编译 (Offline Build)</strong>：</li></ol><ul><li><code>model.onnx</code> -> <strong>Converter</strong> -> <code>model.cpp</code> + <code>model.bin</code></li><li><code>model.cpp</code> -> <strong>Compiler (NDK)</strong> -> <code>libmodel.so</code></li></ul><ol start=2><li><strong>加载 (Load)</strong>：App 通过 <code>dlopen</code> 或 System API 加载 <code>.so</code>。</li><li><strong>注册 (Register)</strong>：库中的函数直接向后端注册计算图（图结构已固化在指令中）。</li><li><strong>执行 (Execute)</strong>：输入数据，运行推理。</li></ol><h3 id=23-类比>2.3 类比<a hidden class=anchor aria-hidden=true href=#23-类比>#</a></h3><p>就像<strong>背下菜谱直接做菜</strong>。厨师已经将菜谱烂熟于心（编译进大脑/代码段），不需要翻书，拿起锅铲直接开始，省去了阅读和理解的时间。</p><hr><h2 id=3-架构视图对比>3. 架构视图对比<a hidden class=anchor aria-hidden=true href=#3-架构视图对比>#</a></h2><p>以下是两种模式在系统层面的架构差异（以 Qualcomm QNN 为例）：</p><pre tabindex=0><code class=language-plantuml data-lang=plantuml>@startuml
!theme plain
skinparam backgroundColor white
skinparam linetype ortho
skinparam nodesep 60
skinparam ranksep 50

title 架构对比：解释执行 vs. 模型库

package &#34;Mode A: 解释执行 (Interpreted)&#34; {
    file &#34;model.onnx&#34; as File
    component &#34;Runtime / Parser&#34; as Parser #e1f5fe
    component &#34;Backend (QNN HTP)&#34; as Backend1
    
    note right of Parser
       &lt;b&gt;运行时开销:&lt;/b&gt;
       1. 解析文件格式
       2. 循环调用 addNode
       3. 动态内存分配
    end note
    
    File --&gt; Parser : 1. Read
    Parser --&gt; Backend1 : 2. Construct Graph (API Calls)
}

package &#34;Mode B: 模型库 (Model Library)&#34; {
    component &#34;libMyModel.so&#34; as Lib #fff9c4
    component &#34;Backend (QNN HTP)&#34; as Backend2
    
    note right of Lib
       &lt;b&gt;零解析开销:&lt;/b&gt;
       图结构已编译为
       二进制指令序列
    end note
    
    Lib .right.&gt; Backend2 : 1. Direct Link / Register
}

@enduml
</code></pre><hr><h2 id=4-核心维度对比表>4. 核心维度对比表<a hidden class=anchor aria-hidden=true href=#4-核心维度对比表>#</a></h2><table><thead><tr><th>维度</th><th>解释执行模式 (Standard)</th><th>模型库模式 (Library .so)</th></tr></thead><tbody><tr><td><strong>模型形态</strong></td><td>文件 (<code>.onnx</code>, <code>.tflite</code>)</td><td>动态库 (<code>.so</code>)</td></tr><tr><td><strong>集成难度</strong></td><td><strong>低</strong>。只需替换文件即可更新模型。</td><td><strong>高</strong>。模型更新需重新编译库，并重新打包 APK。</td></tr><tr><td><strong>启动速度</strong></td><td><strong>较慢</strong>。需要解析文件、动态构图。</td><td><strong>极快</strong>。无解析过程，直接加载符号。</td></tr><tr><td><strong>包体积</strong></td><td>较小（仅模型权重+结构）。但需打包庞大的解析引擎（如 <code>libonnxruntime.so</code>）。</td><td>较大（模型代码化）。但可省去解析引擎，只需核心后端库。</td></tr><tr><td><strong>灵活性</strong></td><td><strong>极高</strong>。支持热更新（下发新模型文件）。</td><td><strong>低</strong>。模型与 APK 版本强绑定。</td></tr><tr><td><strong>安全性</strong></td><td><strong>低</strong>。模型文件容易被解包和逆向查看结构。</td><td><strong>高</strong>。模型结构变成了汇编指令，难以还原。</td></tr><tr><td><strong>适用场景</strong></td><td>快速迭代、需要热更新、非极致性能敏感的应用。</td><td>系统级应用、旗舰游戏、核心算法保密、冷启动要求极高的场景。</td></tr></tbody></table><h2 id=5-总结与建议>5. 总结与建议<a hidden class=anchor aria-hidden=true href=#5-总结与建议>#</a></h2><h3 id=选择解释执行模式如果>选择“解释执行模式”如果：<a hidden class=anchor aria-hidden=true href=#选择解释执行模式如果>#</a></h3><ul><li>您的项目处于快速开发和迭代阶段。</li><li>您需要支持<strong>模型热更新</strong>（不发版更新 APK 即可下发新模型）。</li><li>您不想处理繁琐的 NDK/C++ 编译流程。</li><li>使用的是通用推理框架（TFLite, ONNX Runtime）。</li></ul><h3 id=选择模型库模式如果>选择“模型库模式”如果：<a hidden class=anchor aria-hidden=true href=#选择模型库模式如果>#</a></h3><ul><li><strong>性能是首要指标</strong>：您需要毫秒级的极致冷启动速度（例如相机打开即识别）。</li><li><strong>模型保密</strong>：您不希望竞争对手轻易拿到您的模型结构。</li><li><strong>包体积优化</strong>：您确定只需要运行这一个模型，且不想引入庞大的 ONNX/TF 解析库。</li><li>您的应用是<strong>系统内置应用</strong>，且模型更新频率较低。</li></ul></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ethen-cao.github.io/ethenslab/>Ethen 的实验室</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:!0,theme:"default"})</script><script src=https://cdn.jsdelivr.net/npm/plantuml-encoder@1.4.0/dist/plantuml-encoder.min.js></script><script>(function(){const e=document.querySelectorAll("pre > code.language-plantuml, pre > code.language-planuml");e.forEach(e=>{const s=e.innerText,o=plantumlEncoder.encode(s),i="https://www.plantuml.com/plantuml/svg/"+o,t=document.createElement("img");t.src=i,t.alt="PlantUML Diagram",t.style.maxWidth="100%";const n=e.parentNode;n.parentNode.replaceChild(t,n)})})()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>